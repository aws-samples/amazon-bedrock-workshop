{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context\n",
    "In this pattern we will explore how to use Aurora Postgres PGVector, to store embedding. In this example we will see how to store corpus as embedding in the vector datastore and use that in the context of the query to retrive answer for the model. For embedding we will be using Titan embedding and for llm we will be leveraging Anthropic Claude\n",
    "\n",
    "\n",
    "### Pattern\n",
    "We can improve upon this process by implementing an architecure called Retreival Augmented Generation (RAG). RAG retrieves data from outside the language model (non-parametric) and augments the prompts by adding the relevant retrieved data in context. \n",
    "\n",
    "In this notebook we explain how to approach the pattern of Question Answering to find and leverage the documents to provide answers to the user questions.\n",
    "\n",
    "### Challenges\n",
    "- How to manage large document(s) that exceed the token limit\n",
    "- How to find the document(s) relevant to the question being asked\n",
    "\n",
    "### Proposal\n",
    "To the above challenges, this notebook proposes the following strategy\n",
    "#### Prepare documents\n",
    "![Embeddings](../../imgs/Embeddings_lang.png)\n",
    "\n",
    "Before being able to answer the questions, the documents must be processed and a stored in a document store index\n",
    "- Load the documents\n",
    "- Process and split them into smaller chunks\n",
    "- Create a numerical vector representation of each chunk using Amazon Bedrock Titan Embeddings model\n",
    "- Create an index using the chunks and the corresponding embeddings\n",
    "#### Ask question\n",
    "![Question](../../imgs/Chatbot_lang.png)\n",
    "\n",
    "When the documents index is prepared, you are ready to ask the questions and relevant documents will be fetched based on the question being asked. Following steps will be executed.\n",
    "- Create an embedding of the input question\n",
    "- Compare the question embedding with the embeddings in the index\n",
    "- Fetch the (top N) relevant document chunks\n",
    "- Add those chunks as part of the context in the prompt\n",
    "- Send the prompt to the model under Amazon Bedrock\n",
    "- Get the contextual answer based on the documents retrieved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-requisites\n",
    "\n",
    "a.  will need to have created a Amazon RDS postgres database\n",
    "b.  I executed this pattern against Aurora Postgres serverless v2 v15.3 . This by defaults supports IVF Flat index\n",
    "c.  Once the prostgres cluster is created. Firstly make sure,the VPC's Cluster security group allows access to your device. There are a number of ways to confiugure this, but will not be diving deep in that. \n",
    "\n",
    "     1. Connect to the database \n",
    "     psql -h <<hostname>>  -U <<username>> -d <<databsename>>\n",
    "     \n",
    "     2. Create vector extensions\n",
    "     CREATE EXTENSION vector;\n",
    "     \n",
    "     3. validate the extensions with the command \\dx . It should list all extensions \n",
    "     eg:\n",
    "\n",
    "\n",
    "     \n",
    "     -[ RECORD 1 ]-------------------------------------------\n",
    "Name        | aws_commons\n",
    "Version     | 1.2\n",
    "Schema      | public\n",
    "Description | Common data types across AWS services\n",
    "\n",
    "     -[ RECORD 2 ]-------------------------------------------\n",
    "Name        | aws_ml\n",
    "Version     | 1.0\n",
    "Schema      | public\n",
    "Description | ml integration\n",
    "\n",
    "     -[ RECORD 3 ]-------------------------------------------\n",
    "Name        | plpgsql\n",
    "Version     | 1.0\n",
    "Schema      | pg_catalog\n",
    "Description | PL/pgSQL procedural language\n",
    "\n",
    "     -[ RECORD 4 ]-------------------------------------------\n",
    "Name        | vector\n",
    "Version     | 0.4.1\n",
    "Schema      | public\n",
    "Description | vector data type and ivfflat access method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --no-build-isolation --force-reinstall \\\n",
    "    \"boto3>=1.28.57\" \\\n",
    "    \"awscli>=1.29.57\" \\\n",
    "    \"botocore>=1.31.57\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain>=0.1.11\n",
    "%pip install pypdf==4.1.0\n",
    "%pip install langchain-community faiss-cpu==1.8.0 tiktoken==0.6.0 sqlalchemy==2.0.28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is the driver required to store embeeded data to Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install psycopg psycopg2-binary pgvector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import boto3\n",
    "import botocore\n",
    "\n",
    "boto3_bedrock = boto3.client(\"bedrock-runtime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The next cell we choose Claude as the llm and we use titan-embedding-model embedding format. This will be used to embedd the query and corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import CSVLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "from langchain.vectorstores.pgvector import PGVector, DistanceStrategy\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.embeddings import BedrockEmbeddings\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "import os\n",
    "\n",
    "# Note that the best practise is to fetech from secrets manager\n",
    "\n",
    "os.environ[\"PGVECTOR_DRIVER\"] = \"psycopg2\"\n",
    "os.environ[\"PGVECTOR_USER\"] = \"<<postgres user>>\"\n",
    "os.environ[\"PGVECTOR_PASSWORD\"] = \"<<password>>\"\n",
    "os.environ[\"PGVECTOR_HOST\"] = \"<<host endpoint>>\"\n",
    "os.environ[\"PGVECTOR_PORT\"] = \"5432\"\n",
    "os.environ[\"PGVECTOR_DATABASE\"] = \"<<database name>>\"\n",
    "\n",
    "# anthropic.claude-v1\n",
    "# amazon.titan-embed-text-v1\n",
    "# - create the Anthropic Model for text generation\n",
    "\n",
    "llm = Bedrock(\n",
    "    model_id=\"anthropic.claude-v2\",\n",
    "    client=boto3_bedrock,\n",
    "    model_kwargs={\"max_tokens_to_sample\": 200},\n",
    ")\n",
    "bedrock_embeddings = BedrockEmbeddings(\n",
    "    model_id=\"amazon.titan-embed-text-v1\", client=boto3_bedrock\n",
    ")\n",
    "print(bedrock_embeddings.model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from langchain.text_splitter import (\n",
    "    CharacterTextSplitter,\n",
    "    RecursiveCharacterTextSplitter,\n",
    ")\n",
    "from langchain.document_loaders import PyPDFLoader, PyPDFDirectoryLoader\n",
    "from langchain.vectorstores.pgvector import PGVector, DistanceStrategy\n",
    "from typing import List, Tuple\n",
    "from langchain.vectorstores import pgvector\n",
    "\n",
    "loader = PyPDFDirectoryLoader(\"./data/\")\n",
    "\n",
    "\n",
    "connection_string = PGVector.connection_string_from_db_params(\n",
    "    driver=os.environ.get(\"PGVECTOR_DRIVER\"),\n",
    "    user=os.environ.get(\"PGVECTOR_USER\"),\n",
    "    password=os.environ.get(\"PGVECTOR_PASSWORD\"),\n",
    "    host=os.environ.get(\"PGVECTOR_HOST\"),\n",
    "    port=os.environ.get(\"PGVECTOR_PORT\"),\n",
    "    database=os.environ.get(\"PGVECTOR_DATABASE\"),\n",
    ")\n",
    "\n",
    "documents = loader.load()\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "print(len(documents))\n",
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_name = \"tbl_store_embedding\"\n",
    "\n",
    "print({connection_string})\n",
    "db = PGVector.from_documents(\n",
    "    embedding=bedrock_embeddings,\n",
    "    documents=docs,\n",
    "    collection_name=collection_name,\n",
    "    connection_string=connection_string,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick way\n",
    "You have the possibility to use the wrapper provided by LangChain which wraps around the Vector Store and takes input the LLM.\n",
    "This wrapper performs the following steps behind the scences:\n",
    "- Take the question as input\n",
    "- Create question embedding\n",
    "- Fetch relevant documents\n",
    "- Stuff the documents and the question into a prompt\n",
    "- Invoke the model with the prompt and generate the answer in a human readable manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores.pgvector import PGVector\n",
    "from langchain.indexes.vectorstore import VectorStoreIndexWrapper\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "query = \"Tell me the summary or key take away from AWS Well Architected  framework int bulletd points\"\n",
    "\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "\n",
    "Human: Use the following pieces of context to provide a detailed respone to the question at the end\n",
    "<context>\n",
    "{context}\n",
    "</context\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Assistant:\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "# print(\"Prompt template looks like: \", PROMPT)\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3}),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": PROMPT},\n",
    ")\n",
    "answer = qa({\"query\": query})\n",
    "print(answer[\"result\"])\n",
    "\n",
    "answer[\"source_documents\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
