{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88a5ab2f-d044-4956-b75b-7408d9c3e323",
   "metadata": {},
   "source": [
    "# Invoking a function or do-it-yourself Agents with Bedrock\n",
    "\n",
    "## Overview\n",
    "\n",
    "Conversational interfaces such as chatbots and virtual assistants can be used to enhance the user experience for your customers. These use natural language processing (NLP) and machine learning algorithms to understand and respond to user queries and can be used in a variety of applications, such as customer service, sales, and e-commerce, to provide quick and efficient responses to users. Usually they are augmented by fetching information from various channels such as websites, social media platforms, and messaging apps which involve a complex workflow as shown below\n",
    "\n",
    "\n",
    "### Chatbot using Amazon Bedrock\n",
    "\n",
    "![Amazon Bedrock - Agents Interface](../images/agents.jpg)\n",
    "\n",
    "\n",
    "### Use Cases\n",
    "\n",
    "1. **QA** - Respond to queries based on look ups\n",
    "2. **Contextual-aware chatbot** - Dialog turn by turn with external look ups\n",
    "\n",
    "### Framework for building functions/Agents with Amazon Bedrock\n",
    "In Conversational interfaces such as chatbots, it is highly important to remember previous interactions, both at a short term but also at a long term level. Further we need to enhance with external tools which are needed to complete the queries. There are many providers which help to create functions or agents. We will look at a vanila implementation for how to do it yourself\n",
    "\n",
    "### Building  - Key Elements\n",
    "\n",
    "The first process in a building a contextual-aware chatbot is to identify the tools which can be called by the LLM's. These can be in form of API's invocation or functions calling which can in turn execute code or process data and return the curated results back. \n",
    "\n",
    "Second process is the user request orchestration , interaction,  invoking and returning the results.\n",
    "\n",
    "All of this requires an element of taking action or looking up data dynamically such as invoking an API in the contextual based chatbot. To demonstrate this feature we will build a weather application, which will accept a user input of a place and it will return the weather in real time.\n",
    "\n",
    "### Architecture [Weather lookup]\n",
    "We Search and look for the Latitude and Longitude and then invoke the weather app to get predictions\n",
    "\n",
    "![Amazon Bedrock - Agents Interface](./images/weather.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7cced6",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup `boto3` Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85063bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "from IPython.display import Markdown, display, Pretty\n",
    "\n",
    "# change to True if running locally and not in sagemaker studio\n",
    "local = False\n",
    "if local:\n",
    "    boto3.setup_default_session(profile_name=os.environ['AWS_PROFILE'])\n",
    "\n",
    "region = os.environ.get(\"AWS_REGION\")\n",
    "boto3_bedrock = boto3.client(\n",
    "    service_name='bedrock-runtime',\n",
    "    region_name=region,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909f3e58",
   "metadata": {},
   "source": [
    "---\n",
    "## Defining the API Tools\n",
    "\n",
    "The first thing we need to do for our LLM is define the tools it has access to. In this case we will be defining local Python functions, but it important to not that these could be any type of application service. Examples of what these tools might be on AWS include...\n",
    "\n",
    "* An AWS Lambda function\n",
    "* An Amazon RDS database connection\n",
    "* An Amazon DynamnoDB table\n",
    "  \n",
    "More generic examples include...\n",
    "\n",
    "* REST APIs\n",
    "* Data warehouses, data lakes, and databases\n",
    "* Computation engines\n",
    "\n",
    "In this case, we define two tools which reach external APIs below with two python functions\n",
    "1. the ability to retrieve the latitude and longitude of a place given a natural language input\n",
    "2. the ability to retrieve the weather given an input latitude and longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bb0dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_weather(latitude: str, longitude: str):\n",
    "    url = f\"https://api.open-meteo.com/v1/forecast?latitude={latitude}&longitude={longitude}&current_weather=true\"\n",
    "    response = requests.get(url)\n",
    "    return response.json()\n",
    "\n",
    "def get_lat_long(place: str):\n",
    "    url = \"https://nominatim.openstreetmap.org/search\"\n",
    "    params = {'q': place, 'format': 'json', 'limit': 1}\n",
    "    response = requests.get(url, params=params).json()\n",
    "    if response:\n",
    "        lat = response[0][\"lat\"]\n",
    "        lon = response[0][\"lon\"]\n",
    "        return {\"latitude\": lat, \"longitude\": lon}\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def call_function(tool_name, parameters):\n",
    "    func = globals()[tool_name]\n",
    "    #print(func, tool_name, parameters)\n",
    "    output = func(**parameters)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b2d1c0",
   "metadata": {},
   "source": [
    "We also define a function called `call_function` which is used to abstract the tool name. You can see an example of determining the weather in Las Vegas below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92319d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "place = 'Las Vegas'\n",
    "lat_long_response = call_function('get_lat_long', {'place' : place})\n",
    "weather_response = call_function('get_weather', lat_long_response)\n",
    "print(f'Weather in {place} is...')\n",
    "weather_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18203223",
   "metadata": {},
   "source": [
    "As you might expect, we have to describe our tools to our LLM, so it knows how to use them. The strings below describe the python functions for lat/long and weather to Claude models which can be Claude v-2.0 or Claude Instance v-1.0 in an XML friendly format which we have seen previously in the workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb27ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_weather_description = \"\"\"\\\n",
    "<tool_description>\n",
    "<tool_name>get_weather</tool_name>\n",
    "<parameters>\n",
    "<name>latitude</name>\n",
    "<name>longitude</name>\n",
    "</parameters>\n",
    "</tool_description>\n",
    "\"\"\"\n",
    "\n",
    "get_lat_long_description = \"\"\"\n",
    "<tool_description>\n",
    "<tool_name>get_lat_long</tool_name>\n",
    "<parameters>\n",
    "<name>place</name>  \n",
    "</parameters>\n",
    "</tool_description>\"\"\"\n",
    "\n",
    "list_of_tools_specs = [get_weather_description, get_lat_long_description]\n",
    "tools_string = ''.join(list_of_tools_specs)\n",
    "print(tools_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fac8ab",
   "metadata": {},
   "source": [
    "---\n",
    "## Define Prompts to Orchestrate our LLM Using Tools\n",
    "\n",
    "Now that the tools are defined both programmatically and as a string, we can start orchestrating the flow which will answer user questions. The first step to this is creating a prompt which defines the rules of operation for Claude. In the prompt below, we provide explicit direction on how Claude should use tools to answer these questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edc9104",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "TOOL_TEMPLATE = \"\"\"\\\n",
    "Your job is to formulate a solution to a given <user-request> based on the instructions and tools below.\n",
    "\n",
    "Use these Instructions: \n",
    "1. In this environment you have access to a set of tools and functions you can use to answer the question.\n",
    "2. You can call the functions by using the <function_calls> format below.\n",
    "3. Only invoke one function at a time and wait for the results before invoking another function.\n",
    "4. The Results of the function will be in xml tag <function_results>. Never make these up. The values will be provided for you.\n",
    "5. Only use the information in the <function_results> to answer the question.\n",
    "6. Once you truly know the answer to the question, place the answer in <answer></answer> tags. Make sure to answer in a full sentence which is friendly.\n",
    "7. Never ask any questions\n",
    "\n",
    "<function_calls>\n",
    "<invoke>\n",
    "<tool_name>$TOOL_NAME</tool_name>\n",
    "<parameters>\n",
    "<$PARAMETER_NAME>$PARAMETER_VALUE</$PARAMETER_NAME>\n",
    "...\n",
    "</parameters>\n",
    "</invoke>\n",
    "</function_calls>\n",
    "\n",
    "Here are the tools available:\n",
    "<tools>\n",
    "{tools_string}\n",
    "</tools>\n",
    "\n",
    "<user-request>\n",
    "{user_input}\n",
    "</user-request>\n",
    "\n",
    "Human: What is the first step in order to solve this problem?\n",
    "\n",
    "Assistant:\n",
    "\"\"\"\n",
    "TOOL_PROMPT = PromptTemplate.from_template(TOOL_TEMPLATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bdbf85",
   "metadata": {},
   "source": [
    "---\n",
    "## Executing the RAG Workflow\n",
    "\n",
    "Armed with our prompt and structured tools, we can now write an orchestration function which will iteratively step through the logical tasks to answer a user question. In the cell below we use the `invoke_model` function to generate a response with Claude and the `single_agent_step` function to iteratively call tools when the LLM tells us we need to. The general flow works like this...\n",
    "\n",
    "1. The user enters an input to the application\n",
    "2. The user input is merged with the original prompt and sent to the LLM to determine the next step\n",
    "3. If the LLM knows the answer, it will answer and we are done. If not, go to next step 4.\n",
    "4. The LLM will determine which tool to use to answer the question.\n",
    "5. We will use the tool as directed by the LLM and retrieve the results.\n",
    "6. We provide the results back into the original prompt as more context.\n",
    "7. We ask the LLM the next step or if knows the answer.\n",
    "8. Return to step 3.\n",
    "\n",
    "This can be a bit over whelming so let us walk through this flow in an example shortly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59df5a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xmltodict\n",
    "import json\n",
    "\n",
    "def invoke_model(prompt):\n",
    "    client = boto3.client(service_name='bedrock-runtime', region_name=os.environ.get(\"AWS_REGION\"),)\n",
    "    body = json.dumps({\"prompt\": prompt, \"max_tokens_to_sample\": 500, \"temperature\": 0,})\n",
    "    modelId = \"anthropic.claude-v2\"\n",
    "    #modelId = \"anthropic.claude-instant-v1\"\n",
    "    response = client.invoke_model(\n",
    "        body=body, modelId=modelId, accept=\"application/json\", contentType=\"application/json\"\n",
    "    )\n",
    "    return json.loads(response.get(\"body\").read()).get(\"completion\")\n",
    "\n",
    "def single_agent_step(prompt, output):\n",
    "\n",
    "    # first check if the model has answered the question\n",
    "    done = False\n",
    "    if '<answer>' in output:\n",
    "        answer = output.split('<answer>')[1]\n",
    "        answer = answer.split('</answer>')[0]\n",
    "        done = True\n",
    "        return done, answer\n",
    "    \n",
    "    # if the model has not answered the question, go execute a function\n",
    "    else:\n",
    "\n",
    "        # parse the output for any \n",
    "        function_xml = output.split('<function_calls>')[1]\n",
    "        function_xml = function_xml.split('</function_calls>')[0]\n",
    "        function_dict = xmltodict.parse(function_xml)\n",
    "        func_name = function_dict['invoke']['tool_name']\n",
    "        parameters = function_dict['invoke']['parameters']\n",
    "\n",
    "        #print(f\"single_agent_step:: func_name={func_name}::params={parameters}::function_dict={function_dict}::\")\n",
    "        # call the function which was parsed\n",
    "        func_response = call_function(func_name, parameters)\n",
    "\n",
    "        # create the next human input\n",
    "        func_response_str = '\\n\\nHuman: Here is the result from your function call\\n\\n'\n",
    "        func_response_str = func_response_str + f'<function_results>\\n{func_response}\\n</function_results>'\n",
    "        func_response_str = func_response_str + '\\n\\nIf you know the answer, say it. If not, what is the next step?\\n\\nAssistant:'\n",
    "\n",
    "        # augment the prompt\n",
    "        prompt = prompt + output + func_response_str\n",
    "    return done, prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d569ad",
   "metadata": {},
   "source": [
    "Let's start our first example `What is the weather in Las Vegas?`. The code below asks the LLM what the first step is and you will notice that the LLM is able to ascertain it needs to use the `get_lat_long` tool first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce14d5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = 'What is the weather in Las Vegas?'\n",
    "next_step = TOOL_PROMPT.format(tools_string=tools_string, user_input=user_input)\n",
    "\n",
    "output = invoke_model(next_step).strip()\n",
    "done, next_step = single_agent_step(next_step, output)\n",
    "if not done:\n",
    "    display(Pretty(f'{output}'))\n",
    "else:\n",
    "    display(Pretty('Final answer from LLM:\\n'+f'{next_step}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f27a09",
   "metadata": {},
   "source": [
    "Great, Claude has figured out that we should first call the lat and long tool. The next step is then orchestrated just like the first. This time, Claude uses the lat/long from the first request to now ask for the weather of that specific location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f62b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = invoke_model(next_step).strip()\n",
    "done, next_step = single_agent_step(next_step, output)\n",
    "if not done:\n",
    "    display(Pretty(f'{output}'))\n",
    "else:\n",
    "    display(Pretty('Final answer from LLM:\\n'+f'{next_step}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98eb8dcd",
   "metadata": {},
   "source": [
    "Finally the LLM is able to answer the question based on the input function above. Very cool!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56361993",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = invoke_model(next_step).strip()\n",
    "done, next_step = single_agent_step(next_step, output)\n",
    "if not done:\n",
    "    display(Pretty(f'{output}'))\n",
    "else:\n",
    "    display(Pretty('Final answer from LLM:\\n'+f'{next_step}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d64a45a",
   "metadata": {},
   "source": [
    "Let's try another example to show how a different place (Singapore) can be used in this example. Notice how we set the for loop to 5 iterations even though the model only uses 3 of these. This iteration capping is common in agent workflows and should be tuned according to your use case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ff52cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = 'What is the weather in Singapore?'\n",
    "next_step = TOOL_PROMPT.format(tools_string=tools_string, user_input=user_input)\n",
    "\n",
    "for i in range(5):\n",
    "    output = invoke_model(next_step).strip()\n",
    "    done, next_step = single_agent_step(next_step, output)\n",
    "    if not done:\n",
    "        display(Pretty(f'{output}'))\n",
    "    else:\n",
    "        display(Pretty('Final answer from LLM:\\n'+f'{next_step}'))\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a48a0e8-147d-4525-a6b2-68a09af1b2c4",
   "metadata": {},
   "source": [
    "---\n",
    "## Leveraging Open Source Agents with Bedrock\n",
    "\n",
    "As with the previous notebook, there are many ways to orchestrate agent workflows with LLMs. One of the popular open source frameworks for this is [LangChain's agent module](https://python.langchain.com/docs/modules/agents/). LangChain provides a variety of agent types as well as the same [integration to Amazon Bedrock](https://python.langchain.com/docs/integrations/llms/bedrock) being available for these agent based workflows. Let's work on implementing the same workflow as above into the LangChain ecosystem.\n",
    "\n",
    "First, we can define our LLM with the `Bedrock` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51045678",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms.bedrock import Bedrock\n",
    "\n",
    "agent_llm = Bedrock(\n",
    "    client=boto3.client(service_name='bedrock-runtime', region_name=os.environ.get(\"AWS_REGION\"),),\n",
    "    model_id=\"anthropic.claude-instant-v1\",\n",
    "    model_kwargs={\n",
    "        \"max_tokens_to_sample\": 500,\n",
    "        \"temperature\": 0.0,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27832b5",
   "metadata": {},
   "source": [
    "Now we will use the same python code as our original implementation, but we will use the `@tool` decorator to specify that this python function is able to be used as tool for LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9f914a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "import ast\n",
    "\n",
    "@tool (\"get_lat_long\")\n",
    "def get_lat_long(place: str) -> dict:\n",
    "    \"\"\"Returns the latitude and longitude for a given place name as a dict object of python.\"\"\"\n",
    "    url = \"https://nominatim.openstreetmap.org/search\"\n",
    "\n",
    "    params = {'q': place, 'format': 'json', 'limit': 1}\n",
    "    response = requests.get(url, params=params).json()\n",
    "\n",
    "    if response:\n",
    "        lat = response[0][\"lat\"]\n",
    "        lon = response[0][\"lon\"]\n",
    "        return {\"latitude\": lat, \"longitude\": lon}\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "@tool (\"get_weather_dict\")\n",
    "def get_weather_dict(co_ord_str: str) -> dict:\n",
    "    \"\"\"\n",
    "    Returns weather data for a given a dictionary having latitude and longitude.\n",
    "    \"\"\"\n",
    "    if '{' in co_ord_str:\n",
    "        co_ord_str = ast.literal_eval(co_ord_str)\n",
    "        latitude = co_ord_str['latitude']\n",
    "        longitude = co_ord_str['longitude']\n",
    "    else:\n",
    "        latitude = float(co_ord_str.split(\",\")[0].strip())\n",
    "        longitude = float(co_ord_str.split(\",\")[1].strip())\n",
    "    url = f\"https://api.open-meteo.com/v1/forecast?latitude={latitude}&longitude={longitude}&current_weather=true\"\n",
    "    response = requests.get(url)\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f036aea7",
   "metadata": {},
   "source": [
    "As well as being able to define our own tools, LangChain has some pre-built tools which are general purpose. One well documented issue with LLMs is that they are not good at arithmetic only from natural language. We can use the `LLMMathChain` from LangChain to help solve this problem. In this chain, we are able to ask an LLM simple math questions and the question will be sent to a calculator in order to return a perfect mathematical result. An example is included below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732b97ec-efd5-408c-8566-2a2ecb1bcb9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain import LLMMathChain\n",
    "\n",
    "math_tool_template = \"\"\"\\\n",
    "Given a question from the Human with a math problem, provide only a single line mathematical expression that solves the problem in the following format.\n",
    "Never solve the expression only create a parsable expression.\n",
    "```text\n",
    "${{single line mathematical expression that solves the problem}}\n",
    "```\n",
    "\n",
    "Here is an example response with a single line mathematical expression for solving a math problem:\n",
    "```text\n",
    "37593**(1/5)\n",
    "```\n",
    "\n",
    "Human: {question}\n",
    "\n",
    "Assistant:\"\"\"\n",
    "\n",
    "llm_math_chain = LLMMathChain.from_llm(llm=agent_llm, verbose=True)\n",
    "llm_math_chain.llm_chain.prompt.template = math_tool_template\n",
    "result = llm_math_chain('What is nine times 465?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013bd01d-0b25-4512-afa6-b4b73bed9391",
   "metadata": {},
   "source": [
    "Now that all the tools are defined, lets create a [ReACT agent](https://python.langchain.com/docs/modules/agents/agent_types/react) (Reason + Act) which will leverage the LLM to generate the next set of actions to solve a given question. This next cell creates the agent from a list of tools we provide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b746c5a7-a245-4479-9db0-9c27c02a77c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.agents import AgentType\n",
    "\n",
    "tools_list = [get_lat_long, get_weather_dict]\n",
    "tools_list.append(\n",
    "    Tool.from_function(\n",
    "        func=llm_math_chain.run,\n",
    "        name=\"Calculator\",\n",
    "        description=\"Useful for when you need to answer questions about math.\",\n",
    "    )\n",
    ")\n",
    "react_agent = initialize_agent(\n",
    "    tools_list, \n",
    "    agent_llm, \n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, \n",
    "    verbose=True,\n",
    "    max_iteration=4,\n",
    "    return_intermediate_steps=True,\n",
    "    handle_parsing_errors=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc13f6ee-8b0b-4f26-98b6-93b623e41191",
   "metadata": {},
   "source": [
    "The ReAct framework also expects a specific format to be followed by our LLM. This custom prompt adds place holders for the 3 components which need to be followed.\n",
    "\n",
    "1. **Action** which is the tool to use\n",
    "2. **Action Input** is the parameters coming from the LLM which will go into the tool or function to be called\n",
    "3. **Observation** is the final result of the call from the tool\n",
    "\n",
    "These three components to ReAct are recursively executed until there is an answer to the question or we reach the limit of execution steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a020364-e963-4a8e-8504-4cc4932a9916",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"Use the following format:\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do, Also try to follow steps mentioned above\n",
    "Action: the action to take, should be one of [ \"get_lat_long\", \"get_weather_dict\", \"Calculator\"]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Question: {input}\n",
    "\n",
    "Assistant:\n",
    "{agent_scratchpad}\"\"\"\n",
    "\n",
    "react_agent.agent.llm_chain.prompt.template=prompt_template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca1c4b0-a54d-4949-acd8-9110a54e5982",
   "metadata": {},
   "source": [
    "Now we can send in the first question just like the one we used in the DIY system. Notice how the logging allows you to see how the LLM is reasoning through the task within the ReAct framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810ebdff-2ec0-4976-92d0-bd3af2429a95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result = react_agent(\"Can you check the weather in Las Vegas for me?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12682bc0-1bca-41be-82d2-784cccb870a6",
   "metadata": {},
   "source": [
    "Another example is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e52ec2a-8f28-4298-ae52-7e853be8d619",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result = react_agent(\"can you check the weather in Seattle WA for me?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ddf792-b5b0-4519-b665-7de02ad932cb",
   "metadata": {},
   "source": [
    "In this last example, we will give a different type of problem to the LLM where it will be able to take advantage of the math tool as well as the lat/long tool instead of only talking about weather."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c06be60-4d70-489a-ad09-69c0977aed7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result = react_agent(\"what is the latitude of Washington DC multiplied by 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70b217d",
   "metadata": {},
   "source": [
    "---\n",
    "## Next steps\n",
    "\n",
    "Now that we have a working RAG application with vector search retrieval, we will explore a new type of retrieval. In the next notebook we will see how to use LLM agents to automatically retrieve information from APIs."
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "chat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
