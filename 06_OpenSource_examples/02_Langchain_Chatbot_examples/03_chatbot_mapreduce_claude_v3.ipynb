{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversational Interface - Chatbot with Claude LLM\n",
    "\n",
    "> *This notebook should work well with the **`Data Science 3.0`** kernel in SageMaker Studio*\n",
    "\n",
    "In this notebook, we will build a chatbot using the Foundation Models (FMs) in Amazon Bedrock. For our use-case we use Claude V3 Sonnet as our foundation models.  For more details refer to [Documentation](https://aws.amazon.com/bedrock/claude/). The ideal balance between intelligence and speed—particularly for enterprise workloads. It excels at complex reasoning, nuanced content creation, scientific queries, math, and coding. Data teams can use Sonnet for RAG, as well as search and retrieval across vast amounts of information while sales teams can leverage Sonnet for product recommendations, forecasting, and targeted marketing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Conversational interfaces such as chatbots and virtual assistants can be used to enhance the user experience for your customers.Chatbots uses natural language processing (NLP) and machine learning algorithms to understand and respond to user queries. Chatbots can be used in a variety of applications, such as customer service, sales, and e-commerce, to provide quick and efficient responses to users. They can be accessed through various channels such as websites, social media platforms, and messaging apps.\n",
    "\n",
    "### This notebook goes into the details of how to speed up the responses for large contexts and data sources\n",
    "\n",
    "\n",
    "## Chatbot using Amazon Bedrock\n",
    "\n",
    "![Amazon Bedrock - Conversational Interface](./images/chatbot_bedrock.png)\n",
    "\n",
    "\n",
    "## Use Cases\n",
    "\n",
    "1. **Chatbot (Basic)** - Zero Shot chatbot with a FM model\n",
    "2. **Chatbot using prompt** - template(Langchain) - Chatbot with some context provided in the prompt template\n",
    "3. **Chatbot with persona** - Chatbot with defined roles. i.e. Career Coach and Human interactions\n",
    "4. **Contextual-aware chatbot** - Passing in context through an external file by generating embeddings.\n",
    "\n",
    "## Langchain framework for building Chatbot with Amazon Bedrock\n",
    "In Conversational interfaces such as chatbots, it is highly important to remember previous interactions, both at a short term but also at a long term level.\n",
    "\n",
    "LangChain provides memory components in two forms. First, LangChain provides helper utilities for managing and manipulating previous chat messages. These are designed to be modular and useful regardless of how they are used. Secondly, LangChain provides easy ways to incorporate these utilities into chains.\n",
    "It allows us to easily define and interact with different types of abstractions, which make it easy to build powerful chatbots.\n",
    "\n",
    "## Building Chatbot with Context - Key Elements\n",
    "\n",
    "The first process in a building a contextual-aware chatbot is to **generate embeddings** for the context. Typically, you will have an ingestion process which will run through your embedding model and generate the embeddings which will be stored in a sort of a vector store. In this example we are using Titan Embeddings model for this\n",
    "\n",
    "![Embeddings](./images/embeddings_lang.png)\n",
    "\n",
    "Second process is the user request orchestration , interaction,  invoking and returing the results\n",
    "\n",
    "![Chatbot](./images/chatbot_lang.png)\n",
    "\n",
    "## Architecture [Context Aware Chatbot]\n",
    "![4](./images/context-aware-chatbot.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "⚠️ ⚠️ ⚠️ Before running this notebook, ensure you've  run these installs below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain==0.1.17\n",
    "#!pip install langchain-anthropic\n",
    "#!pip install boto3==1.34.95\n",
    "#!pip install faiss-cpu==1.8.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To install the langchain-aws\n",
    "\n",
    "you can run the `pip install langchain-aws`\n",
    "\n",
    "to get the latest release use these commands below"
   ]
  },
  {
   "cell_type": "raw",
   "id": "01HWZSNZHVAB8KYYEGCM5C0BVE",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "#- run them from a terminal on your machine\n",
    "cd ~\n",
    "mkdir temp_t\n",
    "cd temp_t\n",
    "git clone https://github.com/langchain-ai/langchain-aws/\n",
    "pip install ./langchain-aws/libs/aws/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "from io import StringIO\n",
    "import sys\n",
    "import textwrap\n",
    "import os\n",
    "from typing import Optional\n",
    "\n",
    "# External Dependencies:\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "def print_ww(*args, width: int = 100, **kwargs):\n",
    "    \"\"\"Like print(), but wraps output to `width` characters (default 100)\"\"\"\n",
    "    buffer = StringIO()\n",
    "    try:\n",
    "        _stdout = sys.stdout\n",
    "        sys.stdout = buffer\n",
    "        print(*args, **kwargs)\n",
    "        output = buffer.getvalue()\n",
    "    finally:\n",
    "        sys.stdout = _stdout\n",
    "    for line in output.splitlines():\n",
    "        print(\"\\n\".join(textwrap.wrap(line, width=width)))\n",
    "\n",
    "\n",
    "def get_bedrock_client(\n",
    "    assumed_role: Optional[str] = None,\n",
    "    region: Optional[str] = None,\n",
    "    runtime: Optional[bool] = True,\n",
    "):\n",
    "    \"\"\"Create a boto3 client for Amazon Bedrock, with optional configuration overrides\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    assumed_role :\n",
    "        Optional ARN of an AWS IAM role to assume for calling the Bedrock service. If not\n",
    "        specified, the current active credentials will be used.\n",
    "    region :\n",
    "        Optional name of the AWS Region in which the service should be called (e.g. \"us-east-1\").\n",
    "        If not specified, AWS_REGION or AWS_DEFAULT_REGION environment variable will be used.\n",
    "    runtime :\n",
    "        Optional choice of getting different client to perform operations with the Amazon Bedrock service.\n",
    "    \"\"\"\n",
    "    if region is None:\n",
    "        target_region = os.environ.get(\n",
    "            \"AWS_REGION\", os.environ.get(\"AWS_DEFAULT_REGION\")\n",
    "        )\n",
    "    else:\n",
    "        target_region = region\n",
    "\n",
    "    print(f\"Create new client\\n  Using region: {target_region}\")\n",
    "    session_kwargs = {\"region_name\": target_region}\n",
    "    client_kwargs = {**session_kwargs}\n",
    "\n",
    "    profile_name = os.environ.get(\"AWS_PROFILE\")\n",
    "    if profile_name:\n",
    "        print(f\"  Using profile: {profile_name}\")\n",
    "        session_kwargs[\"profile_name\"] = profile_name\n",
    "\n",
    "    retry_config = Config(\n",
    "        region_name=target_region,\n",
    "        retries={\n",
    "            \"max_attempts\": 10,\n",
    "            \"mode\": \"standard\",\n",
    "        },\n",
    "    )\n",
    "    session = boto3.Session(**session_kwargs)\n",
    "\n",
    "    if assumed_role:\n",
    "        print(f\"  Using role: {assumed_role}\", end=\"\")\n",
    "        sts = session.client(\"sts\")\n",
    "        response = sts.assume_role(\n",
    "            RoleArn=str(assumed_role), RoleSessionName=\"langchain-llm-1\"\n",
    "        )\n",
    "        print(\" ... successful!\")\n",
    "        client_kwargs[\"aws_access_key_id\"] = response[\"Credentials\"][\"AccessKeyId\"]\n",
    "        client_kwargs[\"aws_secret_access_key\"] = response[\"Credentials\"][\n",
    "            \"SecretAccessKey\"\n",
    "        ]\n",
    "        client_kwargs[\"aws_session_token\"] = response[\"Credentials\"][\"SessionToken\"]\n",
    "\n",
    "    if runtime:\n",
    "        service_name = \"bedrock-runtime\"\n",
    "    else:\n",
    "        service_name = \"bedrock\"\n",
    "\n",
    "    bedrock_client = session.client(\n",
    "        service_name=service_name, config=retry_config, **client_kwargs\n",
    "    )\n",
    "\n",
    "    print(\"boto3 Bedrock client successfully created!\")\n",
    "    print(bedrock_client._endpoint)\n",
    "    return bedrock_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import boto3\n",
    "\n",
    "\n",
    "# ---- ⚠️ Un-comment and edit the below lines as needed for your AWS setup ⚠️ ----\n",
    "\n",
    "# os.environ[\"AWS_DEFAULT_REGION\"] = \"<REGION_NAME>\"  # E.g. \"us-east-1\"\n",
    "# os.environ[\"AWS_PROFILE\"] = \"<YOUR_PROFILE>\"\n",
    "# os.environ[\"BEDROCK_ASSUME_ROLE\"] = \"<YOUR_ROLE_ARN>\"  # E.g. \"arn:aws:...\"\n",
    "\n",
    "\n",
    "boto3_bedrock = get_bedrock_client(\n",
    "    assumed_role=os.environ.get(\"BEDROCK_ASSUME_ROLE\", None),\n",
    "    region=\"us-west-2\",  # os.environ.get(\"AWS_DEFAULT_REGION\", None),\n",
    "    runtime=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anthropic Claude\n",
    "\n",
    "#### Input\n",
    "\n",
    "```json\n",
    "\n",
    "\"messages\": [\n",
    "    {\"role\": \"user\", \"content\": \"Hello, Claude\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Hello!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Can you describe LLMs to me?\"}\n",
    "        \n",
    "]\n",
    "{\n",
    "    \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "    \"max_tokens\": 100,\n",
    "    \"messages\": messages,\n",
    "    \"temperature\": 0.5,\n",
    "    \"top_p\": 0.9\n",
    "} \n",
    "```\n",
    "\n",
    "#### Output\n",
    "\n",
    "```json\n",
    "{\n",
    "    'id': 'msg_01T',\n",
    "    'type': 'message',\n",
    "    'role': 'assistant',\n",
    "    'content': [\n",
    "        {\n",
    "            'type': 'text',\n",
    "            'text': 'Sure, the concept...'\n",
    "        }\n",
    "    ],\n",
    "    'model': 'model_id',\n",
    "    'stop_reason': 'max_tokens',\n",
    "    'stop_sequence': None,\n",
    "    'usage': {'input_tokens':xy, 'output_tokens': yz}}\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Chatbot (Basic - without context)\n",
    "\n",
    "This is bare bone boto3 calls examples and samples\n",
    "\n",
    "**Note:** The model outputs are non-deterministic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import BedrockChat\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelId = \"anthropic.claude-3-sonnet-20240229-v1:0\"  # \"anthropic.claude-v2\"\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"type\": \"text\", \"text\": \"What is quantum mechanics? \"}],\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": \"It is a branch of physics that describes how matter and energy interact with discrete energy values \",\n",
    "            }\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": \"Can you explain a bit more about discrete energies?\",\n",
    "            }\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "body = json.dumps(\n",
    "    {\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"max_tokens\": 100,\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": 0.5,\n",
    "        \"top_p\": 0.9,\n",
    "    }\n",
    ")\n",
    "\n",
    "response = boto3_bedrock.invoke_model(body=body, modelId=modelId)\n",
    "response_body = json.loads(response.get(\"body\").read())\n",
    "print(response_body)\n",
    "\n",
    "\n",
    "def test_sample_claude_invoke(prompt_str, boto3_bedrock):\n",
    "    modelId = \"anthropic.claude-3-sonnet-20240229-v1:0\"  # \"anthropic.claude-v2\"\n",
    "    messages = [{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": prompt_str}]}]\n",
    "    body = json.dumps(\n",
    "        {\n",
    "            \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "            \"max_tokens\": 100,\n",
    "            \"messages\": messages,\n",
    "            \"temperature\": 0.5,\n",
    "            \"top_p\": 0.9,\n",
    "        }\n",
    "    )\n",
    "    response = boto3_bedrock.invoke_model(body=body, modelId=modelId)\n",
    "    response_body = json.loads(response.get(\"body\").read())\n",
    "    return response_body\n",
    "\n",
    "\n",
    "test_sample_claude_invoke(\"what is quantum mechanics\", boto3_bedrock)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use BedrockChat class as a bare bones samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(dir(cl_llm))\n",
    "\n",
    "modelId = \"anthropic.claude-3-sonnet-20240229-v1:0\"  # \"anthropic.claude-v2\"\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"type\": \"text\", \"text\": \"What is quantum mechanics? \"}],\n",
    "    },\n",
    "]\n",
    "body_json = json.dumps(\n",
    "    {\n",
    "        # \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        # \"max_tokens\": 100,\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": 0.5,\n",
    "        \"top_p\": 0.9,\n",
    "    }\n",
    ")\n",
    "cl_llm = BedrockChat(\n",
    "    model_id=modelId,\n",
    "    client=boto3_bedrock,\n",
    "    # model_kwargs={\"max_tokens_to_sample\": 100},\n",
    "    model_kwargs={\"temperature\": 0.1, \"max_tokens\": 100},\n",
    ")\n",
    "cl_llm.predict(body_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OPTION 1: Use the Vector DB \n",
    "\n",
    "We use a vector DB to create a curated and simplified content and context . Thi will help to return the curated responses\n",
    "\n",
    "This works to reduce the over all latency\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain provides several classes and functions to make constructing and working with prompts easy. We are going to use the [PromptTemplate](https://python.langchain.com/en/latest/modules/prompts/getting_started.html) class to construct the prompt from a f-string template. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from langchain_community.document_loaders import TextLoader, PyPDFLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain.embeddings.bedrock import BedrockEmbeddings\n",
    "\n",
    "br_embeddings = BedrockEmbeddings(\n",
    "    model_id=\"amazon.titan-embed-text-v1\", client=boto3_bedrock\n",
    ")\n",
    "\n",
    "loader = PyPDFLoader(\n",
    "    \"./rag_data/Amazon-com-Inc-2023-Shareholder-Letter.pdf\"\n",
    ")  # --- > 219 docs with 400 chars, each row consists in a question column and an answer column\n",
    "documents_aws = loader.load()  #\n",
    "print(f\"Number of documents={len(documents_aws)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "docs = CharacterTextSplitter(\n",
    "    chunk_size=2000, chunk_overlap=400, separator=\"\\n\"\n",
    ").split_documents(\n",
    "    documents_aws\n",
    ")  # -  separator=\",\"\n",
    "\n",
    "print(f\"Number of documents after split and chunking={len(docs)}\")\n",
    "vectorstore_faiss_aws = None\n",
    "\n",
    "\n",
    "vectorstore_faiss_aws = FAISS.from_documents(documents=docs, embedding=br_embeddings)\n",
    "\n",
    "print(\n",
    "    f\"vectorstore_faiss_aws: number of elements in the index={vectorstore_faiss_aws.index.ntotal}::\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "boto3_bedrock = get_bedrock_client(\n",
    "    assumed_role=os.environ.get(\"BEDROCK_ASSUME_ROLE\", None),\n",
    "    region=\"us-west-2\",\n",
    "    runtime=True,\n",
    ")\n",
    "\n",
    "# turn verbose to true to see the full logs and documents\n",
    "modelId = \"anthropic.claude-3-sonnet-20240229-v1:0\"  # \"anthropic.claude-v2\"\n",
    "cl_llm = BedrockChat(\n",
    "    model_id=modelId,\n",
    "    client=boto3_bedrock,\n",
    "    model_kwargs={\"temperature\": 0.1, \"max_tokens\": 100},\n",
    ")\n",
    "\n",
    "br_embeddings = BedrockEmbeddings(\n",
    "    model_id=\"amazon.titan-embed-text-v1\", client=boto3_bedrock\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### With Langchain chain and sending in the full context or leverage the Vector DB\n",
    "\n",
    "showing the Streaming and Non Streaming options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "system_message = \"\"\"\n",
    "System: Use the following portion of the long document below in Context to see if any of the text is relevant to answer the question. Return any relevant text verbatim. \n",
    "Use the relevant text returned and the question to create a final answer.\n",
    "Context: {context}\n",
    "\"\"\"\n",
    "human_message = \"{text}\"\n",
    "\n",
    "messages = [(\"system\", system_message), (\"human\", human_message)]\n",
    "\n",
    "context_doc = CharacterTextSplitter(\n",
    "    chunk_size=2000, chunk_overlap=0, separator=\"\\n\"\n",
    ").split_documents(\n",
    "    documents_aws\n",
    ")  # -  separator=\",\"\n",
    "# context_doc = FAISS.from_documents(documents=docs, embedding = br_embeddings).similarity_search_by_vector(br_embeddings.embed_query(\"How did Amazon’s Advertising work do?\"), k=4)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(messages)\n",
    "\n",
    "chain = prompt | cl_llm | StrOutputParser()\n",
    "chain_input = {\n",
    "    \"context\": context_doc,  # \"This is a sample context doc\", #context_doc,\n",
    "    \"text\": \"How did Amazon’s Advertising work do?\",\n",
    "}\n",
    "\n",
    "start_time = time.time()\n",
    "print(\"\\n starting:: Streaming:\\n\")\n",
    "for chunk in chain.stream(chain_input):\n",
    "\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "\n",
    "print(f\"\\n\\nComplete: Results:returned in {time.time()-start_time}:: seconds\")\n",
    "\n",
    "print(\"\\n starting::NON-STREAMING::\\n\")\n",
    "start_time = time.time()\n",
    "chain.invoke(chain_input)\n",
    "print(\n",
    "    f\"\\n\\nComplete: NON-STREAMING:Results:returned in {time.time()-start_time}:: seconds\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 2 with Map-Reduce with Asyncio\n",
    "\n",
    "langchain map-reduce is still sequential. Run with MAP reduce but custom template \n",
    "\n",
    "**Here we will run the MAP in parallel and then reduce post all map completed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import BedrockEmbeddings\n",
    "\n",
    "# break down into smaller chunks\n",
    "\n",
    "context_doc = CharacterTextSplitter(\n",
    "    chunk_size=2000, chunk_overlap=0, separator=\"\\n\"\n",
    ").split_documents(\n",
    "    documents_aws\n",
    ")  # -  separator=\",\"\n",
    "print(\n",
    "    f\"len:context:doc={len(context_doc)}\",\n",
    "    f\"length characters of the doc={len(context_doc[0].page_content)}\",\n",
    "    f\"approx tokens = {len(context_doc[0].page_content)/4}\",\n",
    ")\n",
    "\n",
    "context_doc_list = []\n",
    "for each_doc in context_doc:\n",
    "    context_doc_list.append(each_doc.page_content)\n",
    "    print(\n",
    "        f\"len:context:doc={len(each_doc.page_content)}:: tokens={len(each_doc.page_content)/4}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The CallbackHandler class is actually not needed, but shown for completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "from langchain.callbacks.base import AsyncCallbackHandler, BaseCallbackHandler\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.outputs import LLMResult\n",
    "import boto3\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.chat_models import BedrockChat\n",
    "from langchain.embeddings import BedrockEmbeddings\n",
    "\n",
    "boto3_bedrock = get_bedrock_client(\n",
    "    assumed_role=os.environ.get(\"BEDROCK_ASSUME_ROLE\", None),\n",
    "    region=\"us-west-2\",\n",
    "    runtime=True,\n",
    ")\n",
    "\n",
    "model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "\n",
    "model_kwargs = {\n",
    "    \"max_tokens\": 256,\n",
    "    \"temperature\": 0.0,\n",
    "    \"top_k\": 250,\n",
    "    \"top_p\": 1,\n",
    "    \"stop_sequences\": [\"\\n\\nHuman\"],\n",
    "}\n",
    "\n",
    "\n",
    "class MyCustomSyncHandler(BaseCallbackHandler):\n",
    "    def on_llm_new_token(self, token: str, **kwargs) -> None:\n",
    "        print(f\"Sync handler being called in a `thread_pool_executor`: token: {token}\")\n",
    "\n",
    "\n",
    "class MyCustomAsyncHandler(AsyncCallbackHandler):\n",
    "    \"\"\"Async callback handler that can be used to handle callbacks from langchain.\"\"\"\n",
    "\n",
    "    async def on_llm_start(\n",
    "        self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any\n",
    "    ) -> None:\n",
    "        \"\"\"Run when chain starts running.\"\"\"\n",
    "        class_name = serialized[\"name\"]\n",
    "        print(\n",
    "            f\"{class_name}:: Sonnet: calling for prompt:len={len(prompts[0])/4}:tokens::\"\n",
    "        )\n",
    "\n",
    "    async def on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:\n",
    "        \"\"\"Run when chain ends running.\"\"\"\n",
    "        # print(f\"Sonnet call ended response::{response}::\")\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_llm = BedrockChat(\n",
    "    client=boto3_bedrock,\n",
    "    model_id=model_id,\n",
    "    model_kwargs=model_kwargs,\n",
    "    callbacks=[MyCustomSyncHandler(), MyCustomAsyncHandler()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How did Amazon’s Advertising work do?\"\n",
    "\n",
    "\n",
    "def create_map_question(context_doc):\n",
    "    prompt_question = \"\"\"\n",
    "    System: Use the following portion of a long document to see if any of the text is relevant to answer the question. \n",
    "    Return the content verbatim if relevant. If not relevant then return 'NO'. Do not return anything else\n",
    "\n",
    "    Context:{context}\n",
    "\n",
    "    question: {question}\n",
    "    \"\"\".format(\n",
    "        question=question, context=context_doc\n",
    "    )\n",
    "    return prompt_question\n",
    "\n",
    "\n",
    "def parse_map_yes_no(map_call_list):\n",
    "    ret_list = []\n",
    "    for ai_message_ret_call in map_call_list:\n",
    "        print(ai_message_ret_call.content)\n",
    "        if ai_message_ret_call.content and not \"NO\" in ai_message_ret_call.content:\n",
    "            ret_list.append(ai_message_ret_call.content)\n",
    "\n",
    "    return ret_list\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "map_call_list = await asyncio.gather(\n",
    "    *[\n",
    "        model_llm.ainvoke(create_map_question(prompt_question))\n",
    "        for idx, prompt_question in enumerate(context_doc_list)\n",
    "    ]\n",
    ")\n",
    "map_call_list_new = parse_map_yes_no(map_call_list)\n",
    "print(f\"Time taken for MAP function::{time.time()-start_time}:seconds\")\n",
    "map_call_list_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(map_call_list_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How did Amazon’s Advertising work do?\"\n",
    "\n",
    "\n",
    "def create_map_question(context_doc):\n",
    "    prompt_question = \"\"\"\n",
    "    System: Use the following portion of a long document to answer the question below. If the context does not have the answer reply with 'i Do not know'. Do not replay with anything else\n",
    "\n",
    "    Context:{context}\n",
    "\n",
    "    question: {question}\n",
    "    \"\"\".format(\n",
    "        question=question, context=context_doc\n",
    "    )\n",
    "    return prompt_question\n",
    "\n",
    "\n",
    "final_context = create_map_question(\"\\n\".join(map_call_list_new))\n",
    "\n",
    "await model_llm.ainvoke(final_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "#### Summary template for MAP-reduce\n",
    "\n",
    "**Summary is returning every context back but summarized**\n",
    "\n",
    "create a chunk which is longer than the QA one - closer to like 4000 chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.chat_models import BedrockChat\n",
    "from langchain.embeddings import BedrockEmbeddings\n",
    "\n",
    "boto3_bedrock = get_bedrock_client(\n",
    "    assumed_role=os.environ.get(\"BEDROCK_ASSUME_ROLE\", None),\n",
    "    region=\"us-west-2\",\n",
    "    runtime=True,\n",
    ")\n",
    "\n",
    "model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "\n",
    "model_kwargs = {\n",
    "    \"max_tokens\": 256,\n",
    "    \"temperature\": 0.0,\n",
    "    \"top_k\": 250,\n",
    "    \"top_p\": 1,\n",
    "    \"stop_sequences\": [\"\\n\\nHuman\"],\n",
    "}\n",
    "\n",
    "\n",
    "# break down into smaller chunks\n",
    "\n",
    "context_doc = CharacterTextSplitter(\n",
    "    chunk_size=4000, chunk_overlap=0, separator=\"\\n\"\n",
    ").split_documents(\n",
    "    documents_aws\n",
    ")  # -  separator=\",\"\n",
    "print(\n",
    "    f\"len:context:doc={len(context_doc)}\",\n",
    "    f\"length characters of the doc={len(context_doc[0].page_content)}\",\n",
    "    f\"approx tokens = {len(context_doc[0].page_content)/4}\",\n",
    ")\n",
    "\n",
    "context_doc_list = []\n",
    "for each_doc in context_doc:\n",
    "    context_doc_list.append(each_doc.page_content)\n",
    "    print(\n",
    "        f\"len:context:doc={len(each_doc.page_content)}:: approx tokens={len(each_doc.page_content)/4}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "from langchain.callbacks.base import AsyncCallbackHandler, BaseCallbackHandler\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.outputs import LLMResult\n",
    "\n",
    "\n",
    "class MyCustomSyncHandler(BaseCallbackHandler):\n",
    "    def on_llm_new_token(self, token: str, **kwargs) -> None:\n",
    "        print(f\"Sync handler being called in a `thread_pool_executor`: token: {token}\")\n",
    "\n",
    "\n",
    "class MyCustomAsyncHandler(AsyncCallbackHandler):\n",
    "    \"\"\"Async callback handler that can be used to handle callbacks from langchain.\"\"\"\n",
    "\n",
    "    async def on_llm_start(\n",
    "        self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any\n",
    "    ) -> None:\n",
    "        \"\"\"Run when chain starts running.\"\"\"\n",
    "        # print(\"zzzz....\")\n",
    "        # await asyncio.sleep(0.3)\n",
    "        class_name = serialized[\"name\"]\n",
    "        print(\n",
    "            f\"{class_name}:: Sonnet: calling for prompt:len={len(prompts[0])/4}:tokens::\"\n",
    "        )\n",
    "\n",
    "    async def on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:\n",
    "        \"\"\"Run when chain ends running.\"\"\"\n",
    "        # print(f\"Sonnet call ended response::{response}::\")\n",
    "        # await asyncio.sleep(0.3)\n",
    "        # print(\"Hi! I just woke up. Your llm is ending\")\n",
    "        pass\n",
    "\n",
    "\n",
    "model_llm = BedrockChat(\n",
    "    client=boto3_bedrock,\n",
    "    model_id=model_id,\n",
    "    model_kwargs=model_kwargs,\n",
    "    callbacks=[MyCustomSyncHandler(), MyCustomAsyncHandler()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_map_question(context_doc):\n",
    "    prompt_question = \"\"\"\n",
    "    System: Summarize the following portion of the document below in Context in less than 50 words. Do not return anything else\n",
    "\n",
    "    Context:{context}\n",
    "\n",
    "    \"\"\".format(\n",
    "        context=context_doc\n",
    "    )\n",
    "    return prompt_question\n",
    "\n",
    "\n",
    "def parse_map_yes_no(map_call_list):\n",
    "    ret_list = []\n",
    "    for ai_message_ret_call in map_call_list:\n",
    "        print(ai_message_ret_call.content)\n",
    "        ret_list.append(\n",
    "            ai_message_ret_call.content\n",
    "        )  # this is summary so e have to include every response back\n",
    "\n",
    "    return ret_list\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "map_call_list = await asyncio.gather(\n",
    "    *[\n",
    "        model_llm.ainvoke(create_map_question(prompt_question))\n",
    "        for idx, prompt_question in enumerate(context_doc_list)\n",
    "    ]\n",
    ")\n",
    "map_call_list_new = parse_map_yes_no(map_call_list)\n",
    "print(f\"Time taken for MAP function::{time.time()-start_time}:seconds\")\n",
    "map_call_list_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(\"\".join(map_call_list_new)), len(\"\".join(map_call_list_new)) / 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What does this document talk about?\"\n",
    "\n",
    "\n",
    "def create_map_question(context_doc):\n",
    "    prompt_question = \"\"\"\n",
    "    System: Use the following portion of a long document to answer the question below. Summarize your findings in less than 50 words\n",
    "\n",
    "    Context:{context}\n",
    "\n",
    "    question: {question}\n",
    "    \"\"\".format(\n",
    "        question=question, context=context_doc\n",
    "    )\n",
    "    return prompt_question\n",
    "\n",
    "\n",
    "final_context = create_map_question(\"\\n\".join(map_call_list_new))\n",
    "# print(final_context)\n",
    "\n",
    "await model_llm.ainvoke(final_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finished the test calls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "wkshptest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
