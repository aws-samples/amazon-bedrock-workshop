{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc96dc43-8bbf-4d82-9010-34b802ae4798",
   "metadata": {},
   "source": [
    "# Conversational Interface - Contextual-aware Chatbot with Claude-3 LLM\n",
    "In this notebook we will build a chatbot which stores the conversation history in Dynamo DB and uses context which is stored in an in-memory vector db (FAISS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a5d1b62b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pypdf in /home/yanapo/.local/lib/python3.10/site-packages (4.3.1)\n",
      "Requirement already satisfied: typing_extensions>=4.0 in /home/yanapo/.local/lib/python3.10/site-packages (from pypdf) (4.12.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "/bin/bash: line 1: 2: No such file or directory\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pypdf\n",
    "%pip install faiss-cpu>=1.7,<2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ac9e2e43-c77c-4c8e-a581-f43bfd89fe86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import botocore\n",
    "import botocore.exceptions\n",
    "import pprint\n",
    "import os\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80151c5c-d919-46d2-83d3-c52d0c550bf5",
   "metadata": {},
   "source": [
    "### Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b0761cfa-a2c2-4d52-be5a-dddae776a9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "boto3_session = boto3.session.Session()\n",
    "llm_region = db_region_name = boto3_session.region_name\n",
    "\n",
    "# the statetements below can be used to override the region in the session, comment them out if there is no need to override\n",
    "#db_region = \"us-west-2\"\n",
    "#llm_region = \"us-west-2\"\n",
    "\n",
    "table_name = \"ConversationHistory\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01256c8-d611-428f-92d0-502420efbc19",
   "metadata": {},
   "source": [
    "Create the DynamoDB table that will be storing the conversation history. Note that by default, DynamoDBChatMessageHistory\n",
    "expects a primary key *sessionId*, however you can change the name of the key or you can set a composite key.\n",
    "A general discussion on DynamoDBChatMessageHistory can be found here: \n",
    "https://python.langchain.com/v0.2/docs/integrations/memory/aws_dynamodb/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cafadad7-9fc7-4c8f-8db4-3e822037fefe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table <ConversationHistory> already exists ...\n"
     ]
    }
   ],
   "source": [
    "# create the service client.\n",
    "dynamodb = boto3.client(\"dynamodb\",region_name=db_region)\n",
    "\n",
    "# check if the table exists; if it doesn't, then create the table\n",
    "try:\n",
    "    response = dynamodb.describe_table(TableName=table_name)\n",
    "    print(f'Table <{table_name}> already exists ...')\n",
    "except botocore.exceptions.ClientError as error:\n",
    "    if error.response['Error']['Code'] == 'ResourceNotFoundException':\n",
    "        print(f'Table <{table_name}> does not exist, creating ...')\n",
    "        table = dynamodb.create_table(\n",
    "            TableName=table_name,\n",
    "            KeySchema=[{\"AttributeName\": \"SessionId\", \"KeyType\": \"HASH\"}],\n",
    "            AttributeDefinitions=[{\"AttributeName\": \"SessionId\", \"AttributeType\": \"S\"}],\n",
    "            BillingMode=\"PAY_PER_REQUEST\",\n",
    "        )\n",
    "\n",
    "        # Wait until the table exists.\n",
    "        dynamodb.get_waiter(\"table_exists\").wait(TableName=table_name)\n",
    "\n",
    "        # Print out some data about the table.\n",
    "        print(f'Table <{table_name}> has been created')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6024e0fb-30ab-4a0b-b984-6030f3b53655",
   "metadata": {},
   "source": [
    "### Create and Run the chatbot\n",
    "Here we create a simple chatbot which translates the input from English to some language specified in the system prompt (you are welcome to change the language). \n",
    "The history of the conversation is stored in the Dynamo DB table we created above, you can check out how the conversation is stored in DDB by opening the AWS console"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8fb59de4-fcdf-4cfd-b243-2f16da715516",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up parameters for the model\n",
    "model = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "temperature = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "28852f4f-d7e1-49a8-959b-1a8eb8e8083d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the Chat model\n",
    "from langchain_aws.chat_models import ChatBedrock\n",
    "\n",
    "llm_chat = ChatBedrock(\n",
    "    model_id=model, \n",
    "    model_kwargs={\"temperature\": temperature},\n",
    "    region_name=llm_region\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e5151501-1ec6-4400-9cf8-82194d0836e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_community.chat_message_histories import DynamoDBChatMessageHistory\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"You're an assistant who speaks in {language}. Translate the user input\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"human\", \"{question}\"),\n",
    "])\n",
    "\n",
    "chain = prompt | llm_chat\n",
    "\n",
    "\"\"\"\n",
    "DynamoDBChatMessageHistory internally creates a dynamodb resource in the default region from .aws/config, which is problematic \n",
    "if the DDB table you have created is in a different region. In theory you could use the parameter *endpoint_url* in the \n",
    "DynamoDBChatMessageHistory constructor, however if you try it you get an InvalidSignatureException; in stackoverflow it is \n",
    "suggested that you should only use endpoint_url for local services like localstack or non-standard endpoints. \n",
    "So if you want to force to host the DDB in a specific region, it seems the only workaround is to set the AWS_DEFAULT_REGION env variable\n",
    "\"\"\"\n",
    "os.environ[\"AWS_DEFAULT_REGION\"] = db_region\n",
    "\n",
    "chain_with_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    lambda session_id: DynamoDBChatMessageHistory(\n",
    "        table_name=table_name, session_id=session_id\n",
    "    ),\n",
    "    input_messages_key=\"question\",\n",
    "    history_messages_key=\"history\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6441041d-bf31-4760-abeb-240c04542375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIMessage(content=\"Bonjour, je m'appelle John.\", additional_kwargs={'usage': {'prompt_tokens': 26, 'completion_tokens': 15, 'total_tokens': 41}, 'stop_reason': 'end_turn', 'model_id': 'anthropic.claude-3-sonnet-20240229-v1:0'}, response_metadata={'usage': {'prompt_tokens': 26, 'completion_tokens': 15, 'total_tokens': 41}, 'stop_reason': 'end_turn', 'model_id': 'anthropic.claude-3-sonnet-20240229-v1:0'}, id='run-056ac0c0-911e-451b-993b-5d9c8bc7bc17-0', usage_metadata={'input_tokens': 26, 'output_tokens': 15, 'total_tokens': 41})\n"
     ]
    }
   ],
   "source": [
    "lang = \"French\"\n",
    "session_id = \"zzz\"\n",
    "\n",
    "response = chain_with_history.invoke(\n",
    "                    {\"language\": lang, \"question\": \"Hi my name is John\"},\n",
    "                    config={\"configurable\": {\"session_id\": session_id}}\n",
    "                    )\n",
    "pp.pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5c2288d1-4dc8-4de2-8e3a-a421bf40a1d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIMessage(content='Votre nom est John.', additional_kwargs={'usage': {'prompt_tokens': 49, 'completion_tokens': 10, 'total_tokens': 59}, 'stop_reason': 'end_turn', 'model_id': 'anthropic.claude-3-sonnet-20240229-v1:0'}, response_metadata={'usage': {'prompt_tokens': 49, 'completion_tokens': 10, 'total_tokens': 59}, 'stop_reason': 'end_turn', 'model_id': 'anthropic.claude-3-sonnet-20240229-v1:0'}, id='run-339b4edc-0434-4654-957b-a3a099edf5a2-0', usage_metadata={'input_tokens': 49, 'output_tokens': 10, 'total_tokens': 59})\n"
     ]
    }
   ],
   "source": [
    "response = chain_with_history.invoke(\n",
    "    {\"language\": lang, \"question\": \"What is my name?\"},\n",
    "    config={\"configurable\": {\"session_id\": session_id}}\n",
    "    )\n",
    "pp.pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97a2ae1-3f27-4384-9fb1-82c4cefa6482",
   "metadata": {},
   "source": [
    "Now lets take a look what is stored in the message history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "03977203-0e31-4f8a-a476-507360751372",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Hi my name is John'),\n",
       " AIMessage(content=\"Bonjour, je m'appelle John.\", additional_kwargs={'stop_reason': 'end_turn', 'usage': {'total_tokens': Decimal('41'), 'completion_tokens': Decimal('15'), 'prompt_tokens': Decimal('26')}, 'model_id': 'anthropic.claude-3-sonnet-20240229-v1:0'}, response_metadata={'stop_reason': 'end_turn', 'usage': {'total_tokens': Decimal('41'), 'completion_tokens': Decimal('15'), 'prompt_tokens': Decimal('26')}, 'model_id': 'anthropic.claude-3-sonnet-20240229-v1:0'}, id='run-056ac0c0-911e-451b-993b-5d9c8bc7bc17-0', usage_metadata={'input_tokens': 26, 'output_tokens': 15, 'total_tokens': 41}),\n",
       " HumanMessage(content='What is my name?'),\n",
       " AIMessage(content='Votre nom est John.', additional_kwargs={'stop_reason': 'end_turn', 'usage': {'total_tokens': Decimal('59'), 'completion_tokens': Decimal('10'), 'prompt_tokens': Decimal('49')}, 'model_id': 'anthropic.claude-3-sonnet-20240229-v1:0'}, response_metadata={'stop_reason': 'end_turn', 'usage': {'total_tokens': Decimal('59'), 'completion_tokens': Decimal('10'), 'prompt_tokens': Decimal('49')}, 'model_id': 'anthropic.claude-3-sonnet-20240229-v1:0'}, id='run-339b4edc-0434-4654-957b-a3a099edf5a2-0', usage_metadata={'input_tokens': 49, 'output_tokens': 10, 'total_tokens': 59})]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = chain_with_history.get_session_history(session_id=session_id)\n",
    "h.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ffda923d-3633-4351-8ca9-4d9ca22bdce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HumanMessage(content='Hi my name is John')\n",
      "AIMessage(content=\"Bonjour, je m'appelle John.\", additional_kwargs={'stop_reason': 'end_turn', 'usage': {'total_tokens': Decimal('41'), 'completion_tokens': Decimal('15'), 'prompt_tokens': Decimal('26')}, 'model_id': 'anthropic.claude-3-sonnet-20240229-v1:0'}, response_metadata={'stop_reason': 'end_turn', 'usage': {'total_tokens': Decimal('41'), 'completion_tokens': Decimal('15'), 'prompt_tokens': Decimal('26')}, 'model_id': 'anthropic.claude-3-sonnet-20240229-v1:0'}, id='run-056ac0c0-911e-451b-993b-5d9c8bc7bc17-0', usage_metadata={'input_tokens': 26, 'output_tokens': 15, 'total_tokens': 41})\n",
      "HumanMessage(content='What is my name?')\n",
      "AIMessage(content='Votre nom est John.', additional_kwargs={'stop_reason': 'end_turn', 'usage': {'total_tokens': Decimal('59'), 'completion_tokens': Decimal('10'), 'prompt_tokens': Decimal('49')}, 'model_id': 'anthropic.claude-3-sonnet-20240229-v1:0'}, response_metadata={'stop_reason': 'end_turn', 'usage': {'total_tokens': Decimal('59'), 'completion_tokens': Decimal('10'), 'prompt_tokens': Decimal('49')}, 'model_id': 'anthropic.claude-3-sonnet-20240229-v1:0'}, id='run-339b4edc-0434-4654-957b-a3a099edf5a2-0', usage_metadata={'input_tokens': 49, 'output_tokens': 10, 'total_tokens': 59})\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(h.messages[0])\n",
    "pp.pprint(h.messages[1])\n",
    "pp.pprint(h.messages[2])\n",
    "pp.pprint(h.messages[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43fbc4d-5cc0-409c-8b32-7202dd76ea97",
   "metadata": {},
   "source": [
    "### Now lets add context using the RAG pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd4c4b9-7077-40d5-a6e9-69f65088e210",
   "metadata": {},
   "source": [
    "In a typical Q&A situation, when we want to add context with the RAG pattern, we use the user prompt to first run a similarity search to the vector db which stores our documents, then we \"augment\" the prompt with the chunks returned in the similarity search and finally pass the \"new\" prompt to the LLM to return its answer.\n",
    "\n",
    "However, in a conversational setting, the user query might require conversational context to be understood. For example, consider this exchange:\n",
    "\n",
    "> **Human:** \"What is Graviton?\"\n",
    "> \n",
    "> **AI:** some answer ...\n",
    "\n",
    "> **Human:** \"How much better price-performance do they deliver?\"\n",
    "\n",
    "Clearly the user's question still relates to Graviton, so when we run the similarity search we need to take that into account, otherwise with the simple approach it is very likely we will not get the most relevant chunks back\n",
    "\n",
    "The solution to this problem is to define a sub-chain that takes historical messages and the latest user question, and reformulates the question if it makes reference to any information in the historical information. This approach is depicted in the following diagram\n",
    "\n",
    "![alt text](./images/conversational_retrieval_chain.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c361ba7-57ef-4f8c-a151-dbcdd9b1142f",
   "metadata": {},
   "source": [
    "First lets load the 2022 Shareholder letter from Andy Jassy (pdf) and split it into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "92e46be3-d9fc-4c9a-8e03-d5fd1c8444e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Dear shareholders:\\n'\n",
      " 'As I sit down to write my second annual shareholder letter as CEO, I find '\n",
      " 'myself optimistic and energized\\n'\n",
      " 'by what lies ahead for Amazon. Despite 2022 being one of the harder '\n",
      " 'macroeconomic years in recent memory,and with some of our own operating '\n",
      " 'challenges to boot, we still found a way to grow demand (on top ofthe '\n",
      " 'unprecedented growth we experienced in the first half of the pandemic). We '\n",
      " 'innovated in our largestbusinesses to meaningfully improve customer '\n",
      " 'experience short and long term. And, we made importantadjustments in our '\n",
      " 'investment decisions and the way in which we’ll invent moving forward, while '\n",
      " 'stillpreserving the long-term investments that we believe can change the '\n",
      " 'future of Amazon for customers,\\n'\n",
      " 'shareholders, and employees.\\n'\n",
      " 'While there were an unusual number of simultaneous challenges this past '\n",
      " 'year, the reality is that if you')\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "file_name = \"./rag_data/2022-Shareholder-Letter.pdf\"\n",
    "loader = PyPDFLoader(file_name)\n",
    "pages = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(pages)\n",
    "pp.pprint(splits[0].page_content) # print the first chunk of the document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebce1d84-59fb-4fa6-ba4b-f45d73d23159",
   "metadata": {},
   "source": [
    "Now we will create an in-memory FAISS db to hold the chunks from the previous step, which we will use to retrieve the relevant context (RAG pattern) based on the user query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33167e80-57b7-430d-8a98-4ebbd1adba1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_aws import BedrockEmbeddings\n",
    "\n",
    "bedrock_client = boto3.client(service_name='bedrock-runtime', region_name=llm_region)\n",
    "model=\"amazon.titan-embed-text-v2:0\"\n",
    "bedrock_embeddings = BedrockEmbeddings(model_id=model, client=bedrock_client)\n",
    "\n",
    "vectorstore = FAISS.from_documents(\n",
    "    documents=splits,\n",
    "    embedding = bedrock_embeddings\n",
    ")\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b37219d-6d59-46c5-8dd5-58f897f48a83",
   "metadata": {},
   "source": [
    "At this point we will use the helper function *create_history_retriever()* which constructs a chain that accepts keys **input** and **chat_history** as input and has the same output schema as a receiver. Thsi will effectively implement the sub-chain for the \"contectual\" similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e6e18b-5081-49d9-bb04-319e4e6557bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "contextual_system_prompt = (\n",
    "    \"Given a chat history and the latest user question \"\n",
    "    \"which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood \"\n",
    "    \"without the chat history. Do NOT answer the question, \"\n",
    "    \"just reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextual_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm_chat, retriever, prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b097d836-a694-41b6-81d8-a3fcbf83e0c1",
   "metadata": {},
   "source": [
    "Finally we will build our full QA chain, leveraging 2 helper functions:\n",
    "1. *create_stuff_documents_chain()* specifies how retrieved context is fed into a prompt and LLM; it will include all retrieved context without any summarization or other processing and will generate an answer using the retrieved context and query.\n",
    "2. *create_retrieval_chain()* adds the retrieval step and propagates the retrieved context through the chain, providing it alongside the final answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96a90a3-a567-4505-a5cb-16901c5417d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm_chat, qa_prompt)\n",
    "# note that create_stuff_documents_chain() returns an LCEL Runnable; the input is a dictionary that \n",
    "# must have a “context” key that maps to a List[Document], and any other input variables expected in the prompt\n",
    "\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39cfe73-9d8d-489b-a922-84b06aa07b73",
   "metadata": {},
   "source": [
    "Now lets ask the chatbot some questions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23764062-5141-4b92-b13c-6831eb6f78f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_id = \"abc\"\n",
    "\n",
    "conversational_rag_chain = RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    lambda session_id: DynamoDBChatMessageHistory(\n",
    "        table_name=table_name, session_id=session_id\n",
    "    ),\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    "    output_messages_key=\"answer\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be653c8b-a582-4909-9034-898c513e0a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversational_rag_chain.invoke(\n",
    "    {\"input\": \"What is Graviton?\"},\n",
    "    config={\n",
    "        \"configurable\": {\"session_id\": \"abc\"}\n",
    "    },  # constructs a key \"abc123\" in `store`.\n",
    ")[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b7fbc2-1a70-4431-bafd-9e2b72e1ee65",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversational_rag_chain.invoke(\n",
    "    {\"input\": \"How much better price-performance do they deliver?\"},\n",
    "    config={\n",
    "        \"configurable\": {\"session_id\": \"abc\"}\n",
    "    },  # constructs a key \"abc123\" in `store`.\n",
    ")[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223901c9-6ee1-484e-8997-3eba153d6dff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
