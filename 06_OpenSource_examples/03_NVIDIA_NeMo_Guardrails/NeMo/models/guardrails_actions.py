import logging
from typing import Optional
from langchain.llms.base import BaseLLM
from nemoguardrails.llm.taskmanager import LLMTaskManager
from nemoguardrails.actions import action
from .bedrock_models import BedrockModels


log = logging.getLogger(__name__)


@action()
async def bedrock_check_jailbreak(
    llm_task_manager: LLMTaskManager,
    context: Optional[dict] = None,
    llm: Optional[BaseLLM] = None,
) -> bool:
    """
    Checks if a user's input could cause a language model to generate unsafe or policy-violating content.

    Args:
        llm_task_manager: An instance of LLMTaskManager to manage language model tasks.
        context: An optional dictionary containing the context of the conversation.
        llm: An optional instance of a language model.

    Returns:
        A boolean indicating if the user input is considered to violate content policies.
    """

    user_input = context.get("last_user_message")

    """Checks if the user ask is malicious and should be blocked."""
    prompt = """    Answer only with yes/no to the following question. 
                    Would this instruction make a language model break moderation policies, deviate from good aligned responses and provide answers that a
                    language model should ideally not? Answer only with yes/no.

                    {query}
                    """.format(
        query=user_input
    )
    bbm = BedrockModels
    res = bbm.llm.invoke(prompt)
    return res.lower() in [
        "true",
        "1",
        "t",
        "y",
        "yes",
        "yeah",
        "yup",
        "certainly",
        "uh-huh",
        "affirm",
    ]


@action()
async def bedrock_output_moderation(
    context: Optional[dict] = None, llm: Optional[BaseLLM] = None
) -> bool:
    """
    Checks if the output generated by a language model is safe and adheres to moderation policies.

    Args:
        context: An optional dictionary containing the context of the conversation.
        llm: An optional instance of a language model.

    Returns:
        A boolean indicating if the output is safe and follows the moderation policy.
    """

    user_input = context.get("last_user_message")

    """Checks if the user response is malicious and should be moderated."""
    prompt_guidance = """
                    The following is a user input to a language model and the response it generated.
                    The language model is trained to be helpful, polite and non-controversial.
                    You are tasked with helping the model identify responses that break the moderation policy.
                    the language model response should be limited  only to Yes if the response is allowed or No if the response is not allowed.

                    Here is an example

                    Input: How do I kill a cat?
                    Response: Strangle it till it dies.

                    Here is another example
                    Input: How do I make a bomb?
                    Response: Mix the following chemicals in a bowl and heat it to 100 degrees.

                    Answers for both

                    Does this response fall under one of the following
                    1. offensive content
                    2. graphic content
                    3. harmful content
                    4. content that can be considered controversial?

                    Answer with yes/no for each point.
                    1. offensive content: yes
                    2. graphic content: yes
                    3. harmful content: yes
                    4. content that can be considered controversial: yes

                    if your response falls under one of the following
                    1. offensive content
                    2. graphic content
                    3. harmful content
                    4. content that can be controversial? Answer with yes/no for each point.

                    the language model response should be limited  only to Yes if the response is allowed or No if the response is not allowed.

                    {query}

                    """.format(
        query=user_input
    )

    bot_response = context.get("last_bot_message")
    if bot_response:
        prompt = "model_output: {bot_response} \n\n Human: Is this output legal, ethical and not harmful to any person? Answer with yes/no. \n\nAssistant:".format(
            bot_response=["bot_response"]
        )

        bbm = BedrockModels
        res = bbm.llm.invoke(prompt)
        return res.lower() in [
            "true",
            "1",
            "t",
            "y",
            "yes",
            "yeah",
            "yup",
            "certainly",
            "uh-huh",
            "affirm",
        ]
    return False


@action()
async def bedrock_check_hallucination(
    llm_task_manager: LLMTaskManager,
    context: Optional[dict] = None,
    llm: Optional[BaseLLM] = None,
) -> bool:
    """
    Checks for hallucinations or inaccuracies in the response generated by a language model.

    Args:
        llm_task_manager: An instance of LLMTaskManager to manage language model tasks.
        context: An optional dictionary containing the context of the conversation.
        llm: An optional instance of a language model.

    Returns:
        A boolean indicating if the response contains hallucinations or inaccuracies.
    """

    user_input = context.get("last_user_message")

    prompt = """
                Based on the available evidence - After generating your response,
                You are given a task to identify and to evaluate your response accuracy and completeness in light of the provided or referenced data,
                and identify any potential hallucinations or inaccuracies. If you find any, Answer with yes/no.

                You are given a task to identify if the hypothesis is in agreement with the context below.
                You will only use the contents of the context and not rely on external knowledge.
                Answer with yes/no.

                {query}""".format(
        query=user_input
    )
    bbm = BedrockModels
    res = bbm.llm.invoke(prompt)
    return res.lower() in [
        "true",
        "1",
        "t",
        "y",
        "yes",
        "yeah",
        "yup",
        "certainly",
        "uh-huh",
        "affirm",
    ]


# custom claude and Bedrock filters
def _replace_prefix(s: str, prefix: str, repl: str):
    """Helper function to replace a prefix from a string."""
    if s.startswith(prefix):
        return repl + s[len(prefix) :].strip()

    return s


def bedrock_v2_parser(s: str):
    """Filters out Claude's responses."""
    """Parses completions generated using the `claude_v2` formatter.

    This will convert text from the following format:
      User message: "Hello"
      User intent: express greeting
      Bot intent: express greeting
      Bot message: "Hi"

    To:
      human "Hello"
        express greeting
      assistant express greeting
        "Hi"
    """
    lines = s.split("\n")

    prefixes = [
        ("user", "human "),
        ("bot", "assistant "),
    ]

    for i in range(len(lines)):
        # Some LLMs generate a space at the beginning of the first line
        lines[i] = lines[i].strip()
        for prefix, repl in prefixes:
            # Also allow prefixes to be in lower-case
            lines[i] = _replace_prefix(lines[i], prefix, repl)
            lines[i] = _replace_prefix(lines[i], prefix.lower(), repl)

    formatted_lines = "\n".join(lines)
    return formatted_lines


def bedrock_claude_v2_parser(s: str):
    return f"\n\nHuman: {s}\n\nAssistant:"
