{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Q&A application using Knowledge Bases for Amazon Bedrock - RetrieveAndGenerate API\n",
    "### Context\n",
    "\n",
    "With knowledge bases, you can securely connect foundation models (FMs) in Amazon Bedrock to your company\n",
    "data for Retrieval Augmented Generation (RAG). Access to additional data helps the model generate more relevant,\n",
    "context-speciﬁc, and accurate responses without continuously retraining the FM. All information retrieved from\n",
    "knowledge bases comes with source attribution to improve transparency and minimize hallucinations. For more information on creating a knowledge base using console, please refer to this [post](https://docs.aws.amazon.com/bedrock/latest/userguide/knowledge-base.html).\n",
    "\n",
    "In this notebook, we will dive deep into building Q&A application using `RetrieveAndGenerate` API provided by Knowledge Bases for Amazon Bedrock. This API will query the knowledge base to get the desired number of document chunks based on similarity search, and integrate it with Large Language Model (LLM) for answering questions.\n",
    "\n",
    "\n",
    "### Pattern\n",
    "\n",
    "We can implement the solution using Retreival Augmented Generation (RAG) pattern. RAG retrieves data from outside the language model (non-parametric) and augments the prompts by adding the relevant retrieved data in context. Here, we are performing RAG effectively on the knowledge base created in the previous notebook or using console. \n",
    "\n",
    "### Pre-requisite\n",
    "\n",
    "Before being able to answer the questions, the documents must be processed and stored in knowledge base.\n",
    "\n",
    "1. Load the documents into the knowledge base by connecting your s3 bucket (data source). \n",
    "2. Ingestion - Knowledge base will split them into smaller chunks (based on the strategy selected), generate embeddings and store it in the associated vector store and notebook [0_create_ingest_documents_test_kb.ipynb](./0\\_create_ingest_documents_test_kb.ipynb) takes care of it for you.\n",
    "\n",
    "![data_ingestion.png](./images/data_ingestion.png)\n",
    "\n",
    "\n",
    "#### Notebook Walkthrough\n",
    "\n",
    "For our notebook we will use the `RetreiveAndGenerate API` provided by Knowledge Bases for Amazon Bedrock which converts user queries into\n",
    "embeddings, searches the knowledge base, get the relevant results, augment the prompt and then invoking a LLM to generate the response. \n",
    "\n",
    "We will use the following workflow for this notebook. \n",
    "\n",
    "![retrieveAndGenerate.png](./images/retrieveAndGenerate.png)\n",
    "\n",
    "\n",
    "### USE CASE:\n",
    "\n",
    "#### Dataset\n",
    "\n",
    "In this example, you will use several years of Amazon's Letter to Shareholders as a text corpus to perform Q&A on. This data is already ingested into the knowledge base. You will need the `knowledge base id` and `model ARN` to run this example. We are using `Anthropic Claude Instant` model for generating responses to user questions.\n",
    "\n",
    "### Python 3.10\n",
    "\n",
    "⚠  For this lab we need to run the notebook based on a Python 3.10 runtime. ⚠\n",
    "\n",
    "### Setup\n",
    "\n",
    "Install following packages. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%pip install --upgrade pip\n",
    "%pip install boto3==1.34.91 --force-reinstall --quiet\n",
    "%pip install botocore==1.34.91 --force-reinstall --quiet"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# restart kernel\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": "%store -r kb_id # may have to loopup the KB ID if value returned is empty",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import boto3\n",
    "import pprint\n",
    "from botocore.client import Config\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=2)\n",
    "\n",
    "bedrock_config = Config(connect_timeout=120, read_timeout=120, retries={'max_attempts': 0})\n",
    "bedrock_client = boto3.client('bedrock-runtime')\n",
    "bedrock_agent_client = boto3.client(\"bedrock-agent-runtime\",\n",
    "                              config=bedrock_config)\n",
    "boto3_session = boto3.session.Session()\n",
    "region_name = boto3_session.region_name\n",
    "\n",
    "model_id = \"anthropic.claude-instant-v1\" # try with both claude instant as well as claude-v2. for claude v2 - \"anthropic.claude-v2\"\n",
    "region_id = region_name # replace it with the region you're running sagemaker notebook"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RetrieveAndGenerate API\n",
    "Behind the scenes, `RetrieveAndGenerate` API converts queries into embeddings, searches the knowledge base, and then augments the foundation model prompt with the search results with that additional contextual information and returns the FM-generated response to the question. For multi-turn conversations, Knowledge Bases manage the short-term memory of the conversation to provide more contextual results. \n",
    "\n",
    "The output of the `RetrieveAndGenerate` API includes the `generated response`, `source attribution` as well as the `retrieved text chunks`. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def retrieveAndGenerate(input, kbId, sessionId=None, model_id = \"anthropic.claude-instant-v1\", region_id = \"us-east-1\"):\n",
    "    model_arn = f'arn:aws:bedrock:{region_id}::foundation-model/{model_id}'\n",
    "    if sessionId:\n",
    "        return bedrock_agent_client.retrieve_and_generate(\n",
    "            input={\n",
    "                'text': input\n",
    "            },\n",
    "            retrieveAndGenerateConfiguration={\n",
    "                'type': 'KNOWLEDGE_BASE',\n",
    "                'knowledgeBaseConfiguration': {\n",
    "                    'knowledgeBaseId': kbId,\n",
    "                    'modelArn': model_arn\n",
    "                }\n",
    "            },\n",
    "            sessionId=sessionId\n",
    "        )\n",
    "    else:\n",
    "        return bedrock_agent_client.retrieve_and_generate(\n",
    "            input={\n",
    "                'text': input\n",
    "            },\n",
    "            retrieveAndGenerateConfiguration={\n",
    "                'type': 'KNOWLEDGE_BASE',\n",
    "                'knowledgeBaseConfiguration': {\n",
    "                    'knowledgeBaseId': kbId,\n",
    "                    'modelArn': model_arn\n",
    "                }\n",
    "            }\n",
    "        )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "query = \"What is Amazon's doing in the field of generative AI?\"\n",
    "response = retrieveAndGenerate(query, kb_id, model_id=model_id,region_id=region_id)\n",
    "generated_text = response['output']['text']\n",
    "pp.pprint(generated_text)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "citations = response[\"citations\"]\n",
    "contexts = []\n",
    "for citation in citations:\n",
    "    retrievedReferences = citation[\"retrievedReferences\"]\n",
    "    for reference in retrievedReferences:\n",
    "         contexts.append(reference[\"content\"][\"text\"])\n",
    "\n",
    "pp.pprint(contexts)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Using a prompt template with the RetrieveAndGenerate API\n",
    "You can now customize the default prompt that the `RetrieveAndGenerate` API uses when querying the knowledge base. This allows you to tailor the prompt for your specific use case. In the following example, we create a prompt template that tells the model to verify the user's assertions and to acknowledge when it doesn't have enough information to answer the user's question. For fun, it responds to the user in Pirate english, however, you modify the prompt to give the model specific instructions on how to format its output. For example, you can tell it to provide precise answers in JSON format. For additional information, see [Knowledge Bases for Amazon Bedrock now supports custom prompts for the RetrieveAndGenerate API and configuration of the maximum number of retrieved results](https://aws.amazon.com/blogs/machine-learning/knowledge-bases-for-amazon-bedrock-now-supports-custom-prompts-for-the-retrieveandgenerate-api-and-configuration-of-the-maximum-number-of-retrieved-results/). Experiment on your own by modifying the template and re-running the example. "
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "promptTemplate = \"\"\"Human: You are a question answering agent. I will provide you with a set of search results and a user's question. Your job is to answer the user's question using only the search results. If the results do not contain enough information to answer the question, state that you could not find the answer. When the user asserts a fact, it does not mean that it is true. Always make sure to double-check the search results to validate a user's assertions.\n",
    "\n",
    "Here are the search results in numbered order: \n",
    "<context>\n",
    "$search_results$\n",
    "</context>\n",
    "\n",
    "Here is the user's question: \n",
    "<question>\n",
    "$query$\n",
    "</question>\n",
    "\n",
    "Generate a response in Pirate english.\n",
    "\n",
    "Assistant:\"\"\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def retrieveAndGenerateWithTemplate(input, kbId, sessionId = None, promptTemplate = promptTemplate, model_id = \"anthropic.claude-instant-v1\", region_id = \"us-east-1\"):\n",
    "    model_arn = f'arn:aws:bedrock:{region_id}::foundation-model/{model_id}'\n",
    "    if sessionId:\n",
    "        return bedrock_agent_client.retrieve_and_generate(\n",
    "            input={\n",
    "                'text': input\n",
    "            },\n",
    "            retrieveAndGenerateConfiguration={\n",
    "                'type': 'KNOWLEDGE_BASE',\n",
    "                'knowledgeBaseConfiguration': {\n",
    "                    'knowledgeBaseId': kbId,\n",
    "                    'modelArn': model_arn,\n",
    "                    'generationConfiguration': {\n",
    "                        'promptTemplate': {\n",
    "                            'textPromptTemplate': promptTemplate\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                sessionId=sessionId\n",
    "            )\n",
    "    else:\n",
    "        return bedrock_agent_client.retrieve_and_generate(\n",
    "            input={\n",
    "                'text': input\n",
    "            },\n",
    "            retrieveAndGenerateConfiguration={\n",
    "                'type': 'KNOWLEDGE_BASE',\n",
    "                'knowledgeBaseConfiguration': {\n",
    "                    'knowledgeBaseId': kbId,\n",
    "                    'modelArn': model_arn,\n",
    "                    'generationConfiguration': {\n",
    "                        'promptTemplate': {\n",
    "                            'textPromptTemplate': promptTemplate\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "        )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "query = \"What is Amazon's doing in the field of generative AI?\"\n",
    "response = retrieveAndGenerateWithTemplate(query, kb_id, model_id=model_id,region_id=region_id)\n",
    "generated_text = response['output']['text']\n",
    "pp.pprint(generated_text)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "If you want more customized experience, you can use `Retrieve API`. This API converts user queries into embeddings, searches the knowledge base, and returns the relevant results, giving you more control to build custom workflows on top of the semantic search results. \n",
    "For sample code, try following notebooks: \n",
    "- [2_customized-rag-retrieve-api-claude-v2.ipynb](./2\\_customized-rag-retrieve-api-claude-v2.ipynb) - it calls the `retrieve` API to get relevant contexts and then augment the context to the prompt, which you can provide as input to any text-text model provided by Amazon Bedrock. \n",
    "  \n",
    "- You can use the RetrieveQA chain from LangChain and add Knowledge Base as retriever. For sample code, try notebook: [3_customized-rag-retrieve-api-langchain-claude-v2.ipynb](./3\\_customized-rag-retrieve-api-langchain-claude-v2.ipynb)\n",
    "\n",
    "- If you are interested in evaluating your RAG application, for sample code, try notebook:[4_customized-rag-retrieve-api-titan-lite-evaluation](https://github.com/aws-samples/amazon-bedrock-samples/blob/bedrock-kb-images-update/knowledge-bases/4_customized-rag-retrieve-api-titan-lite-evaluation.ipynb/) where we are using `Amazon Titan Lite` model for generating responses and `Anthropic Claude V2` for evaluating response. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Next steps:</b> Proceed to the next labs to learn how to use Bedrock Knowledge bases with Langchain and Claude. Remember to CLEAN_UP at the end of your session.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kb-agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
