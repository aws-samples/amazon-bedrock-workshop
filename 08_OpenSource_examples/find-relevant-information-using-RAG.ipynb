{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "daed6e8f-8426-4d44-9d48-5cda928f82b8",
   "metadata": {},
   "source": [
    "# Complete Document Retrieval and RAG\n",
    "\n",
    "## Overview\n",
    "- **Retrieval Pipeline** With customers having the ability to enter any number of possibilities into the solution, it is helpful to detect intent and normalize the query. Few-shots are a useful tool to tailor the normalization to the nature of the query in-line. \n",
    "- **Advanced methods** For more complex cases, it can be beneficial to generate hypothetical queries and documents solving for sub-queries and improving the semantic similarity.\n",
    "- **Model answer generation** Once the model is shown a set of documents, it must generate an answer while staying as closely aligned to the contents of the documents as possible. We cover self-verification and citation as methods giving greater flexibility to the model for a given query and set of retrieved documents.\n",
    "\n",
    "## Context\n",
    "\n",
    "Retrieval Augmented Generation (RAG) requires the indexation of relevant unstructured documents into a vector database. Then given a customer query, the relevant are retrieved and past as context to the model, which generates an answer. This can best be described by the following flow.\n",
    "\n",
    "<img src=\"./assets/rag-architecture.png\" />\n",
    "\n",
    "Once our documents (PDFs, CSV, Tables, JSON, ...) have been indexed into our knowledge base, we start working towards retrieval of a relevant subset of documents based on a given query. For many applications, the success of the retrieval is a strong indicator for the performance of the overall response. This notebook assumes you are familiar with the basics of RAG, embedding models and vector databases.\n",
    "\n",
    "In this notebook, we seek to go beyond RAG to generate the model answer by applying other relevant steps in the answer pipeline.\n",
    "\n",
    "<h2>Prerequisites</h2>\n",
    "\n",
    "Before you can use Amazon Bedrock, you must carry out the following steps:\n",
    "\n",
    "- Sign up for an AWS account (if you don't already have one) and IAM Role with the necessary permissions for Amazon Bedrock, see [AWS Account and IAM Role](https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html#new-to-aws){:target=\"_blank\"}.\n",
    "- Request access to the foundation models (FM) that you want to use, see [Request access to FMs](https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html#getting-started-model-access){:target=\"_blank\"}. \n",
    "    \n",
    "<h2>Setup</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9239b9-3894-449c-97e8-7b04c680817c",
   "metadata": {},
   "source": [
    "We import the relevant objects used in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "id": "d7a8e04e-ddba-491d-a52a-448138bf07f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T20:03:08.288566Z",
     "start_time": "2025-02-13T20:03:08.281477Z"
    }
   },
   "source": [
    "import boto3\n",
    "import faiss\n",
    "import re\n",
    "from operator import itemgetter\n",
    "from typing import List\n",
    "from langchain_aws.chat_models.bedrock import ChatBedrock\n",
    "from langchain_aws import BedrockEmbeddings\n",
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    FewShotChatMessagePromptTemplate,\n",
    ")\n",
    "from IPython.display import display_markdown, Markdown\n",
    "from langchain_community.docstore import InMemoryDocstore\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.example_selectors import SemanticSimilarityExampleSelector\n",
    "from langchain_core.prompts import HumanMessagePromptTemplate, AIMessagePromptTemplate\n",
    "from langchain.output_parsers import PydanticToolsParser\n",
    "from langchain_community.retrievers import WikipediaRetriever\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.messages import AIMessage\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.runnables import (\n",
    "    RunnableLambda,\n",
    "    RunnableParallel,\n",
    "    RunnablePassthrough,\n",
    "    RunnableBranch,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "d8141160-6799-4c30-8cf5-3f917c87a646",
   "metadata": {},
   "source": "Although this example leverages Nova Pro & Nova Lite, Bedrock supports many other models. This full list of models and supported features can be found [here](https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference.html). The models are invoked via `bedrock-runtime`."
  },
  {
   "cell_type": "code",
   "id": "3c95bebf-c30b-47a3-8566-438275fe37da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T20:02:10.404760Z",
     "start_time": "2025-02-13T20:02:10.284615Z"
    }
   },
   "source": [
    "region = 'us-west-2'\n",
    "bedrock = boto3.client(\n",
    "    service_name = 'bedrock-runtime',\n",
    "    region_name = region,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "144041e6-f221-4f3a-a546-5356a4edee29",
   "metadata": {},
   "source": [
    "We use `ChatBedrock` and `BedrockEmbeddings` to interact with the Bedrock API. We enable `beta_use_converse_api` to use the Converse API."
   ]
  },
  {
   "cell_type": "code",
   "id": "e1723192-9fed-4fb1-8dc0-9a9d26f531f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T20:20:12.275860Z",
     "start_time": "2025-02-13T20:20:12.273354Z"
    }
   },
   "source": [
    "modelId = \"us.amazon.nova-lite-v1:0\"\n",
    "nova = ChatBedrock(\n",
    "    model_id=modelId,\n",
    "    client=bedrock,\n",
    "    beta_use_converse_api=True\n",
    ")\n",
    "embeddingId = \"amazon.titan-embed-text-v1\"\n",
    "embeddings = BedrockEmbeddings(\n",
    "    model_id=embeddingId,\n",
    "    client=bedrock)"
   ],
   "outputs": [],
   "execution_count": 91
  },
  {
   "cell_type": "markdown",
   "id": "dc2bb343-bf2c-4429-bc20-90e4dc48bcd4",
   "metadata": {},
   "source": [
    "We correctly get a generic answer message from the model."
   ]
  },
  {
   "cell_type": "code",
   "id": "e7e9f73a-36bc-4767-a94a-3ca85c90e8ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T20:20:17.121473Z",
     "start_time": "2025-02-13T20:20:13.177172Z"
    }
   },
   "source": "display_markdown(Markdown(nova.invoke(\"Help me with my travel needs today.\").content))",
   "outputs": [
    {
     "data": {
      "text/markdown": "Of course! I'd be happy to help you with your travel needs today. Here are some common areas where you might need assistance:\n\n1. **Itinerary Planning**:\n   - **Destination**: Where are you traveling to?\n   - **Duration**: How long will you be there?\n   - **Activities**: What do you want to see and do?\n\n2. **Transportation**:\n   - **Getting to the Airport/Station**: Do you need information on public transport, ride-sharing services, or a taxi?\n   - **Flights/Trains**: Have you booked your tickets? If not, I can help you find options.\n   - **Local Transport**: Do you need advice on how to get around the city (subway, buses, taxis, bike rentals)?\n\n3. **Accommodation**:\n   - **Hotels/Airbnb**: Have you booked a place to stay? If not, I can help you find options based on your budget and preferences.\n   - **Check-in/Check-out Times**: Do you need information on the procedures?\n\n4. **Packing**:\n   - **Weather**: What’s the weather like at your destination? Any special clothing or gear needed?\n   - **Essentials**: List of items you shouldn’t forget (passport, tickets, chargers, etc.).\n\n5. **Local Information**:\n   - **Language**: Common phrases in the local language.\n   - **Currency**: Information on the local currency and exchange rates.\n   - **Emergency Numbers**: Local emergency contact numbers.\n\n6. **Health and Safety**:\n   - **Vaccinations**: Are there any required or recommended vaccinations for your destination?\n   - **Travel Insurance**: Do you have travel insurance? If not, I can suggest some providers.\n   - **Safety Tips**: General safety tips for your destination.\n\n7. **Dining**:\n   - **Restaurants**: Recommendations for local cuisine or specific types of food you enjoy.\n   - **Dietary Restrictions**: Any dietary restrictions or preferences?\n\n8. **Cultural Etiquette**:\n   - **Customs**: Any local customs or etiquette you should be aware of?\n\nFeel free to provide more details about your travel plans, and I can give you more specific advice!"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 92
  },
  {
   "cell_type": "markdown",
   "id": "1b8c374c-2c9e-4093-ab7c-48b9c39ff3c3",
   "metadata": {},
   "source": [
    "## Reformating the initial query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac197d7a-0397-4ab8-8f82-3d8dc63fdc3a",
   "metadata": {},
   "source": [
    "### Intent Detection\n",
    "\n",
    "In order to limit the scope of answers handled by the solution with RAG, a common first step in the answer pipeline is **Intent Detection or Classification**. This step is important to ensure the relevancy of the question to the indexed content, which works to limit the model's tendancy to answer questions that may not have been accounted for or tested by the application developers.\n",
    "\n",
    "When requesting some information that is irrelevant to the previously stated purpose, we quickly see the model attempting to provide an answer."
   ]
  },
  {
   "cell_type": "code",
   "id": "c4f19692-d210-45c0-81fb-d50c3f9ae1fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T20:20:32.732386Z",
     "start_time": "2025-02-13T20:20:22.853558Z"
    }
   },
   "source": "display_markdown(Markdown(nova.invoke(\"I want to learn more about my mom's pie recipe\").content))",
   "outputs": [
    {
     "data": {
      "text/markdown": "Certainly! Pie recipes can vary widely depending on the type of pie your mom makes. Here are some common types of pies and their general ingredients and methods. If you provide more specifics about your mom's recipe, I can give more tailored advice.\n\n### 1. **Apple Pie**\n#### Ingredients:\n- **Crust:**\n  - 2 1/2 cups all-purpose flour\n  - 1 teaspoon salt\n  - 1 teaspoon sugar\n  - 1 cup (2 sticks) unsalted butter, chilled and cut into small pieces\n  - 6-8 tablespoons ice water\n\n- **Filling:**\n  - 6-8 medium apples (Granny Smith, Honeycrisp, or a mix)\n  - 3/4 cup granulated sugar\n  - 1/4 cup brown sugar\n  - 1 teaspoon ground cinnamon\n  - 1/4 teaspoon ground nutmeg\n  - 1/4 teaspoon salt\n  - 2 tablespoons all-purpose flour\n  - 1 tablespoon lemon juice\n  - 1 teaspoon vanilla extract\n  - 1 egg, beaten (for egg wash)\n\n#### Method:\n1. **Crust:**\n   - Mix flour, salt, and sugar in a bowl.\n   - Cut in the butter until it resembles coarse crumbs.\n   - Gradually add ice water until the dough comes together.\n   - Divide dough in half, shape into disks, wrap in plastic, and chill for at least 1 hour.\n\n2. **Filling:**\n   - Peel, core, and slice apples.\n   - In a large bowl, combine apples, sugars, spices, salt, flour, lemon juice, and vanilla.\n   - Toss until apples are well-coated.\n\n3. **Assembly:**\n   - Roll out one disk of dough to fit a 9-inch pie dish.\n   - Place in the dish and trim edges.\n   - Add apple filling and dot with butter.\n   - Roll out the second disk of dough, place over the filling, and crimp edges.\n   - Brush with beaten egg and cut slits for steam.\n\n4. **Baking:**\n   - Bake at 425°F (220°C) for 20 minutes, then reduce heat to 375°F (190°C) and bake for 40-50 minutes until crust is golden and filling is bubbly.\n\n### 2. **Cherry Pie**\n#### Ingredients:\n- **Crust:**\n  - Same as apple pie crust\n\n- **Filling:**\n  - 3 cups fresh or frozen pitted cherries\n  - 3/4 cup granulated sugar\n  - 1/4 cup brown sugar\n  - 1/4 cup all-purpose flour\n  - 1 tablespoon lemon juice\n  - 1 teaspoon vanilla extract\n  - 1/2 teaspoon salt\n  - 1/2 teaspoon ground cinnamon\n  - 1/4 teaspoon ground nutmeg\n\n#### Method:\n1. **Crust:**\n   - Same as apple pie crust\n\n2. **Filling:**\n   - In a large bowl, combine cherries, sugars, flour, lemon juice, vanilla, salt, cinnamon, and nutmeg.\n   - Toss until cherries are well-coated.\n\n3. **Assembly:**\n   - Roll out one disk of dough to fit a 9-inch pie dish.\n   - Place in the dish and trim edges.\n   - Add cherry filling.\n\n4. **Top Crust and Baking:**\n   - Roll out the second disk of dough, place over the filling, and crimp edges.\n   - Cut slits for steam and brush with beaten egg.\n   - Bake at 425°F (220°C) for 20 minutes, then reduce heat to 375°F (190°C) and bake for 40-50 minutes until crust is golden and filling is bubbly.\n\n### 3. **Pumpkin Pie**\n#### Ingredients:\n- **Crust:**\n  - Same as apple pie crust\n\n- **Filling:**\n  - 2 cups pumpkin puree (fresh or canned)\n  - 3/4 cup granulated sugar\n  - 1 teaspoon salt\n  - 1 teaspoon ground cinnamon\n  - 1/2 teaspoon ground ginger\n  - 1/4 teaspoon ground cloves\n  - 2 large eggs\n  - 1 can (12 oz) evaporated milk\n\n#### Method:\n1. **Crust:**\n   - Same as apple pie crust\n\n2. **Filling:**\n   - In a large bowl, combine pumpkin, sugars, salt, cinnamon, ginger, and cloves.\n   - Whisk in eggs, then stir in evaporated milk.\n\n3. **Assembly:**\n   - Roll out one disk of dough to fit a 9-inch pie dish.\n   - Place in the dish and trim edges.\n   - Pour filling into the crust.\n\n4. **Baking:**\n   - Bake at 425°F (220°C) for 15 minutes, then reduce heat to 350°F (175°C) and bake for 45-55 minutes until a knife inserted near the center comes out clean.\n\n### General Tips:\n- **Chilling the Dough:** Always chill the pie dough before rolling it out to make it easier to handle and prevent shrinking.\n- **Blind Baking:** For a really crisp bottom crust, you can blind bake the crust before adding the filling. This means baking the crust alone for a few minutes before adding the filling.\n- **Flavor Variations:** Feel free to experiment with different spices, fruits, or even add a bit of bourbon or rum to your pie for unique flavors.\n\nIf you can provide more details about your mom’s specific recipe, I can give you more precise guidance!"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 93
  },
  {
   "cell_type": "markdown",
   "id": "c00d6258-510c-46ee-b5ae-2f31800be497",
   "metadata": {},
   "source": [
    "Hence, we provide an initial system prompt defining the model's role as an intent classifier. We supply the classes and few-shots to improve performance and ensure the model is aligned to the desired intended output, which needs to include `<intention></intention>` tags."
   ]
  },
  {
   "cell_type": "code",
   "id": "e16103d7-2c42-449a-b50b-bb20631dceea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T20:20:34.495333Z",
     "start_time": "2025-02-13T20:20:34.493185Z"
    }
   },
   "source": [
    "intent_system_prompt = \"\"\"You are a precise classifier. Your task is to assess customer intent and categorize customer inquiry into one of the intentions. \n",
    "\n",
    "Intentions with their description:\n",
    "vacation: Information on vacations, various travel destinations and my recent travels.\n",
    "contact: Expressing the desire to talk to support.\n",
    "irrelevant: Not related to vacations and travel.\n",
    "\n",
    "Here is an example of how to respond in a standard interaction:\n",
    "<example>\n",
    "    Human: I am seeking a place that is sunny a family friendly.\n",
    "    AI: <intention>vacation</intention>\n",
    "</example>\n",
    "<example>\n",
    "    Human: I want to learn more about my mom's pie recipe\n",
    "    AI: <intention>irrelevant</intention>\n",
    "</example>\n",
    "<example>\n",
    "    Human: I want to talk to a someone.\n",
    "    AI: <intention>contact</intention>\n",
    "</example>\n",
    "\n",
    "Think about your answer first before you respond. Think step-by-step and insert the classification in <intention></intention> tags and do not include anything after.\"\"\""
   ],
   "outputs": [],
   "execution_count": 94
  },
  {
   "cell_type": "markdown",
   "id": "52f456a5-cc22-4235-9433-406487284269",
   "metadata": {},
   "source": [
    "We supply the prompt as part of `ChatPromptTemplate`and use the pipe operator to define a chain connecting the model to the resulting prompt."
   ]
  },
  {
   "cell_type": "code",
   "id": "c124c190-f1b3-4cfa-89c7-c7c409bd0411",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T20:20:36.598760Z",
     "start_time": "2025-02-13T20:20:36.596229Z"
    }
   },
   "source": [
    "intent_detection_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", intent_system_prompt),\n",
    "        (\"human\", \"Here is the customer's question: <question>{question}</question> How do you answer to the instructions?\"),\n",
    "    ]\n",
    ")\n",
    "intent_detection_chain = intent_detection_prompt | nova"
   ],
   "outputs": [],
   "execution_count": 95
  },
  {
   "cell_type": "markdown",
   "id": "bde5c4a1-c682-4632-9b79-0503a4bd5f10",
   "metadata": {},
   "source": [
    "We invoke the model with the same query and notice the classification result. We invite you to try additional questions."
   ]
  },
  {
   "cell_type": "code",
   "id": "8fce706b-187f-4518-9927-c72ae44f3ad9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T20:20:45.684811Z",
     "start_time": "2025-02-13T20:20:45.312662Z"
    }
   },
   "source": "display_markdown(Markdown(intent_detection_chain.invoke(\"Tell me about my mother's pie recipe\").content))",
   "outputs": [
    {
     "data": {
      "text/markdown": "<intention>irrelevant</intention>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 96
  },
  {
   "cell_type": "markdown",
   "id": "ca0efc0d-9826-4a54-94e6-81056a58c166",
   "metadata": {},
   "source": [
    "Since we expect the answer to always contain these tags, we can parse it and branch off depending on the model's classification. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8804bd6e-4c66-458e-98c2-7f4948bad4fa",
   "metadata": {},
   "source": [
    "### Dynamic few-shots\n",
    "\n",
    "Although static few-shots are helpful, they have two major obstacles. On the one hand, they do not cover the breadth of necessary examples, and on the other, given that any submitted query is rarely relevant to all supplied examples, they often introduce unecessary tokens and noise to the prompt. In constrast, supplying dynamic few-shots from a larger corpus of examples enables us to select a number of the most relevant examples prior to inference. Evidently, these are determined by the nature of the query. Although we apply it to intend classification, dynamic few-shots can be applied anywhere in the RAG pipeline and generally yield stronger results compared to static examples. \n",
    "\n",
    "We bootstrap `few_shot_library` using examples distilled by **Amazon Nova Pro**. It is important to continuously iterate on the library after the initial deployment. During this phase, it is a general best practice to collect and label real interactions where the model made mistakes and append those to the set of examples."
   ]
  },
  {
   "cell_type": "code",
   "id": "61480afe-7501-4e3b-a191-7d66dd456964",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T20:21:08.620638Z",
     "start_time": "2025-02-13T20:21:08.615379Z"
    }
   },
   "source": [
    "few_shot_library = [\n",
    "    {\n",
    "        \"question\": \"Can you recommend some tropical beach destinations?\",\n",
    "        \"class\": \"vacation\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"I need to speak with a customer service representative.\",\n",
    "        \"class\": \"contact\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What's the best way to cook spaghetti?\",\n",
    "        \"class\": \"irrelevant\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Are there any family-friendly resorts in Florida?\",\n",
    "        \"class\": \"vacation\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How do I file a complaint about my recent stay?\",\n",
    "        \"class\": \"contact\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What's the weather like in Paris in June?\",\n",
    "        \"class\": \"vacation\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Can you help me with my car insurance claim?\",\n",
    "        \"class\": \"irrelevant\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"I'd like to book an all-inclusive Caribbean cruise.\",\n",
    "        \"class\": \"vacation\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Is there a phone number for your reservations team?\",\n",
    "        \"class\": \"contact\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What's the best way to learn a new language?\",\n",
    "        \"class\": \"irrelevant\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Are there any good hiking trails in Yellowstone?\",\n",
    "        \"class\": \"vacation\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"I need to update my billing information.\",\n",
    "        \"class\": \"contact\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How do I make homemade bread?\",\n",
    "        \"class\": \"irrelevant\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What are some popular tourist attractions in Rome?\",\n",
    "        \"class\": \"vacation\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Can I speak with a manager about my recent experience?\",\n",
    "        \"class\": \"contact\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What's the best time to visit Japan?\",\n",
    "        \"class\": \"vacation\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How do I reset my Netflix password?\",\n",
    "        \"class\": \"irrelevant\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Are there any good ski resorts in Colorado?\",\n",
    "        \"class\": \"vacation\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"I need help with my online booking.\",\n",
    "        \"class\": \"contact\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What's the plot of the latest Marvel movie?\",\n",
    "        \"class\": \"irrelevant\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Can you suggest some budget-friendly European cities?\",\n",
    "        \"class\": \"vacation\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How do I request a refund for my canceled trip?\",\n",
    "        \"class\": \"contact\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What's the best way to train a puppy?\",\n",
    "        \"class\": \"irrelevant\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Are there any good wildlife safaris in Africa?\",\n",
    "        \"class\": \"vacation\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"I need to change my flight reservation.\",\n",
    "        \"class\": \"contact\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What are some must-see landmarks in New York City?\",\n",
    "        \"class\": \"vacation\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How do I fix a leaky faucet?\",\n",
    "        \"class\": \"irrelevant\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Can you recommend some romantic getaways for couples?\",\n",
    "        \"class\": \"vacation\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"I have a question about my loyalty points balance.\",\n",
    "        \"class\": \"contact\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What's the best way to prepare for a job interview?\",\n",
    "        \"class\": \"irrelevant\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Tell me about my travel history\",\n",
    "        \"class\": \"vacation\"\n",
    "    },\n",
    "    \n",
    "]"
   ],
   "outputs": [],
   "execution_count": 97
  },
  {
   "cell_type": "markdown",
   "id": "2fec08d8-4273-4689-9a5c-b189186a451d",
   "metadata": {},
   "source": [
    "In this notebook, we use FAISS (Facebook AI Similarity Search) [(github)](https://github.com/facebookresearch/faiss), which is an open-source library developed by Facebook AI Research for efficient similarity search and clustering of dense vector embeddings. We call the Lanchain's `FAISS` object to interact with the in-memory vector store.\n",
    "\n",
    "We embed the examples using the Titan Embedding model."
   ]
  },
  {
   "cell_type": "code",
   "id": "87fce4d9-b019-4f22-a0fc-bf15c680932c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T20:21:10.159374Z",
     "start_time": "2025-02-13T20:21:10.157315Z"
    }
   },
   "source": [
    "embedding_size = 1536\n",
    "index = faiss.IndexFlatL2(embedding_size)\n",
    "embedding_fn = embeddings.embed_query\n",
    "vectorstore = FAISS(embedding_fn, index, InMemoryDocstore({}), {})"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`embedding_function` is expected to be an Embeddings object, support for passing in a function will soon be removed.\n"
     ]
    }
   ],
   "execution_count": 98
  },
  {
   "cell_type": "markdown",
   "id": "ec04c8c5-21dc-4c89-bb7d-dd0af1e91b0c",
   "metadata": {},
   "source": [
    "We use `SemanticSimilarityExampleSelector` to dynamically select the `k` most relevant examples based on our query. When instantiated, this object embeds the set of examples into our vector store of choice. `FewShotChatMessagePromptTemplate` defines the formatting of the selected examples into a given prompt. We define the template to be consistent with what will be generated by the model during intent classification."
   ]
  },
  {
   "cell_type": "code",
   "id": "303a35b5-a45e-42ce-ad2e-e0043f204c6c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T20:21:20.266966Z",
     "start_time": "2025-02-13T20:21:15.168799Z"
    }
   },
   "source": [
    "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "    few_shot_library,\n",
    "    embeddings,\n",
    "    vectorstore,\n",
    "    k=5,\n",
    ")\n",
    "\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_selector=example_selector,\n",
    "    example_prompt=(\n",
    "        HumanMessagePromptTemplate.from_template(\"{question}\")\n",
    "        + AIMessagePromptTemplate.from_template(\"<intention>{class}</intention>\")\n",
    "    ),\n",
    "    input_variables=[\"question\"],\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 99
  },
  {
   "cell_type": "markdown",
   "id": "5038eca0-b90b-4847-997b-4fd84b17c082",
   "metadata": {},
   "source": [
    "We print the relevant examples for a given query. Notice that the distribution of labels will change based on the nature of the query. This helps further align the model with our expectations."
   ]
  },
  {
   "cell_type": "code",
   "id": "8c171e51-e5cd-48b6-887b-09c915672607",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T20:21:20.450437Z",
     "start_time": "2025-02-13T20:21:20.283536Z"
    }
   },
   "source": "display_markdown(Markdown(few_shot_prompt.format(question=\"tell me about my travels\")))",
   "outputs": [
    {
     "data": {
      "text/markdown": "Human: Tell me about my travel history\nAI: <intention>vacation</intention>\nHuman: I'd like to book an all-inclusive Caribbean cruise.\nAI: <intention>vacation</intention>\nHuman: Can you suggest some budget-friendly European cities?\nAI: <intention>vacation</intention>\nHuman: Can I speak with a manager about my recent experience?\nAI: <intention>contact</intention>\nHuman: How do I request a refund for my canceled trip?\nAI: <intention>contact</intention>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 100
  },
  {
   "cell_type": "markdown",
   "id": "e7e6abc8-0bea-42fb-8925-e33bbaa6a6f3",
   "metadata": {},
   "source": [
    "We redefine the system prompt to accomodate for the dynamic few-shots."
   ]
  },
  {
   "cell_type": "code",
   "id": "db144844-194f-4c8b-b623-23c0d901c276",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T20:21:20.478221Z",
     "start_time": "2025-02-13T20:21:20.476440Z"
    }
   },
   "source": [
    "few_shot_intent_system_prompt = \"\"\"You are a precise classifier. Your task is to assess customer intent and categorize customer inquiry into one of the intentions. \n",
    "\n",
    "Intentions with their description:\n",
    "vacation: Information on vacations, various travel destinations and my recent travels.\n",
    "contact: Expressing the desire to talk to support.\n",
    "irrelevant: Not related to vacations and travel.\n",
    "\n",
    "Here is an example of how to respond in a standard interaction:\n",
    "\"\"\""
   ],
   "outputs": [],
   "execution_count": 101
  },
  {
   "cell_type": "markdown",
   "id": "3b6c2811-3a7e-4d8b-af4b-e8b326ed6b74",
   "metadata": {},
   "source": [
    "We redefine the prompt template to accomodate for the dynamic few-shots. As expected, the final string created from `intent_detection_prompt` will change based on message similarity to previous examples."
   ]
  },
  {
   "cell_type": "code",
   "id": "e215997a-e940-4a94-a4f3-a424b595c90c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T20:21:20.501123Z",
     "start_time": "2025-02-13T20:21:20.498866Z"
    }
   },
   "source": [
    "few_shot_intent_detection_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", few_shot_intent_system_prompt),\n",
    "        few_shot_prompt,\n",
    "        (\"human\", \"Think step-by-step and always ensure you insert the classification in <intention></intention> tags and do not include anything after.\\\n",
    "        Here is the customer's question: <question>{question}</question> How do you answer to the instructions?\"),\n",
    "    ]\n",
    ")\n",
    "few_shot_intent_chain = intent_detection_prompt | nova"
   ],
   "outputs": [],
   "execution_count": 102
  },
  {
   "cell_type": "markdown",
   "id": "efde4eed-db7c-40aa-a037-fbbd30ba0492",
   "metadata": {},
   "source": [
    "We test the newly created chain."
   ]
  },
  {
   "cell_type": "code",
   "id": "94b35d0b-1fbe-4a0b-a331-b445d0388607",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T20:21:22.992234Z",
     "start_time": "2025-02-13T20:21:22.564561Z"
    }
   },
   "source": "display_markdown(Markdown(few_shot_intent_chain.invoke({\"question\": \"tell me about my travel history\"}).content))",
   "outputs": [
    {
     "data": {
      "text/markdown": "<intention>vacation</intention>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 103
  },
  {
   "cell_type": "markdown",
   "id": "a359d7ba-ce4d-421f-901b-1c97aab0590c",
   "metadata": {},
   "source": [
    "### Normalizing the user message\n",
    "\n",
    "We may want to restrict the queries that are sent to downstream inference without restricting the user experience. Normalizing messages enables us to do exactly this. It can often be used to set a certain tone, reduce length and extract the specific purpose of the message while reducing unecessary noise. Notice the role the rule book plays in determining the nature of the returned message.\n",
    "\n",
    "Alternatively, it is common to supply few-shot examples as we have done in the previous step. We again return the resulting message in between tags."
   ]
  },
  {
   "cell_type": "code",
   "id": "3018bf44-5cb6-48bd-9846-b9bf08562e85",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T20:21:28.280127Z",
     "start_time": "2025-02-13T20:21:28.278111Z"
    }
   },
   "source": [
    "norm_system_prompt = \"\"\"You are a precise message synthesizer. Your task is to write a condensed message encompassing the latest original message's intent and main keywords. \n",
    "The condensed message must follow the rule book.\n",
    "\n",
    "Rule book:\n",
    "- Must be a complete sentence formulated as a request from the perspective of the original requester.\n",
    "- No longer than 2 short sentences with no concatination.\n",
    "- Never include names.\n",
    "- It is safe to reformulate questions with only keyword as looking for information on the place they mention.\n",
    " \n",
    "Think about your answer first before you respond. Think step-by-step and the condensed message in <condensed_message></condensed message> tags and do not include anything after.\"\"\""
   ],
   "outputs": [],
   "execution_count": 104
  },
  {
   "cell_type": "markdown",
   "id": "5ef0927d-2ca2-40c7-8068-180f7f5eac6f",
   "metadata": {},
   "source": [
    "We define the prompt template incorporating the system prompt with the user defined message. "
   ]
  },
  {
   "cell_type": "code",
   "id": "5fe2af57-4dbe-4e1b-bf68-7a59d72eb5d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T20:21:29.876339Z",
     "start_time": "2025-02-13T20:21:29.874433Z"
    }
   },
   "source": [
    "norm_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", norm_system_prompt),\n",
    "        (\"human\", \"Here is the customer's question: <question>{question}</question> How do you answer to the instructions?\"),\n",
    "    ]\n",
    ")\n",
    "norm_chain = norm_prompt | nova"
   ],
   "outputs": [],
   "execution_count": 105
  },
  {
   "cell_type": "markdown",
   "id": "bbad2627-6be4-41af-8520-de8879165732",
   "metadata": {},
   "source": [
    "When executing the chain on a longer query, the returned message pulls out only the information necessary to the task at hand."
   ]
  },
  {
   "cell_type": "code",
   "id": "f9467956-8af5-4acc-9197-9a0c34428263",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T20:07:09.334143Z",
     "start_time": "2025-02-13T20:07:08.647094Z"
    }
   },
   "source": [
    "display_markdown(Markdown(norm_chain.invoke({\"question\": \"\"\"I have been all around the world seing a bunch of stuff. \n",
    "I met a bunch of people like Bernard and Tamy. Tell me about my travel history\"\"\"}).content))"
   ],
   "outputs": [
    {
     "data": {
      "text/markdown": "<condensed_message>Requesting information on travel history and people met.</condensed_message>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "id": "80d22a61-b1f2-4de1-9702-214b0c70ebf0",
   "metadata": {},
   "source": [
    "When executing the chain on a query that only has keywords, the model fills in the gap to provide additional context. Although the initial queries are quite different, notice that their resulting output is quite similar."
   ]
  },
  {
   "cell_type": "code",
   "id": "95585830-965a-4bbf-9fef-7304a74b48c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T20:07:45.094986Z",
     "start_time": "2025-02-13T20:07:44.531820Z"
    }
   },
   "source": "display_markdown(Markdown(norm_chain.invoke({\"question\": \"\"\"New York\"\"\"}).content))",
   "outputs": [
    {
     "data": {
      "text/markdown": "<condensed_message>Looking for information on New York.</condensed_message>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 29
  },
  {
   "cell_type": "markdown",
   "id": "acf9ab56-f876-43f0-a9c7-a2d84449e8fb",
   "metadata": {},
   "source": [
    "Once we have detected the message's intent and normalized it to some extent, we are able to have much greater assurance as to the nature of the messages sent to subsequent steps, namely the retrieval."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565f4fb5-c973-42cb-b8ca-64516fcb12c3",
   "metadata": {},
   "source": [
    "## Advanced methods of retrieval\n",
    "\n",
    "The main driver of performance for RAG pipelines is the retrieval mechanism. This step involves identifying a subset of documents that are most relevant to the original query. The common baseline is generally to embed the query in its original form and pull the top-K nearest documents. However, for some datasets this begins to fall short in cases where queries address multiple topics or, more generally, are phrased in a way that is incompatible or is dissimilar to the documents that should be retrieved. We look at how it is possible to improve on these types of queries. \n",
    "\n",
    "Given the increase complexity of the tasks in this section, we choose to leverage Amazon Nova Pro in this part of the pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "id": "85f16644-77cf-4e4a-bc0f-bf5e6d1eade8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T20:09:41.919318Z",
     "start_time": "2025-02-13T20:09:41.912616Z"
    }
   },
   "source": [
    "modelId = \"us.amazon.nova-pro-v1:0\"\n",
    "nova_pro = ChatBedrock(\n",
    "    model_id=modelId,\n",
    "    client=bedrock,\n",
    "    beta_use_converse_api=True\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 44
  },
  {
   "cell_type": "markdown",
   "id": "69f9cdc5-5b83-4be3-be7e-32c96aebc71d",
   "metadata": {},
   "source": [
    "### Decomposition\n",
    "\n",
    "For more complex queries, it may be helpful to breakdown the original question into sub-problems each having their own retrieval step. We perform query decomposition to return the original question or an equivalent set of questions each with a single target.\n",
    "\n",
    "This process is driven by the underlying model. We define the system prompt describing the intended task and supply static few-shot examples to enable the model to better generalize. Removing these examples yields results that are less robust."
   ]
  },
  {
   "cell_type": "code",
   "id": "f5ab2ee7-9785-45ab-a149-6c45b3b2aa69",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T20:09:42.674598Z",
     "start_time": "2025-02-13T20:09:42.672977Z"
    }
   },
   "source": [
    "decomp_system_prompt = \"\"\"You are a expert assistant that prepares queries that will be sent to a search component. \n",
    "These queries may be very complex. Your job is to simplify complex queries into multiple queries that can be answered in isolation to eachother.\n",
    "\n",
    "If the query is simple, then keep it as it is.\n",
    "\n",
    "If there are acronyms or words you are not familiar with, do not try to rephrase them.\n",
    "Here is an example of how to respond in a standard interaction:\n",
    "<example>\n",
    "- Query: Did Meta or Nvidia make more money last year?\n",
    "Decomposed Questions: [SubQuery(sub_query='How much profit did Meta make last year?'), SubQuery(sub_query'How much profit did Nvidia make last year?')]\n",
    "</example>\n",
    "<example>\n",
    "- Query: What is the capital of France?\n",
    "Decomposed Questions: [SubQuery(sub_query='What is the capital of France?')]\n",
    "</example>\"\"\""
   ],
   "outputs": [],
   "execution_count": 45
  },
  {
   "cell_type": "markdown",
   "id": "7abe7d9f-d645-4a8f-9139-ff87a5a9cd2e",
   "metadata": {},
   "source": [
    "To ensure a consistent format is returned for subsequent steps, we use Pydantic, a data-validation library. We rely on a Pydantic-based helper function for doing the tool config translation for us in a way that ensures we avoid potential mistakes when defining our tool config schema in a JSON dictionary.\n",
    "\n",
    "We define `SubQuery` to be a query corresponding to a subset of the points of a larger parent query. "
   ]
  },
  {
   "cell_type": "code",
   "id": "13e37172-db97-44d4-91d7-0f70504be87f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T20:09:43.492896Z",
     "start_time": "2025-02-13T20:09:43.490253Z"
    }
   },
   "source": [
    "class SubQuery(BaseModel):\n",
    "    \"\"\"You have performed query decomposition to generate a subquery of a question\"\"\"\n",
    "\n",
    "    sub_query: str = Field(description=\"A unique subquery of the original question.\")"
   ],
   "outputs": [],
   "execution_count": 46
  },
  {
   "cell_type": "markdown",
   "id": "e1e910b8-2271-4ea8-bc03-a1bdbd37b8ec",
   "metadata": {},
   "source": [
    "We define the prompt template leveraging the previously defined system prompt. We then expose `SubQuery` as a tool the model can leverage. This enables to model to format one or more requests to this tool."
   ]
  },
  {
   "cell_type": "code",
   "id": "1cf57154-3307-407a-844f-41576bfd0b64",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T20:09:45.239217Z",
     "start_time": "2025-02-13T20:09:45.036928Z"
    }
   },
   "source": [
    "query_decomposition_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", decomp_system_prompt),\n",
    "        (\"human\", \"Here is the customer's question: <question>{question}</question> How do you answer to the instructions?\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "llm_with_tools = nova_pro.bind_tools([SubQuery])\n",
    "decomp_query_analyzer = query_decomposition_prompt | llm_with_tools | PydanticToolsParser(tools=[SubQuery])"
   ],
   "outputs": [],
   "execution_count": 47
  },
  {
   "cell_type": "markdown",
   "id": "bb9173ba-2af7-4dc8-a708-8d32db799fc5",
   "metadata": {},
   "source": [
    "We asking a broad question about multiple destinations, the model chooses to return multiple calls to `SubQuery`. Each can be sent for document retrieval in parallel, thus ensuring we do not encure additional latency beyond that of the model inferencing. "
   ]
  },
  {
   "cell_type": "code",
   "id": "f7566eca-fc92-41ff-ba5f-ae588540217e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T20:10:42.431837Z",
     "start_time": "2025-02-13T20:10:40.935080Z"
    }
   },
   "source": [
    "queries = decomp_query_analyzer.invoke({\"question\": \"How do go on vacation in thailand and in California?\"})\n",
    "queries"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SubQuery(sub_query='How do you go on vacation in Thailand?'),\n",
       " SubQuery(sub_query='How do you go on vacation in California?')]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 50
  },
  {
   "cell_type": "markdown",
   "id": "aec0ab1a-90ea-473c-92e7-4c5fb4108b57",
   "metadata": {},
   "source": [
    "### Expansion\n",
    "\n",
    "Query expansion is similar to decomposition in that it produces multiple queries as a strategy to improve the odds of hitting a relevant result. However, expansion returns multiple different wordings of the original query.  \n",
    "\n",
    "We define the system prompt to consistently return 3 versions of the original query. "
   ]
  },
  {
   "cell_type": "code",
   "id": "4cbcd9b3-a6fb-456f-88ba-be9eb40d240f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T20:10:44.839343Z",
     "start_time": "2025-02-13T20:10:44.837204Z"
    }
   },
   "source": [
    "paraphrase_system_prompt = \"\"\"You are an expert at converting user questions into database queries. \n",
    "You have access to a database of travel destinations and a list of recent destinations for travelers. \n",
    "\n",
    "Perform query expansion. If there are multiple common ways of phrasing a user question \n",
    "or common synonyms for key words in the question, make sure to return multiple versions \n",
    "of the query with the different phrasings.\n",
    "\n",
    "If there are acronyms or words you are not familiar with, do not try to rephrase them.\n",
    "\n",
    "Always return at least 3 versions of the question.\"\"\""
   ],
   "outputs": [],
   "execution_count": 51
  },
  {
   "cell_type": "markdown",
   "id": "fc546c0b-64eb-4463-a93b-4b5a3c6a3bd7",
   "metadata": {},
   "source": [
    "We define the prompt template leveraging the previously defined system prompt. We then expose `ParaphrasedQuery` as a tool the model can leverage. This enables to model to format one or more requests to this tool."
   ]
  },
  {
   "cell_type": "code",
   "id": "5676aca2-d322-4dbe-bc7d-a3ab5ece2796",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T20:10:46.572300Z",
     "start_time": "2025-02-13T20:10:46.568609Z"
    }
   },
   "source": [
    "class ParaphrasedQuery(BaseModel):\n",
    "    \"\"\"You have performed query expansion to generate a paraphrasing of a question.\"\"\"\n",
    "\n",
    "    paraphrased_query: str = Field(description=\"A unique paraphrasing of the original question.\")"
   ],
   "outputs": [],
   "execution_count": 52
  },
  {
   "cell_type": "markdown",
   "id": "219d3baf-47ae-45db-838e-46bcb89071fd",
   "metadata": {},
   "source": [
    "We define the prompt template leveraging the previously defined system prompt. We then expose `ParaphrasedQuery` as a tool the model can leverage. This enables to model to format one or more requests to this tool."
   ]
  },
  {
   "cell_type": "code",
   "id": "61071899-b1d1-42e4-a91f-dbf3e5bb5859",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T20:10:54.156134Z",
     "start_time": "2025-02-13T20:10:54.089587Z"
    }
   },
   "source": [
    "query_expansion_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", paraphrase_system_prompt),\n",
    "        (\"human\", \"Here is the customer's question: <question>{question}</question> How do you answer to the instructions?\"),\n",
    "    ]\n",
    ")\n",
    "llm_with_tools = nova_pro.bind_tools([ParaphrasedQuery])\n",
    "query_expansion = query_expansion_prompt | llm_with_tools | PydanticToolsParser(tools=[ParaphrasedQuery])"
   ],
   "outputs": [],
   "execution_count": 54
  },
  {
   "cell_type": "markdown",
   "id": "b39c037c-6b4f-4768-943b-19e9dd023b16",
   "metadata": {},
   "source": [
    "Now no matter the nature of the query, the model generates alternatives that can be sent for retrieval in parallel."
   ]
  },
  {
   "cell_type": "code",
   "id": "c5470a22-899a-4bb9-8224-ce9c3cd2a859",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T20:10:57.646048Z",
     "start_time": "2025-02-13T20:10:55.661343Z"
    }
   },
   "source": [
    "query_expansion.invoke({\"question\": \"how to use travel to Canada and to Mexico?\"})"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ParaphrasedQuery(paraphrased_query='What are the steps to travel to Canada and Mexico?'),\n",
       " ParaphrasedQuery(paraphrased_query='How can I plan a trip to both Canada and Mexico?'),\n",
       " ParaphrasedQuery(paraphrased_query='What do I need to know to travel to Canada and Mexico?')]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 55
  },
  {
   "cell_type": "markdown",
   "id": "c56aaaf5-ac64-4e58-92a9-898be2174e91",
   "metadata": {},
   "source": [
    "### Hypothetical Document Embeddings (HyDE)\n",
    "\n",
    "Given that models have been trained large volumes of data, we can generate a relevant hypothetical document to answer the user question. Then for retrieval, this new (or *hypethetical*) document can be embedded with the original query. This approach has been shown in [Precise Zero-Shot Dense Retrieval without Relevance Labels](https://arxiv.org/abs/2212.10496) to improve recall. We define the system prompt relevant to this task."
   ]
  },
  {
   "cell_type": "code",
   "id": "5ed9886a-707c-4782-bdc1-0239f54134b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T20:11:00.073889Z",
     "start_time": "2025-02-13T20:11:00.072316Z"
    }
   },
   "source": [
    "hyde_system_prompt = \"\"\"You are an expert about travel destinations all over the worlds. Your task is to provide your best response based on the question.\n",
    "You need to produce a high-quality and complete sentence hyper focused on answer the question. \n",
    "Do not answer in bulletpoints.\n",
    "\n",
    "Think about your answer first before you respond. Think step-by-step and the answer in <hyde></hyde> tags and do not include anything after.\"\"\""
   ],
   "outputs": [],
   "execution_count": 56
  },
  {
   "cell_type": "markdown",
   "id": "8f2de46d-651b-4b7f-af83-273966a03478",
   "metadata": {},
   "source": [
    "We define the prompt template leveraging the previously defined system prompt."
   ]
  },
  {
   "cell_type": "code",
   "id": "f741b5a3-481c-467b-b454-40d243232747",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T20:11:05.606832Z",
     "start_time": "2025-02-13T20:11:05.603928Z"
    }
   },
   "source": [
    "hyde_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", hyde_system_prompt),\n",
    "        (\"human\", \"Here is the customer's question: <question>{question}</question> How do you answer to the instructions?\"),\n",
    "    ]\n",
    ")\n",
    "hyde_chain = hyde_prompt | nova_pro | StrOutputParser()"
   ],
   "outputs": [],
   "execution_count": 57
  },
  {
   "cell_type": "markdown",
   "id": "2f319385-8b34-489b-84d9-8ec921e4853c",
   "metadata": {},
   "source": [
    "We produce a document for the query in between tags that is be appended at retrieval time."
   ]
  },
  {
   "cell_type": "code",
   "id": "4a2467e3-94d9-461d-9a76-c3f8bb7862a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T20:11:21.309801Z",
     "start_time": "2025-02-13T20:11:17.727658Z"
    }
   },
   "source": [
    "queries = hyde_chain.invoke({\"question\": \"How do go on vacation in thailand and in California?\"})\n",
    "display_markdown(Markdown(queries))"
   ],
   "outputs": [
    {
     "data": {
      "text/markdown": "<hyde>To answer the customer's question about vacationing in Thailand and California, I need to provide a comprehensive and detailed response that covers essential aspects of planning a trip to both destinations. This includes information on the best times to visit, must-see attractions, travel tips, and any specific cultural or logistical considerations for each location. Thailand and California offer vastly different experiences, so the response should highlight unique features and activities available in each place. Additionally, practical advice on transportation, accommodation, and local customs will enhance the usefulness of the response.</hyde>\n\nTo go on vacation in Thailand, plan your trip around the cool and dry season from November to February, explore iconic sites like the Grand Palace and Wat Pho in Bangkok, and immerse yourself in local culture by visiting vibrant night markets and participating in traditional festivals, while in California, choose between the sunny beaches of Southern California, the tech-savvy atmosphere of Silicon Valley, or the wine country of Napa and Sonoma, and enjoy diverse activities such as surfing in Malibu, hiking in Yosemite National Park, and experiencing the vibrant city life in Los Angeles or San Francisco."
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 58
  },
  {
   "cell_type": "markdown",
   "id": "841d8d70-8801-40f6-bc29-67d10de85228",
   "metadata": {},
   "source": [
    "In this section we demonstrated the possiblity of augmented the original message to produce stronger results. Naturally, this LLM-driven approach requires an additional inference, which introduces some additional latency.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c2aa6a-d144-4b0b-a5ca-7c8cd2ce2d4e",
   "metadata": {},
   "source": [
    "## Model answer generation\n",
    "\n",
    "In most RAG pipelines, the number of documents shown to the model is driven by the retrieval mechanism. This generally returns up to some static number of documents provided they meeting the necessary similarity treshold. Often, this results in irrelevant documents being sent to the model for inference. Although we can easily intruct the model to ignore irrelevant documents, it is often useful for the model to explicitly call-out the documents it did use. Furthermore, many lines of research have demonstrated the effectiveness of enabling the model to correct itself. In both cases, we make an additional call to the model once an initial answer is generated in order to improve the output for the end-user. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da954d0-e9a9-4d66-a8bd-e385d175ed6a",
   "metadata": {},
   "source": [
    "### Citation\n",
    "\n",
    "We generate an output with `answer` and `docs` keys. `docs` contains a list of Langchain `Document` objects. These are the documents the model has picked as being relevant to answering the original query. Although the documents are currently returned with title and summaries, these keys are part of a `metadata` attribute letting you determine any number of field that may be relevant to be used by your application such as author, source URL, etc... \n",
    "\n",
    "We define the system prompt to generate the model answer. Note that this is a simple template that can be further augmented with additional sections better describing our task and intended output."
   ]
  },
  {
   "cell_type": "code",
   "id": "42545ca3-8139-4e53-b30b-351d64687ed7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T20:11:26.233377Z",
     "start_time": "2025-02-13T20:11:26.231305Z"
    }
   },
   "source": [
    "citation_system_prompt = \"\"\"You're a helpful AI assistant. Given a user question and some article snippets, answer the user question. \n",
    "If none of the articles answer the question, just say you don't know.\n",
    "\n",
    "Here are the articles: {context}\n",
    "\"\"\""
   ],
   "outputs": [],
   "execution_count": 59
  },
  {
   "cell_type": "markdown",
   "id": "5d6282c2-7963-4b49-b3d9-5897d9cf5204",
   "metadata": {},
   "source": [
    "This prompt is past as part the broader chat template."
   ]
  },
  {
   "cell_type": "code",
   "id": "30c8d42b-b30c-4ed2-888b-3cf92bb2095b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T20:11:30.988556Z",
     "start_time": "2025-02-13T20:11:30.984968Z"
    }
   },
   "source": [
    "citation_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", citation_system_prompt),\n",
    "        (\"human\", \"Here is the customer's question: <question>{question}</question> How do you answer to the instructions?\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "answer_generator = citation_prompt | nova_pro | StrOutputParser()"
   ],
   "outputs": [],
   "execution_count": 60
  },
  {
   "cell_type": "markdown",
   "id": "b920107b-6736-4828-9be1-5fc4a72dfd3c",
   "metadata": {},
   "source": [
    "Lets use the `WikipediaRetriever` allowing us to interact with the Wikipedia API."
   ]
  },
  {
   "cell_type": "code",
   "id": "54740d6b-f8ee-4a3c-a3b5-0bbcf264fad3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T20:12:10.434209Z",
     "start_time": "2025-02-13T20:12:10.383291Z"
    }
   },
   "source": [
    "wiki = WikipediaRetriever(top_k_results=6, doc_content_chars_max=2000)"
   ],
   "outputs": [],
   "execution_count": 62
  },
  {
   "cell_type": "markdown",
   "id": "3cb40110-0144-489f-ab17-32b610c8eb8f",
   "metadata": {},
   "source": [
    "The `format_docs` helper function is used to format the documents returned by the retriever to make them more friendly to the model. We supply the document's title and summary snippet. At the end, we pass the function to a child of Lanchain's `Runnable` class. This simply enables us to call the function with a standard API (invoke, batch, stream, transform and compose). Many object in Langchain implement this interface including `BaseModel`. \n",
    "\n",
    "To demonstrate the power of citations, we also append an additional obviously irrelevant document to the formatted documents."
   ]
  },
  {
   "cell_type": "code",
   "id": "954c48d0-df8c-4f95-be9e-51dae41d0e5c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T20:12:12.188942Z",
     "start_time": "2025-02-13T20:12:12.186041Z"
    }
   },
   "source": [
    "def format_docs(docs: List[Document]) -> str:\n",
    "    \"\"\"Convert Documents to a single string.:\"\"\"\n",
    "    formatted = [\n",
    "        f\"Article Title: {doc.metadata['title']}\\nArticle Snippet: {doc.page_content}\"\n",
    "        for doc in docs\n",
    "    ]\n",
    "    formatted.append(\"Article Title: This is an irrelevant document \\\n",
    "    Article Snippet: The document is most irrelevant.\")\n",
    "    return \"\\n\\n\" + \"\\n\\n\".join(formatted)\n",
    "\n",
    "\n",
    "format = itemgetter(\"docs\") | RunnableLambda(format_docs)"
   ],
   "outputs": [],
   "execution_count": 63
  },
  {
   "cell_type": "markdown",
   "id": "9539ea88-f505-47e2-a6af-9153655077ce",
   "metadata": {},
   "source": [
    "We define a chain as `RunnableParallel` object, which is an extention of `Runnable` that runs a mapping of Runnables in parallel, and returns a mapping of their outputs. We set the question property using `RunnablePassthrough`. This passes the input unchanged. Then, we assign values to keys in the prompt templates. "
   ]
  },
  {
   "cell_type": "code",
   "id": "18dab078-a3ca-4873-9534-65cb48d731cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T20:12:15.796567Z",
     "start_time": "2025-02-13T20:12:15.790509Z"
    }
   },
   "source": [
    "citation_chain = (\n",
    "    RunnableParallel(question=RunnablePassthrough(), docs=wiki)\n",
    "    .assign(context=format)\n",
    "    .assign(answer=answer_generator)\n",
    "    .pick([\"answer\", \"docs\"])\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 64
  },
  {
   "cell_type": "markdown",
   "id": "cd5fb5bb-c78f-4ba9-9693-764f3e1c39a9",
   "metadata": {},
   "source": [
    "When invoking the chain, it returns the original answer and the documents used for generation. Notice that some documents are relevant to the final answer and some are not. We can address this challenge with further LLM or metadata document filtering."
   ]
  },
  {
   "cell_type": "code",
   "id": "ea89c0af-063e-4c40-bf6b-961b5d920c47",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T20:12:28.882199Z",
     "start_time": "2025-02-13T20:12:19.915919Z"
    }
   },
   "source": [
    "citation_chain.invoke(\"How do go on vacation in thailand and in California?\")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': \"Here's how you can go on vacation in Thailand and California:\\n\\n### Vacation in Thailand\\n\\n1. **Plan Your Itinerary**: Decide which cities or regions you want to visit. Popular destinations include Bangkok, Phuket, Chiang Mai, and Krabi.\\n2. **Book Flights**: Find flights to major airports like Suvarnabhumi Airport (BKK) in Bangkok or Phuket International Airport (HKT).\\n3. **Accommodation**: Book hotels, resorts, or Airbnbs in your chosen locations. Consider the type of experience you want (luxury, budget, etc.).\\n4. **Visa**: Check if you need a visa. Many countries can enter Thailand visa-free for a limited period.\\n5. **Transportation**: Arrange for local transportation. Options include taxis, tuk-tuks, Grab (ride-hailing service), and public transport.\\n6. **Experience Local Culture**: Visit temples, markets, and try local cuisine. Participate in cultural activities or tours.\\n\\n### Vacation in California\\n\\n1. **Plan Your Itinerary**: Decide which cities or regions you want to visit. Popular destinations include Los Angeles, San Francisco, San Diego, and Napa Valley.\\n2. **Book Flights**: Find flights to major airports like Los Angeles International Airport (LAX), San Francisco International Airport (SFO), or San Diego International Airport (SAN).\\n3. **Accommodation**: Book hotels, resorts, or Airbnbs in your chosen locations. Consider proximity to attractions.\\n4. **Rental Car**: Consider renting a car for flexibility, especially if you plan to visit multiple cities or national parks.\\n5. **Attractions**: Plan visits to popular attractions like Hollywood, Golden Gate Bridge, Disneyland, beaches, and national parks.\\n6. **Local Activities**: Enjoy local activities such as wine tasting in Napa Valley, hiking, and exploring city attractions.\\n\\nBoth destinations offer a variety of experiences, from urban adventures to natural beauty, ensuring a memorable vacation.\",\n",
       " 'docs': [Document(metadata={'title': 'Belinda Carlisle', 'summary': 'Belinda Jo Carlisle ( KAR-lyle; born August 17, 1958) is an American singer and songwriter. She gained fame as the lead vocalist of the Go-Go\\'s, one of the most successful all-female rock bands of all time, and went on to have a prolific career as a solo artist.\\nRaised in Southern California, Carlisle was the lead vocalist of the Go-Go\\'s, which she co-founded in 1978. With their chart-topping debut studio album Beauty and the Beat in 1981, the group helped popularize new wave music in the United States. The Go-Go\\'s have sold over seven million records worldwide.\\nAfter the break-up of the Go-Go\\'s in 1985, Carlisle went on to have a successful solo career with radio hits such as \"Mad About You\", \"I Get Weak\", \"Circle in the Sand\", \"Leave a Light On\", \"Summer Rain\", and \"Heaven Is a Place on Earth\". The Go-Go\\'s reformed in 1999, and Carlisle performed with them until their disbandment in 2022, while also maintaining her solo career.\\nCarlisle\\'s autobiography, Lips Unsealed, published in June 2010, was a New York Times Best Seller and received favorable reviews. In 1999, Carlisle was ranked No. 76 with the Go-Go\\'s in VH1\\'s 100 Greatest Women of Rock & Roll. In 2011, Carlisle, as a member of the Go-Go\\'s, received a star on the Hollywood Walk of Fame. She and the band were inducted into the Rock and Roll Hall of Fame in 2021, and the California Hall of Fame in 2024.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Belinda_Carlisle'}, page_content='Belinda Jo Carlisle ( KAR-lyle; born August 17, 1958) is an American singer and songwriter. She gained fame as the lead vocalist of the Go-Go\\'s, one of the most successful all-female rock bands of all time, and went on to have a prolific career as a solo artist.\\nRaised in Southern California, Carlisle was the lead vocalist of the Go-Go\\'s, which she co-founded in 1978. With their chart-topping debut studio album Beauty and the Beat in 1981, the group helped popularize new wave music in the United States. The Go-Go\\'s have sold over seven million records worldwide.\\nAfter the break-up of the Go-Go\\'s in 1985, Carlisle went on to have a successful solo career with radio hits such as \"Mad About You\", \"I Get Weak\", \"Circle in the Sand\", \"Leave a Light On\", \"Summer Rain\", and \"Heaven Is a Place on Earth\". The Go-Go\\'s reformed in 1999, and Carlisle performed with them until their disbandment in 2022, while also maintaining her solo career.\\nCarlisle\\'s autobiography, Lips Unsealed, published in June 2010, was a New York Times Best Seller and received favorable reviews. In 1999, Carlisle was ranked No. 76 with the Go-Go\\'s in VH1\\'s 100 Greatest Women of Rock & Roll. In 2011, Carlisle, as a member of the Go-Go\\'s, received a star on the Hollywood Walk of Fame. She and the band were inducted into the Rock and Roll Hall of Fame in 2021, and the California Hall of Fame in 2024.\\n\\n\\n== Early life and education ==\\n\\nBelinda Jo Carlisle was born in Hollywood, Los Angeles, California, on August 17, 1958, to Harold Carlisle, a gas station employee, and his wife, Joanne (née Thompson), a homemaker. Her mother met her father, who was 20 years her senior, at age 18, and Carlisle was born nine months later. She was named after her mother\\'s favorite film, Johnny Belinda (1948). Carlisle was the first of seven siblings; she has three brothers and three sisters. When she was five years old, Carlisle\\'s father abandoned their family, and she has stated that she spent most of her childhood impoverished'),\n",
       "  Document(metadata={'title': 'List of minimum annual leave by country', 'summary': 'In the majority of nations, including all industrialised nations except the United States, advances in employee relations have seen the introduction of statutory agreements for minimum employee leave from work—that is the amount of entitlement to paid vacation and public holidays. Companies may offer contractually more time. Companies and the law may also differ as to whether public holidays are counted as part of the minimum leave.\\nDisparities in national minimums are still subject of debate regarding work-life balance and perceived differences between nations. These numbers usually refer to full-time employment – part-time workers may get a reduced number of days. In most countries, public holidays are paid and usually not considered part of the annual leave. Also, in most countries there are additional paid leave benefits such as parental leave and sick leave that are not listed here.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/List_of_minimum_annual_leave_by_country'}, page_content='In the majority of nations, including all industrialised nations except the United States, advances in employee relations have seen the introduction of statutory agreements for minimum employee leave from work—that is the amount of entitlement to paid vacation and public holidays. Companies may offer contractually more time. Companies and the law may also differ as to whether public holidays are counted as part of the minimum leave.\\nDisparities in national minimums are still subject of debate regarding work-life balance and perceived differences between nations. These numbers usually refer to full-time employment – part-time workers may get a reduced number of days. In most countries, public holidays are paid and usually not considered part of the annual leave. Also, in most countries there are additional paid leave benefits such as parental leave and sick leave that are not listed here.\\n\\n\\n== Methodology ==\\nFor the purpose of comparison, the paid vacation column has been normalised to a five-day workweek. For instance, a calendar month is divided by seven and multiplied by five, while a six-day workweek day is divided by six and multiplied by five. The paid vacation column gives the minimum mandatory vacation days for an employee who has one year of service with the same employer.\\nIn some countries, the public holidays are strictly bound to the calendar dates, so if they happen on Saturday or Sunday, they are \"lost\" for that year. As a result, the average number of paid extra free days can be lower than the table shows. For example, in the Czech Republic, where the official number of paid public holidays is 13, the average number of public holidays during working days in the years 2000–2016 was only 8.9 days. In other countries, such as the United Kingdom and the United States, the public holidays which would fall on Saturday or Sunday are moved to the nearest Monday or Friday.\\n\\n\\n== Countries ==\\n\\n\\n== See also ==\\nAnnual leave\\nLong service leave\\nParental leave\\nList of'),\n",
       "  Document(metadata={'title': 'The Hangover Part II', 'summary': 'The Hangover Part II is a 2011 American comedy film produced by Legendary Pictures and distributed by Warner Bros. Pictures. The sequel to the 2009 film The Hangover and the second installment in The Hangover trilogy, the film was directed by Todd Phillips, who co-wrote the script with Craig Mazin and Scot Armstrong, and stars Bradley Cooper, Ed Helms, Zach Galifianakis, Ken Jeong, Jeffrey Tambor, Justin Bartha, and Paul Giamatti.\\nIt tells the story of Phil, Stu, Alan, and Doug, as they travel to Thailand. After the bachelor party in Las Vegas, Stu takes no chances and opts for a safe, subdued pre-wedding brunch. Things do not go as planned, resulting in another bad hangover with no memories of the previous night.\\nDevelopment began in April 2009, two months before The Hangover was released. The principal actors were cast in March 2010 to reprise their roles from the first film. Production began in October 2010, in Ontario, California, before moving on location in Thailand. The film was released on May 26, 2011, and became the eighth-highest-grossing film of 2011 and the highest-grossing R-rated comedy during its theatrical run, but unlike the first film, The Hangover Part II received mixed reviews.\\nA third installment, The Hangover Part III, was released on May 24, 2013.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/The_Hangover_Part_II'}, page_content=\"The Hangover Part II is a 2011 American comedy film produced by Legendary Pictures and distributed by Warner Bros. Pictures. The sequel to the 2009 film The Hangover and the second installment in The Hangover trilogy, the film was directed by Todd Phillips, who co-wrote the script with Craig Mazin and Scot Armstrong, and stars Bradley Cooper, Ed Helms, Zach Galifianakis, Ken Jeong, Jeffrey Tambor, Justin Bartha, and Paul Giamatti.\\nIt tells the story of Phil, Stu, Alan, and Doug, as they travel to Thailand. After the bachelor party in Las Vegas, Stu takes no chances and opts for a safe, subdued pre-wedding brunch. Things do not go as planned, resulting in another bad hangover with no memories of the previous night.\\nDevelopment began in April 2009, two months before The Hangover was released. The principal actors were cast in March 2010 to reprise their roles from the first film. Production began in October 2010, in Ontario, California, before moving on location in Thailand. The film was released on May 26, 2011, and became the eighth-highest-grossing film of 2011 and the highest-grossing R-rated comedy during its theatrical run, but unlike the first film, The Hangover Part II received mixed reviews.\\nA third installment, The Hangover Part III, was released on May 24, 2013.\\n\\n\\n== Plot ==\\nStu Price will travel to Thailand for his upcoming wedding to Lauren, his fiancée. To avoid what happened in Las Vegas, Stu does not allow his three best friends, Doug Billings, Phil Wenneck and Alan Garner to throw him a bachelor party. He instead hosts his bachelor party at IHOP with Phil and Doug.\\nTracy gets Doug to convince Stu to allow her brother Alan to accompany them to Thailand. Stu allows his friends (including Alan), Tracy and Phil's wife Stephanie to go with him. At the airport, they are joined by Lauren's 16-year-old brother Teddy, a Stanford University scholar.\\nAt the rehearsal dinner, Lauren's father Fong reveals his disapproval of Stu during a toast. Later that night, St\"),\n",
       "  Document(metadata={'title': 'Resort town', 'summary': 'A resort town, resort city or resort destination is an urban area where tourism or vacationing is the primary component of the local culture and economy.  A typical resort town has one or more actual resorts in the surrounding area. Sometimes the term resort town is used simply for a locale popular among tourists. One task force in British Columbia used the definition of an incorporated or unincorporated contiguous area where the ratio of transient rooms, measured in bed units, is greater than 60% of the permanent population.\\nGenerally, tourism is the main export in a resort town economy, with most residents of the area working in the tourism or resort industry. Shops and luxury boutiques selling locally themed souvenirs, motels, and unique restaurants often proliferate the downtown areas of a resort town.\\nIn the case of the United States, resort towns were created around the late 1800s and early 1900s with the development of early town-making. Many resort towns feature ambitious architecture, romanticizing their location, and dependence on cheap labor.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Resort_town'}, page_content='A resort town, resort city or resort destination is an urban area where tourism or vacationing is the primary component of the local culture and economy.  A typical resort town has one or more actual resorts in the surrounding area. Sometimes the term resort town is used simply for a locale popular among tourists. One task force in British Columbia used the definition of an incorporated or unincorporated contiguous area where the ratio of transient rooms, measured in bed units, is greater than 60% of the permanent population.\\nGenerally, tourism is the main export in a resort town economy, with most residents of the area working in the tourism or resort industry. Shops and luxury boutiques selling locally themed souvenirs, motels, and unique restaurants often proliferate the downtown areas of a resort town.\\nIn the case of the United States, resort towns were created around the late 1800s and early 1900s with the development of early town-making. Many resort towns feature ambitious architecture, romanticizing their location, and dependence on cheap labor.\\n\\n\\n== Resort town economy ==\\nIf the resorts or tourist attractions are seasonal in nature (such as a ski resort), resort towns typically experience an on-season where the town is bustling with tourists and workers, and an off-season where the town is populated only by a small amount of local year-round residents.\\nIn addition, resort towns are often popular with wealthy retirees and people wishing to purchase vacation homes, which typically drives up property values and the cost of living in the region. Sometimes, resort towns can become boomtowns due to the quick development of retirement and vacation-based residences.\\nHowever, most of the employment available in resort towns are typically low paying and it can be difficult for workers to afford to live the area in which they are employed. Many resort towns have spawned nearby bedroom communities where the majority of the resort workforce lives.\\nResorts towns sometime'),\n",
       "  Document(metadata={'title': 'Pump (album)', 'summary': 'Pump is the tenth studio album by American rock band Aerosmith. It was released on September 12, 1989, by Geffen Records. The album peaked at No. 5 on the US charts, and was certified septuple platinum by the RIAA in 1995.\\nThe album contains the hit singles \"Love in an Elevator\", \"The Other Side\", \"What It Takes\", \"Janie\\'s Got a Gun\", which all entered the Top 40 of the Hot 100. It also has certified sales of seven million copies in the U.S. to date, and is tied with its successor Get a Grip as Aerosmith\\'s second best-selling studio album in the U.S. (Toys in the Attic leads with nine million). It produced a variety of successes and \"firsts\" for the band including their first Grammy Award (\"Janie\\'s Got a Gun\"). \"Love in an Elevator\" became the first Aerosmith song to hit number one on the Mainstream Rock Tracks chart. The album was the fourth best-selling album of the year 1990.\\nIn the UK, it was the second Aerosmith album to be certified Silver (60,000 units sold) by the British Phonographic Industry, achieving this in September 1989.\\nPump was the second of three sequentially recorded Aerosmith albums to feature producer Bruce Fairbairn and engineers Mike Fraser and Ken Lomas at Little Mountain Sound Studios.\\nTwo video documentaries on the recording, Things That Go Pump in the Night and The Making of Pump, were released on VHS and LaserDisc in 1990, with the latter also released on DVD in 1997.', 'source': 'https://en.wikipedia.org/wiki/Pump_(album)'}, page_content='Pump is the tenth studio album by American rock band Aerosmith. It was released on September 12, 1989, by Geffen Records. The album peaked at No. 5 on the US charts, and was certified septuple platinum by the RIAA in 1995.\\nThe album contains the hit singles \"Love in an Elevator\", \"The Other Side\", \"What It Takes\", \"Janie\\'s Got a Gun\", which all entered the Top 40 of the Hot 100. It also has certified sales of seven million copies in the U.S. to date, and is tied with its successor Get a Grip as Aerosmith\\'s second best-selling studio album in the U.S. (Toys in the Attic leads with nine million). It produced a variety of successes and \"firsts\" for the band including their first Grammy Award (\"Janie\\'s Got a Gun\"). \"Love in an Elevator\" became the first Aerosmith song to hit number one on the Mainstream Rock Tracks chart. The album was the fourth best-selling album of the year 1990.\\nIn the UK, it was the second Aerosmith album to be certified Silver (60,000 units sold) by the British Phonographic Industry, achieving this in September 1989.\\nPump was the second of three sequentially recorded Aerosmith albums to feature producer Bruce Fairbairn and engineers Mike Fraser and Ken Lomas at Little Mountain Sound Studios.\\nTwo video documentaries on the recording, Things That Go Pump in the Night and The Making of Pump, were released on VHS and LaserDisc in 1990, with the latter also released on DVD in 1997.\\n\\n\\n== Production ==\\nIn December 1988, Aerosmith got together at Rik Tinory Productions in Cohasset, Massachusetts to rehearse and compose new songs, as the band members thought the isolated nature of the studio would help their creativity. Over 19 songs were written, split between an \"A-list\" with songs considered possible hits, such as \"Love in an Elevator\" and \"What It Takes\", and the \"B-list\" having songs yet to be developed such as \"Voodoo Medicine Man\". Producer Bruce Fairbairn focused on getting as many hooks on the songs as possible.\\nSome songs proposed for the album, '),\n",
       "  Document(metadata={'title': 'Patrick Schwarzenegger', 'summary': 'Patrick Arnold Shriver Schwarzenegger (born September 18, 1993) is an American actor. He is the son of Arnold Schwarzenegger and Maria Shriver. He began his career playing minor roles in the early 2000s, and has since starred in the television series The Staircase (2022), American Sports Story (2024), and the third season of The White Lotus (2025).\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Patrick_Schwarzenegger'}, page_content=\"Patrick Arnold Shriver Schwarzenegger (born September 18, 1993) is an American actor. He is the son of Arnold Schwarzenegger and Maria Shriver. He began his career playing minor roles in the early 2000s, and has since starred in the television series The Staircase (2022), American Sports Story (2024), and the third season of The White Lotus (2025).\\n\\n\\n== Early life and family ==\\nSchwarzenegger was born on September 18, 1993, at Providence St John's Health Center in Santa Monica, California, and raised in Los Angeles, California. He is the eldest son of author and journalist Maria Shriver and Austrian-born bodybuilder, actor, and former Governor of California Arnold Schwarzenegger. Schwarzenegger has two older sisters, Katherine and Christina, a younger brother, Christopher, and a younger paternal half-brother, Joseph Baena. He holds dual Austrian-American citizenship, speaks English and a little German, and regularly visits Austria. He is a cousin of Patrick M. Knapp Schwarzenegger.\\nWhile studying at Brentwood High School, Schwarzenegger took private acting lessons with Nancy Banks. In 2012, he matriculated at the University of Southern California, and graduated from the USC Marshall School of Business with a minor in Cinematic Arts in May 2016. He was part of the Lambda Chi Alpha fraternity. \\n\\n\\n== Career ==\\n\\n\\n=== Acting ===\\n\\nAt the age of 10, Schwarzenegger had a small role in the film The Benchwarmers. Throughout his childhood, he practiced acting with his father, and through his young adult years, he studied theater at USC while taking acting classes weekly at Nancy Banks Studio. At 15 years old, he started a clothing line.\\nIn the following years, Schwarzenegger had supporting roles in 2012's Stuck in Love, 2013's Grown Ups 2 and 2015's Scouts Guide to the Zombie Apocalypse. His first leading role was opposite Bella Thorne in 2018's Midnight Sun, a romantic drama about a teenage girl with a rare medical condition. His performance in the 2019 horror Daniel Isn't Re\")]}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 65
  },
  {
   "cell_type": "markdown",
   "id": "cb664b45-a5cf-4c7a-865c-706b96a27385",
   "metadata": {},
   "source": [
    "### Self-validation\n",
    "\n",
    "Giving the model an opportunity to correct itself has been shown to increase performance on a number of tasks. We perform self-validation and define a set of formatting rules that align with the conversational tone we expect to have from our application. We define a system prompt with this task and set of rules."
   ]
  },
  {
   "cell_type": "code",
   "id": "dcb47a9d-ce6b-44db-9a1e-33fbf3c9dfff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T20:12:38.454961Z",
     "start_time": "2025-02-13T20:12:38.451796Z"
    }
   },
   "source": [
    "valid_system_prompt = \"\"\"You are a validator and message synthesize. \n",
    "Your task is to create one coherent answer and double check the original responses to the question {question} for common mistakes, including:\n",
    "- Answer in bullet points. It should be a complete paragraph instead.\n",
    "- Inaccuracies or things that seem impossible\n",
    "\n",
    "If there are any of the above mistakes, rewrite the response. If there are no mistakes, just reproduce the original response.\n",
    "Think about your answer first before you respond. \n",
    "If some exist, put all the issues and then put your final response in <validation></validation> tags and do not include anything after.\n",
    "\"\"\""
   ],
   "outputs": [],
   "execution_count": 66
  },
  {
   "cell_type": "markdown",
   "id": "36a703b1-5978-4d26-9b20-57bc61a4e980",
   "metadata": {},
   "source": [
    "We define the prompt template with the system prompt and original model answer."
   ]
  },
  {
   "cell_type": "code",
   "id": "005d6b62-78bb-4086-abfe-38454b2420b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T20:12:44.098053Z",
     "start_time": "2025-02-13T20:12:44.096156Z"
    }
   },
   "source": [
    "validation_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", valid_system_prompt), \n",
    "        (\"human\", \"Here is the original message produced: <orignal_message>{original}</orignal_message> How do you answer to the instructions?\")]\n",
    ")\n",
    "validation_chain = validation_prompt | nova_pro | StrOutputParser()"
   ],
   "outputs": [],
   "execution_count": 67
  },
  {
   "cell_type": "markdown",
   "id": "686fb39d-3246-4b02-873e-593c4fef3b74",
   "metadata": {},
   "source": [
    "We invoke model, which points out obvious issues in the original document and answers with a more consistent alternative. "
   ]
  },
  {
   "cell_type": "code",
   "id": "f3fa985b-d328-4f1b-9989-8d1dcaecf0c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T20:13:06.413602Z",
     "start_time": "2025-02-13T20:13:04.060042Z"
    }
   },
   "source": [
    "validation = validation_chain.invoke({\n",
    "    \"question\" : \"how to go to thailand from Montreal?\",\n",
    "    \"original\": \"1- by plane 2-by car.\",\n",
    "})\n",
    "display_markdown(Markdown(validation))"
   ],
   "outputs": [
    {
     "data": {
      "text/markdown": "The original message contains inaccuracies and is overly simplistic. Traveling from Montreal to Thailand by car is not feasible due to geographical constraints and the need to cross multiple countries and bodies of water. Additionally, the response is presented in bullet points rather than a complete paragraph. \n\nHere is the revised response:\n\n<validation>\nTraveling from Montreal to Thailand typically involves flying, as it is the most practical and direct method. You would need to book a flight from Montreal's Pierre Elliott Trudeau International Airport to a major airport in Thailand, such as Suvarnabhumi Airport in Bangkok. This journey usually requires one or more layovers, depending on the airline and route chosen. It is important to check visa requirements and ensure all travel documents are in order before departure.\n</validation>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 69
  },
  {
   "cell_type": "markdown",
   "id": "4d9bebfc-9aa1-4f33-b4a2-400c7fc5ead8",
   "metadata": {},
   "source": [
    "## Putting it all together \n",
    "\n",
    "The previous components offer important primitives to build a performant RAG solution. They act as building blocks of a broader solution. We provide an example showcasing how they can be brought together in a single chain to improve response accuracy. To minimize latency and improve accuracy, we use Amazon Nova Lite for simpler tasks and Nova Pro where we need more performance. The pipeline is described by the following diagram.\n",
    "\n",
    "<img src=\"./assets/rag-pipeline.png\" width=\"400\" height=\"400\" />\n",
    "\n",
    "First, we create helper functions to parse the return messages for the relevant section that can be found in between tags."
   ]
  },
  {
   "cell_type": "code",
   "id": "34226c16-cea3-4989-8ffe-40390228e68a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T20:13:09.613170Z",
     "start_time": "2025-02-13T20:13:09.610886Z"
    }
   },
   "source": [
    "def parse_intent(ai_message: AIMessage) -> str:\n",
    "    \"\"\"Parse the AI message.\"\"\"\n",
    "    intent_pattern = r\"<intention>(.*?)</intention>\"\n",
    "    intent_match = re.findall(intent_pattern, ai_message.content, flags=0)\n",
    "    if intent_match:\n",
    "        return intent_match[0]\n",
    "    else:\n",
    "        return \"No intention found.\"\n",
    "\n",
    "def parse_norm_message(ai_message: AIMessage) -> str:\n",
    "    \"\"\"Parse the AI message.\"\"\"\n",
    "    norm_pattern = r\"<condensed_message>(.*?)</condensed_message>\"\n",
    "    norm_match = re.findall(norm_pattern, ai_message['question'].content, flags=0)\n",
    "    if norm_match:\n",
    "        return norm_match[0]\n",
    "    else:\n",
    "        return \"Message could not be successfully normalized.\""
   ],
   "outputs": [],
   "execution_count": 70
  },
  {
   "cell_type": "markdown",
   "id": "b75e6b95-a615-408d-8824-33f79a54d83c",
   "metadata": {},
   "source": [
    "We define an end-to-end RAG chain primairly using LangChain Expression Language (LCEL), which allows us to define `Runnable` objects in success to one another. The resulting chain reuses many of the components we previously defined including intent detection with **dynamic few-shots, message normalization and citation**. "
   ]
  },
  {
   "cell_type": "code",
   "id": "2b6b651f-3b9b-4fe3-aff2-26e5b44920e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T20:15:15.295813Z",
     "start_time": "2025-02-13T20:15:06.736669Z"
    }
   },
   "source": [
    "rag_chain = RunnableParallel(\n",
    "    question=RunnablePassthrough(),\n",
    "    intent=few_shot_intent_detection_prompt | nova | parse_intent\n",
    ") | RunnableBranch(\n",
    "    (lambda payload: \"vacation\" == payload[\"intent\"].lower(), lambda x: (\n",
    "        RunnablePassthrough().pick([\"question\"])\n",
    "        .assign(question=norm_chain)\n",
    "        .assign(question=parse_norm_message)\n",
    "        .assign(context=lambda inputs: wiki.invoke(inputs[\"question\"]))\n",
    "        .assign(answer=answer_generator)\n",
    "        .pick([\"answer\", \"context\"])\n",
    "    )),\n",
    "    (lambda payload: \"irrelevant\" == payload[\"intent\"].lower(), lambda x: AIMessage(content=\"I am only able to answer questions about travel and vacations.\")),\n",
    "    (lambda payload: \"contact\" == payload[\"intent\"].lower(), lambda x: AIMessage(content=\"I am transfering you to an agent now...\")),\n",
    "    lambda payload: AIMessage(content=\"I am only able to answer questions about travel and vacations.\" )\n",
    ")\n",
    "\n",
    "display_markdown(Markdown(rag_chain.invoke(\"I want to know more about how to plan a vacation?\")['answer']))"
   ],
   "outputs": [
    {
     "data": {
      "text/markdown": "Based on the provided articles, here are some key points to consider when planning a vacation:\n\n1. **Vacation Duration**:\n   - The length of a summer vacation can vary significantly by country. For example, in countries like Spain, Portugal, and Italy, the summer break is typically three months, whereas in countries like Australia, the United Kingdom, and Germany, it may be two to six weeks.\n\n2. **Vacation Rentals**:\n   - HomeAway (now Vrbo) is a marketplace for vacation rentals, offering a variety of properties such as cabins, condos, villas, and more. It can be a useful resource for finding a place to stay during your vacation.\n\n3. **Schedules and Time Management**:\n   - Creating a schedule or timetable can help you manage your vacation time effectively. This can include planning daily activities, knowing the hours of operation for attractions, and setting times for specific events.\n\n4. **Entertainment**:\n   - If you’re looking for vacation-themed entertainment, the film \"RV\" (2006) is a comedy about a family road trip that might provide some inspiration or simply entertainment during your planning phase.\n\nIf you need more specific information or have particular questions about your vacation plans, feel free to ask!"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 76
  },
  {
   "cell_type": "markdown",
   "id": "4d4040b0-eaae-4f12-a98a-cdc777392b52",
   "metadata": {},
   "source": [
    "It is evident that latency is increased in corralation with the number calls being made in succession. Hence, it is optimal to make calls in parallel where possible to reduce overall time to execute the entire pipeline. Notice in in our example that the intent detection could be made in parallel to message normalization and citation (model inference).\n",
    "\n",
    "Additionally, it may be benifitial to modify the pipeline to include a query augmentation step for reasons described earlier in the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d52a0ce-4d5e-4f0f-bd01-326a249fa4a1",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "Where RAG enables single-turn conversations where users and agents alternate sending eachother messages, agents supply the ability to the application developer to build increased complexity into the conversation flow. These applications are characterized by increase **autonomy, reactivity, proactiveness, adaptability and situatedness**. They typically have some form of validation, the ability to loop back and call external functions to improve outputs. You can dive deeper into agents in the next lab of this workshop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77518672-412b-4ddc-a548-ebae5c3ff12e",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "\n",
    "There is no necessary clean up for this notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
