{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b53c359-3beb-4187-8e1b-9f8e38cb919b",
   "metadata": {},
   "source": [
    "# Build a contextual text and image search engine for product recommendations using Amazon Bedrock (Titan Multimodal Embedding) and Amazon OpenSearch Serverless"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724a8cce-b821-46e6-b802-32f56ed0234c",
   "metadata": {
    "tags": []
   },
   "source": [
    "The solution presented provides an implementation for building a Amazon Titan Multilodal Embedding Model powered search engine prototype to retrieve and recommend products based on text or image queries. This is a step-by-step guide on how to use [Amazon Bedrock Titan models](https://aws.amazon.com/bedrock/titan) to encode images and text into embeddings, ingest embeddings into [Amazon OpenSearch Service](https://aws.amazon.com/opensearch-service/) index, and query the index using OpenSearch Service [k-nearest neighbors (KNN) functionality](https://docs.aws.amazon.com/opensearch-service/latest/developerguide/knn.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99eadd13-b0d0-4522-9a82-316e4717e3fb",
   "metadata": {},
   "source": [
    "It's recommended to execute the notebook in SageMaker Studio Notebooks `Python 3.0(Data Science)` Kernel with `ml.t3.medium` instance.\n",
    "\n",
    "This notebook has been borrrowed from -- Bedrock samples link here -- [MultiModal Embeddings](https://github.com/aws-samples/amazon-bedrock-samples/tree/main/multimodal/titan-multimodal-embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df481b1e-5052-4bd4-a906-9a6fe87c16ca",
   "metadata": {},
   "source": [
    "Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e87b5af-e909-4941-b5e4-33341b96a4a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install opensearch-py\n",
    "!pip install requests-aws4auth\n",
    "!pip install -U boto3\n",
    "!pip install -U botocore\n",
    "!pip install -U awscli\n",
    "!pip install s3fs\n",
    "!pip install sns\n",
    "!pip install seaborn\n",
    "!pip install sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a684c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U --no-cache-dir boto3\n",
    "%pip install -U --no-cache-dir  \\\n",
    "    \"langchain>=0.1.11\" \\\n",
    "    sqlalchemy -U \\\n",
    "    \"faiss-cpu>=1.7,<2\" \\\n",
    "    \"pypdf>=3.8,<4\" \\\n",
    "    pinecone-client==2.2.4 \\\n",
    "    apache-beam==2.52. \\\n",
    "    tiktoken==0.5.2 \\\n",
    "    \"ipywidgets>=7,<8\" \\\n",
    "    matplotlib==3.8.2 \\\n",
    "    anthropic==0.9.0\n",
    "%pip install -U --no-cache-dir transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36de85df",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sqlalchemy --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3391fc25-1843-4339-9673-9f08adcf1b2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# restart kernel\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621adbe6-d2b4-4f38-9b1c-44bba2bc64d9",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fedfd22-e34e-4d0d-9d90-ddf122161c5b",
   "metadata": {},
   "source": [
    "Install some python packages we are going to use in the POC. For the sake of abstraction, we have defined all important function used in this notebook in utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3763f77-ab52-4f7e-9358-15b48896f608",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import boto3\n",
    "import json\n",
    "import time\n",
    "import base64\n",
    "import logging\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "from urllib.parse import urlparse\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from sagemaker.s3 import S3Downloader as s3down\n",
    "\n",
    "# import sagemaker\n",
    "# from utils import *\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth, helpers\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a1a111-d79e-4b16-8bdf-35b68b198c4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "from IPython.display import Markdown, display, Pretty\n",
    "\n",
    "# getting boto3 clients for required AWS services\n",
    "sts_client = boto3.client(\"sts\")\n",
    "s3_client = boto3.client(\"s3\")\n",
    "# aoss_client = boto3.client('opensearchserverless')\n",
    "\n",
    "\n",
    "region = os.environ.get(\"AWS_REGION\")\n",
    "boto3_bedrock = boto3.client(\n",
    "    service_name=\"bedrock-runtime\",\n",
    "    region_name=region,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7cbfd79-97b7-4067-9688-26be5148693c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Load publically available dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6daf6b82-7e3e-4c56-bb2f-288c0e0c8fb8",
   "metadata": {},
   "source": [
    "For this notebook, you are using the Amazon Berkeley Objects Dataset. The dataset is a collection of 147,702 product listings with multilingual metadata and 398,212 unique catalog images. 8,222 listings come with turntable photography. You will only make use of the item images and item name in US English (which is we consider as the product’s short description). For demo purposes you are going to use about 1,600 products for this practice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88974bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bedrock models\n",
    "# Select Amazon titan-embed-image-v1 as Embedding model for multimodal indexing\n",
    "multimodal_embed_model = f\"amazon.titan-embed-image-v1\"\n",
    "\n",
    "\n",
    "\"\"\" \n",
    "Function to plot heatmap from embeddings\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def plot_similarity_heatmap(embeddings_a, embeddings_b):\n",
    "    inner_product = np.inner(embeddings_a, embeddings_b)\n",
    "    sns.set(font_scale=1.1)\n",
    "    graph = sns.heatmap(\n",
    "        inner_product,\n",
    "        vmin=np.min(inner_product),\n",
    "        vmax=1,\n",
    "        cmap=\"OrRd\",\n",
    "    )\n",
    "\n",
    "\n",
    "\"\"\" \n",
    "Function to fetch the image based on image id from dataset\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_image_from_item_id(item_id=\"0\", dataset=None, return_image=True):\n",
    "\n",
    "    item_idx = dataset.query(f\"item_id == {item_id}\").index[0]\n",
    "    img_path = dataset.iloc[item_idx].image_path\n",
    "\n",
    "    if return_image:\n",
    "        img = Image.open(img_path)\n",
    "        return img, dataset.iloc[item_idx].item_desc\n",
    "    else:\n",
    "        return img_path, dataset.iloc[item_idx].item_desc\n",
    "    print(item_idx, img_path)\n",
    "\n",
    "\n",
    "\"\"\" \n",
    "Function to fetch the image based on image id from S3 bucket\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_image_from_item_id_s3(\n",
    "    item_id=\"B0896LJNLH\", dataset=None, image_path=None, return_image=True\n",
    "):\n",
    "\n",
    "    item_idx = dataset.query(f\"item_id == '{item_id}'\").index[0]\n",
    "    img_loc = dataset.iloc[item_idx].img_full_path\n",
    "\n",
    "    if img_loc.startswith(\"s3\"):\n",
    "        # download and store images locally\n",
    "        local_data_root = f\"./data/images\"\n",
    "        local_file_name = img_loc.split(\"/\")[-1]\n",
    "\n",
    "        s3down.download(img_loc, local_data_root)\n",
    "\n",
    "    local_image_path = f\"{local_data_root}/{local_file_name}\"\n",
    "\n",
    "    if return_image:\n",
    "        img = Image.open(local_image_path)\n",
    "        return img, dataset.iloc[item_idx].item_name_in_en_us\n",
    "    else:\n",
    "        return local_image_path, dataset.iloc[item_idx].item_name_in_en_us\n",
    "\n",
    "\n",
    "\"\"\" \n",
    "Function to display the images.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def display_images(\n",
    "    images: [Image],\n",
    "    columns=2,\n",
    "    width=20,\n",
    "    height=8,\n",
    "    max_images=15,\n",
    "    label_wrap_length=50,\n",
    "    label_font_size=8,\n",
    "):\n",
    "\n",
    "    if not images:\n",
    "        print(\"No images to display.\")\n",
    "        return\n",
    "\n",
    "    if len(images) > max_images:\n",
    "        print(f\"Showing {max_images} images of {len(images)}:\")\n",
    "        images = images[0:max_images]\n",
    "\n",
    "    height = max(height, int(len(images) / columns) * height)\n",
    "    plt.figure(figsize=(width, height))\n",
    "    for i, image in enumerate(images):\n",
    "\n",
    "        plt.subplot(int(len(images) / columns + 1), columns, i + 1)\n",
    "        plt.imshow(image)\n",
    "\n",
    "        if hasattr(image, \"name_and_score\"):\n",
    "            plt.title(image.name_and_score, fontsize=label_font_size);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01027603-23b4-43a0-836c-fe7e9a76870c",
   "metadata": {},
   "source": [
    "### 2.1 Data overview and preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9031eae0-b532-40fd-8170-cecb41d0fb94",
   "metadata": {},
   "source": [
    "Load the metadata\n",
    "\n",
    "You can use pandas to load metadata, then select products which have titles in US English from the data frame. You will use a column called main_image_id to merge item name with item image later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fb9f25-79fc-418d-8c65-68920afca46d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "meta = pd.read_json(\n",
    "    \"s3://amazon-berkeley-objects/listings/metadata/listings_0.json.gz\", lines=True\n",
    ")\n",
    "\n",
    "\n",
    "def func_(x):\n",
    "    us_texts = [item[\"value\"] for item in x if item[\"language_tag\"] == \"en_US\"]\n",
    "    return us_texts[0] if us_texts else None\n",
    "\n",
    "\n",
    "meta = meta.assign(item_name_in_en_us=meta.item_name.apply(func_))\n",
    "meta = meta[~meta.item_name_in_en_us.isna()][\n",
    "    [\"item_id\", \"item_name_in_en_us\", \"main_image_id\"]\n",
    "]\n",
    "print(f\"#products with US English title: {len(meta)}\")\n",
    "meta.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab441ee-61bc-4c5c-84a0-2766359ec76f",
   "metadata": {},
   "source": [
    "You should be able to see over 1600 products in the data frame.\n",
    "Next, you can link the item names with item images. `images/metadata/images.csv.gz` contains Image metadata. This file is a gzip-compressed comma-separated value (CSV) file with the following columns: `image_id`, `height`, `width`, and `path`. You can read the meta data file and then merge it with item metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e956a3-e679-4529-be39-a065f4601e91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_meta = pd.read_csv(\"s3://amazon-berkeley-objects/images/metadata/images.csv.gz\")\n",
    "dataset = meta.merge(image_meta, left_on=\"main_image_id\", right_on=\"image_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63706e95-2038-4160-bd4e-11d247639780",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a new column in dataset with FULL PATH of the image\n",
    "dataset = dataset.assign(\n",
    "    img_full_path=f\"s3://amazon-berkeley-objects/images/small/\"\n",
    "    + dataset.path.astype(str)\n",
    ")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac302f2e-faaa-428a-9b3d-7f240aaff49e",
   "metadata": {},
   "source": [
    "You can have a look at one sample image from the dataset by running the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5027897-7281-4144-82ce-583908c5c71e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image, item_name = get_image_from_item_id_s3(\n",
    "    item_id=\"B0896LJNLH\",\n",
    "    dataset=dataset,\n",
    "    image_path=f\"s3://amazon-berkeley-objects/images/small/\",\n",
    ")\n",
    "print(item_name)\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdb6f7e-5f07-4e41-9b38-24cbb8434930",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Generate embedding from item images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2167e377-7457-4853-b984-d246bf27eec9",
   "metadata": {},
   "source": [
    "Amazon Titan Multimodal Embeddings G1 Generation 1 (G1) is able to project both images and text into the same latent space, so we only need to encode item images or texts into embedding space. In this practice, you can use [batch inference](https://docs.aws.amazon.com/bedrock/latest/userguide/batch-inference.html) to encode item images. Before creating the job, you need to copy item images from Amazon Berkeley Objects Dataset public S3 bucket to your own S3 Bucket. The operation needs take less than 10 mins.\n",
    "\n",
    "But for this notebook, we'll use real-time API than batch inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1895c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "dataset = dataset.iloc[:batch_size]\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6c068b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in enumerate(zip(dataset['img_full_path'], dataset['item_name_in_en_us'])):\n",
    "#     print(i[0], i[1])\n",
    "for img_details in enumerate(\n",
    "    zip(dataset[\"img_full_path\"], dataset[\"item_name_in_en_us\"])\n",
    "):\n",
    "    print(img_details[0], img_details[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397f7307-a892-4ed9-9856-2de4772d8ea9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "def get_titan_multimodal_embedding_fix(\n",
    "    image_path: str = None,  # maximum 2048 x 2048 pixels\n",
    "    description: str = None,  # English only and max input tokens 128\n",
    "    dimension: int = 1024,  # 1,024 (default), 384, 256\n",
    "    model_id: str = multimodal_embed_model,\n",
    "):\n",
    "    # print(image_path)\n",
    "    # print(description)\n",
    "    payload_body = {}\n",
    "    embedding_config = {\"embeddingConfig\": {\"outputEmbeddingLength\": dimension}}\n",
    "    # You can specify either text or image or both\n",
    "    if image_path:\n",
    "        if image_path.startswith(\"s3\"):\n",
    "            s3 = boto3.client(\"s3\")\n",
    "            bucket_name, key = image_path.replace(\"s3://\", \"\").split(\"/\", 1)\n",
    "            obj = s3.get_object(Bucket=bucket_name, Key=key)\n",
    "            # Read the object's body\n",
    "            body = obj[\"Body\"].read()\n",
    "            # Encode the body in base64\n",
    "            base64_image = base64.b64encode(body).decode(\"utf-8\")\n",
    "            payload_body[\"inputImage\"] = base64_image\n",
    "        else:\n",
    "            with open(image_path, \"rb\") as image_file:\n",
    "                input_image = base64.b64encode(image_file.read()).decode(\"utf8\")\n",
    "            payload_body[\"inputImage\"] = input_image\n",
    "    if description:\n",
    "        payload_body[\"inputText\"] = description\n",
    "\n",
    "    # print(payload_body)\n",
    "    # print(json.dumps({**payload_body, **embedding_config}))\n",
    "    print(\n",
    "        f\" get_titan_multimodal_embedding_fix()::payload:keys={payload_body.keys()}::\"\n",
    "    )\n",
    "    response = boto3_bedrock.invoke_model(\n",
    "        body=json.dumps({**payload_body, **embedding_config}),\n",
    "        modelId=model_id,\n",
    "        accept=\"application/json\",\n",
    "        contentType=\"application/json\",\n",
    "    )\n",
    "\n",
    "    return json.loads(response.get(\"body\").read())\n",
    "\n",
    "\n",
    "multimodal_embeddings_img = []\n",
    "for img_details in enumerate(\n",
    "    zip(dataset[\"img_full_path\"], dataset[\"item_name_in_en_us\"])\n",
    "):\n",
    "    # print(img_details[1])\n",
    "    embedding = get_titan_multimodal_embedding_fix(\n",
    "        description=img_details[1][1], image_path=img_details[1][0], dimension=1024\n",
    "    )[\"embedding\"]\n",
    "    print(np.array(embedding).shape)\n",
    "    multimodal_embeddings_img.append(embedding)\n",
    "\n",
    "\n",
    "dataset = dataset.assign(embedding_img=multimodal_embeddings_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e8f78b-5ccc-41c1-859a-eb26f83333d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5325716f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"item_name_in_en_us\"].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a17342-73c6-41db-af69-942de7e56ea9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.1 Visualize the Image Embedding\n",
    "Let's visualize the embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c88d49-bda6-4aa4-95e0-e525b9b87cd5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_similarity_heatmap(\n",
    "    multimodal_embeddings_img[:batch_size], multimodal_embeddings_img[:batch_size]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a24806-a881-44a1-a9f2-b7b709c9fb8f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.2 [OPTIONAL] Store datset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa12b38-b1e5-47cc-85ee-905065f890de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store dataset\n",
    "# dataset.to_csv('dataset.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beac9ea1-a633-4e49-a49e-5a175d28db6b",
   "metadata": {},
   "source": [
    "## 4. Create a vector store - FAISS In memory vector store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa21dc0-24b3-4ef2-be0f-40ad779fdc44",
   "metadata": {},
   "source": [
    "Before creating the new vector search collection and index, we must first create three associated OpenSearch policies: encryption security policy, network security policy, and data access policy. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f4c99a-78d4-49a3-8673-f567d9b7663d",
   "metadata": {},
   "source": [
    "### 4.1 Create a new FAISS vector Database\n",
    "\n",
    "we will use the metat data to store the image location so we can read the image back from the vector db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9de6a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import CSVLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.schema import Document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26978df0-0758-48f8-8a35-c2f7b453b7c0",
   "metadata": {},
   "source": [
    "### 4.2 Setting up the In-Memory KNN search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac9f261",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_dict = [\n",
    "    {key: value}\n",
    "    for i, (key, value) in enumerate(\n",
    "        zip(dataset[\"item_name_in_en_us\"].to_list(), dataset[\"img_full_path\"].to_list())\n",
    "    )\n",
    "]\n",
    "\n",
    "metadata_dict  # ['AmazonBasics Serene 16-Piece Old Fashioned and Coolers Glass Drinkware Set']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e534e699-88d8-42a7-a6d6-5b228a04c9bf",
   "metadata": {},
   "source": [
    "### 4.3 Ingest the image embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16d969f-4a2a-4f6a-b53b-fc4c7f7a059c",
   "metadata": {},
   "source": [
    "Next you need to loop through your dataset and ingest items data into the cluster. A more robust and scalable solution for the embedding ingestion can be found in [Ingesting enriched data into Amazon ES](https://aws.amazon.com/blogs/industries/novartis-ag-uses-amazon-elasticsearch-k-nearest-neighbor-knn-and-amazon-sagemaker-to-power-search-and-recommendation/). The data ingestion for this POC should finish within 60 seconds. It also executes a simple query to verify the data have been ingested into the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3777b7e3-cb90-4dcd-8160-fec305464be5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# collection = aoss_client.create_collection(name=vector_store_name,type='VECTORSEARCH')\n",
    "# create vector store\n",
    "from langchain.document_loaders import CSVLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "\n",
    "from langchain.embeddings import BedrockEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "multimodal_embed_model = f\"amazon.titan-embed-image-v1\"\n",
    "# create instantiation to embedding model\n",
    "embedding_model = BedrockEmbeddings(\n",
    "    client=boto3_bedrock, model_id=multimodal_embed_model\n",
    ")\n",
    "\n",
    "text_embedding_pairs = zip(\n",
    "    dataset[\"item_name_in_en_us\"].to_list(), multimodal_embeddings_img\n",
    ")\n",
    "# metadata_dict =  dict ( [(key, value) for i, (key, value) in enumerate(zip(dataset['item_name_in_en_us'].to_list(), dataset['img_full_path'].to_list()))] )\n",
    "\n",
    "\n",
    "db = FAISS.from_embeddings(\n",
    "    text_embedding_pairs, embedding_model, metadatas=metadata_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85908804",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_prompt = \"drinkware glass\"\n",
    "\n",
    "v = embedding_model.embed_query(query_prompt)\n",
    "print(v[0:10])\n",
    "results = db.similarity_search_by_vector(v, k=2)\n",
    "display(\n",
    "    Markdown(\n",
    "        \"Let us look at the documents which had the relevant information pertaining to our query\"\n",
    "    )\n",
    ")\n",
    "for r in results:\n",
    "    display(Markdown(f\"{r.page_content}\"), Markdown(f\"{r.metadata}\"))\n",
    "    display(Markdown(f\"------------------------------------\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9667fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results[0].metadata.values())\n",
    "print(results[0].metadata.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c52b22-2d08-43fb-88e2-fd39f4ff0b1b",
   "metadata": {},
   "source": [
    "## 5. Perform a real-time Multimodal Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88e3020-0228-4c3b-b910-73451ba9ea04",
   "metadata": {},
   "source": [
    "Now that you have a working OpenSearch index to contain embeddings for your inventory, let's have a look at how you can generate embedding for new items. You'll use Amazon Titan Multimodal Embeddings G1 Generation 1 (G1) extracting text features and image features. \n",
    "\n",
    "Let’s take a look at the results of a simple query. After retrieving results from the OpenSearch service, we get the item names and images from dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfa367c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_from_faiss_results(results=None):\n",
    "    image_list = []\n",
    "    for img_path in iter(results[0].metadata.values()):\n",
    "        print(img_path)\n",
    "\n",
    "        if img_path.startswith(\"s3\"):\n",
    "            # download and store images locally\n",
    "            local_data_root = f\"./data/images\"\n",
    "            local_file_name = img_path.split(\"/\")[-1]\n",
    "\n",
    "            s3down.download(img_path, local_data_root)\n",
    "\n",
    "            local_image_path = f\"{local_data_root}/{local_file_name}\"\n",
    "\n",
    "        img = Image.open(local_image_path)\n",
    "        image_list.append(img)\n",
    "\n",
    "    return image_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d097223a-3374-43bd-8dee-7fe9886883bc",
   "metadata": {},
   "source": [
    "### 5.1. Perform Image Search based on Text Input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55530289-7577-4384-a671-812aac736e81",
   "metadata": {},
   "source": [
    "Let’s take a look at the results of a simple query. In below example, we'll receive an text input i.e. \"drinkware glass\" from user, and then will send it to search engine to find the similar items."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8eb2e17-4549-46af-b4b9-9f77456693cf",
   "metadata": {},
   "source": [
    "Find the similar items based on use queries. You can see that we found glass drinkware from our dataset based on the input query. That's what we want to achieve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da3c5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_prompt = \"drinkware glass\"\n",
    "v = embedding_model.embed_query(query_prompt)\n",
    "results = db.similarity_search_by_vector(v, k=2)\n",
    "\n",
    "all_images = get_image_from_faiss_results(results)\n",
    "\n",
    "display_images(all_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ad56db-fc9f-482c-b110-2c8b3089113d",
   "metadata": {},
   "source": [
    "### 5.2 Perform Image Search based on Image Input\n",
    "\n",
    "Let’s take a look at the results based on a simple image. The input image will get coverted into vector embeddings and based on the similarity search, it will return the result,\n",
    "\n",
    "You can use any image, but for below example, we'll select a random image from the above dataset based on item_id (for ex. item_id = \"B07JCDQWM6\" ),  and then will send this image to search engine to find the similar items. First, Let's get the image amd image location based on the item id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd83dc1b-f572-48a8-ad86-07e3cfee52d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "item_id = \"B0896LJNLH\"\n",
    "\n",
    "image, item_name = get_image_from_item_id_s3(\n",
    "    item_id=item_id,\n",
    "    dataset=dataset,\n",
    "    image_path=f\"s3://amazon-berkeley-objects/images/small/\",\n",
    ")\n",
    "print(item_name)\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c90df57-6420-4024-ab8a-fc7bcd4c21a7",
   "metadata": {},
   "source": [
    "Then, get the similar items based on the image above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73910b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Function for semantic search capability using knn on input image prompt.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def find_similar_items_from_image(image_path: str, k_nn: int) -> []:\n",
    "    \"\"\"\n",
    "    Main semantic search capability using knn on input image prompt.\n",
    "    Args:\n",
    "        k: number of top-k similar vectors to retrieve from OpenSearch index\n",
    "        num_results: number of the top-k similar vectors to retrieve\n",
    "        index_name: index name in OpenSearch\n",
    "    \"\"\"\n",
    "    query_emb = get_titan_multimodal_embedding_fix(\n",
    "        image_path=search_image_path, dimension=1024\n",
    "    )[\"embedding\"]\n",
    "    # print(query_emb)\n",
    "    results = db.similarity_search_by_vector(query_emb, k=2)\n",
    "    print(results)\n",
    "    image_list = get_image_from_faiss_results(results)\n",
    "    return image_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f44217",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_id = \"B0896LJNLH\"\n",
    "search_image_path = dataset[dataset[\"item_id\"] == item_id][\"img_full_path\"].iloc[0]\n",
    "print(search_image_path)\n",
    "\n",
    "image_list = find_similar_items_from_image(search_image_path, 2)\n",
    "display_images(image_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d261cdb0",
   "metadata": {},
   "source": [
    "## Query an Image - Multimodal model Claude sonnet\n",
    "\n",
    " Now let us send in a query based on an image. The image is a generic flights dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd1f466",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "with open(\"./images/departure_rate.jpg\", \"rb\") as image_file:\n",
    "    content_image = base64.b64encode(image_file.read()).decode(\"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42af548b",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(content_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ec7829",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "import os\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "\n",
    "region = os.environ.get(\"AWS_REGION\")\n",
    "bedrock = boto3.client(\n",
    "    service_name=\"bedrock-runtime\",\n",
    "    region_name=region,\n",
    ")\n",
    "\n",
    "with open(\"./images/departure_rate.jpg\", \"rb\") as image_file:\n",
    "    content_image = base64.b64encode(image_file.read()).decode(\"utf8\")\n",
    "\n",
    "body = json.dumps(\n",
    "    {\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"max_tokens\": 100,\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"image\",\n",
    "                        \"source\": {\n",
    "                            \"type\": \"base64\",\n",
    "                            \"media_type\": \"image/jpeg\",\n",
    "                            \"data\": content_image,\n",
    "                        },\n",
    "                    },\n",
    "                    {\"type\": \"text\", \"text\": \"Give me the flight timings from here.\"},\n",
    "                ],\n",
    "            }\n",
    "        ],\n",
    "        \"temperature\": 0.5,\n",
    "        \"top_p\": 0.9,\n",
    "    }\n",
    ")\n",
    "modelId = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "accept = \"application/json\"\n",
    "contentType = \"application/json\"\n",
    "\n",
    "response = bedrock.invoke_model(\n",
    "    body=body, modelId=modelId, accept=accept, contentType=contentType\n",
    ")\n",
    "response_body = json.loads(response.get(\"body\").read())\n",
    "response_body"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6113040e-a16c-4529-be74-b8baea9f6e28",
   "metadata": {},
   "source": [
    "## 6. Clean up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87766ad7-6326-4000-a9de-3543c94fff47",
   "metadata": {},
   "source": [
    "When you finish this exercise, remove your resources with the following steps:\n",
    "\n",
    "Delete vector index.\n",
    "Delete data, network, and encryption access ploicies.\n",
    "Delete collection.\n",
    "Delete SageMaker Studio user profile and domain.\n",
    "Optionally, empty and delete the S3 bucket, or keep whatever you want.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0337b04e-d37c-400c-9899-d56b352f6c08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# - since these are in memory nothing to delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9fa0ee-8f55-4967-911f-b2e7a7462574",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "ragtestenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
