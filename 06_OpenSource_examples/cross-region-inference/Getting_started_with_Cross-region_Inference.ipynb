{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started with Cross-region Inference in Amazon Bedrock\n",
    "\n",
    "Inference profiles via Amazon Bedrock allow you to perform cross-region inference without the need to setup anything in a fully-managed, secure and private manner. Inference profiles is a managed feature built on Amazon Bedrock, offering generative AI application builders a seamless solution for managing traffic bursts and ensuring optimal availability and performance. By leveraging this feature, builders no longer have to spend time and effort building complex resiliency structure. Instead, inference profiles intelligently routes traffic across multiple opted-in regions, automatically adapting to peak utilization surges. Any inferences carried out through inference profiles will dynamically utilize on-demand capacity available from any of the configured regions, maximizing availability and throughput.\n",
    "\n",
    "## Key Features & Benefits\n",
    "\n",
    "Some of the key features include:\n",
    "\n",
    "* Access to capacity in different regions allowing generative AI workloads to scale with demand.\n",
    "* Access to region-agnostic models to achieve higher throughput.\n",
    "* Become resilient to any traffic bursts.\n",
    "* Ability to select between the pre-defined region sets suiting application needs.\n",
    "* Compatible with existing APIs.\n",
    "* No additional cost.\n",
    "* Same model pricing as the primary region.\n",
    "\n",
    "The end-user experience is not impacted as the model behind cross-region inference remains the same, and builders can focus on writing logic for a differentiated application. \n",
    "\n",
    "Bedrock is the easiest way to build and scale generative AI applications. Cross region inference further enhances the usability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by installing the dependencies to ensure we have a recent version\n",
    "#!pip install --quiet --upgrade --force-reinstall boto3 botocore awscli\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "from io import StringIO\n",
    "import sys\n",
    "import textwrap\n",
    "import os\n",
    "from typing import Optional\n",
    "\n",
    "# External Dependencies:\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def print_ww(*args, width: int = 100, **kwargs):\n",
    "    \"\"\"Like print(), but wraps output to `width` characters (default 100)\"\"\"\n",
    "    buffer = StringIO()\n",
    "    try:\n",
    "        _stdout = sys.stdout\n",
    "        sys.stdout = buffer\n",
    "        print(*args, **kwargs)\n",
    "        output = buffer.getvalue()\n",
    "    finally:\n",
    "        sys.stdout = _stdout\n",
    "    for line in output.splitlines():\n",
    "        print(\"\\n\".join(textwrap.wrap(line, width=width)))\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "def get_bedrock_client(\n",
    "    assumed_role: Optional[str] = None,\n",
    "    region: Optional[str] = None,\n",
    "    runtime: Optional[bool] = True,\n",
    "    service_name: Optional[str] = None,\n",
    "):\n",
    "    \"\"\"Create a boto3 client for Amazon Bedrock, with optional configuration overrides\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    assumed_role :\n",
    "        Optional ARN of an AWS IAM role to assume for calling the Bedrock service. If not\n",
    "        specified, the current active credentials will be used.\n",
    "    region :\n",
    "        Optional name of the AWS Region in which the service should be called (e.g. \"us-east-1\").\n",
    "        If not specified, AWS_REGION or AWS_DEFAULT_REGION environment variable will be used.\n",
    "    runtime :\n",
    "        Optional choice of getting different client to perform operations with the Amazon Bedrock service.\n",
    "    \"\"\"\n",
    "    if region is None:\n",
    "        target_region = os.environ.get(\"AWS_REGION\", os.environ.get(\"AWS_DEFAULT_REGION\"))\n",
    "    else:\n",
    "        target_region = region\n",
    "\n",
    "    print(f\"Create new client\\n  Using region: {target_region}\")\n",
    "    session_kwargs = {\"region_name\": target_region}\n",
    "    client_kwargs = {**session_kwargs}\n",
    "\n",
    "    profile_name = os.environ.get(\"AWS_PROFILE\")\n",
    "    if profile_name:\n",
    "        print(f\"  Using profile: {profile_name}\")\n",
    "        session_kwargs[\"profile_name\"] = profile_name\n",
    "\n",
    "    retry_config = Config(\n",
    "        region_name=target_region,\n",
    "        retries={\n",
    "            \"max_attempts\": 10,\n",
    "            \"mode\": \"standard\",\n",
    "        },\n",
    "    )\n",
    "    session = boto3.Session(**session_kwargs)\n",
    "\n",
    "    if assumed_role:\n",
    "        print(f\"  Using role: {assumed_role}\", end='')\n",
    "        sts = session.client(\"sts\")\n",
    "        response = sts.assume_role(\n",
    "            RoleArn=str(assumed_role),\n",
    "            RoleSessionName=\"langchain-llm-1\"\n",
    "        )\n",
    "        print(\" ... successful!\")\n",
    "        client_kwargs[\"aws_access_key_id\"] = response[\"Credentials\"][\"AccessKeyId\"]\n",
    "        client_kwargs[\"aws_secret_access_key\"] = response[\"Credentials\"][\"SecretAccessKey\"]\n",
    "        client_kwargs[\"aws_session_token\"] = response[\"Credentials\"][\"SessionToken\"]\n",
    "\n",
    "    if not service_name:\n",
    "        if runtime:\n",
    "            service_name='bedrock-runtime'\n",
    "        else:\n",
    "            service_name='bedrock'\n",
    "\n",
    "    bedrock_client = session.client(\n",
    "        service_name=service_name,\n",
    "        config=retry_config,\n",
    "        **client_kwargs\n",
    "    )\n",
    "\n",
    "    print(\"boto3 Bedrock client successfully created!\")\n",
    "    print(bedrock_client._endpoint)\n",
    "    return bedrock_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create new client\n",
      "  Using region: us-east-1\n",
      "  Using profile: profile_acct_4\n",
      "boto3 Bedrock client successfully created!\n",
      "bedrock(https://bedrock.us-east-1.amazonaws.com)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'd8d39c3f-5ad1-4089-ac3d-dce0f70a2716',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'date': 'Fri, 06 Sep 2024 16:58:14 GMT',\n",
       "   'content-type': 'application/json',\n",
       "   'content-length': '29963',\n",
       "   'connection': 'keep-alive',\n",
       "   'x-amzn-requestid': 'd8d39c3f-5ad1-4089-ac3d-dce0f70a2716'},\n",
       "  'RetryAttempts': 0},\n",
       " 'modelSummaries': [{'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-tg1-large',\n",
       "   'modelId': 'amazon.titan-tg1-large',\n",
       "   'modelName': 'Titan Text Large',\n",
       "   'providerName': 'Amazon',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-image-generator-v1:0',\n",
       "   'modelId': 'amazon.titan-image-generator-v1:0',\n",
       "   'modelName': 'Titan Image Generator G1',\n",
       "   'providerName': 'Amazon',\n",
       "   'inputModalities': ['TEXT', 'IMAGE'],\n",
       "   'outputModalities': ['IMAGE'],\n",
       "   'customizationsSupported': ['FINE_TUNING'],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-image-generator-v1',\n",
       "   'modelId': 'amazon.titan-image-generator-v1',\n",
       "   'modelName': 'Titan Image Generator G1',\n",
       "   'providerName': 'Amazon',\n",
       "   'inputModalities': ['TEXT', 'IMAGE'],\n",
       "   'outputModalities': ['IMAGE'],\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-image-generator-v2:0',\n",
       "   'modelId': 'amazon.titan-image-generator-v2:0',\n",
       "   'modelName': 'Titan Image Generator G1 v2',\n",
       "   'providerName': 'Amazon',\n",
       "   'inputModalities': ['TEXT', 'IMAGE'],\n",
       "   'outputModalities': ['IMAGE'],\n",
       "   'customizationsSupported': ['FINE_TUNING'],\n",
       "   'inferenceTypesSupported': ['PROVISIONED', 'ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-text-premier-v1:0',\n",
       "   'modelId': 'amazon.titan-text-premier-v1:0',\n",
       "   'modelName': 'Titan Text G1 - Premier',\n",
       "   'providerName': 'Amazon',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-embed-g1-text-02',\n",
       "   'modelId': 'amazon.titan-embed-g1-text-02',\n",
       "   'modelName': 'Titan Text Embeddings v2',\n",
       "   'providerName': 'Amazon',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['EMBEDDING'],\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-text-lite-v1:0:4k',\n",
       "   'modelId': 'amazon.titan-text-lite-v1:0:4k',\n",
       "   'modelName': 'Titan Text G1 - Lite',\n",
       "   'providerName': 'Amazon',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': ['FINE_TUNING', 'CONTINUED_PRE_TRAINING'],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-text-lite-v1',\n",
       "   'modelId': 'amazon.titan-text-lite-v1',\n",
       "   'modelName': 'Titan Text G1 - Lite',\n",
       "   'providerName': 'Amazon',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-text-express-v1:0:8k',\n",
       "   'modelId': 'amazon.titan-text-express-v1:0:8k',\n",
       "   'modelName': 'Titan Text G1 - Express',\n",
       "   'providerName': 'Amazon',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': ['FINE_TUNING', 'CONTINUED_PRE_TRAINING'],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-text-express-v1',\n",
       "   'modelId': 'amazon.titan-text-express-v1',\n",
       "   'modelName': 'Titan Text G1 - Express',\n",
       "   'providerName': 'Amazon',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-embed-text-v1:2:8k',\n",
       "   'modelId': 'amazon.titan-embed-text-v1:2:8k',\n",
       "   'modelName': 'Titan Embeddings G1 - Text',\n",
       "   'providerName': 'Amazon',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['EMBEDDING'],\n",
       "   'responseStreamingSupported': False,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-embed-text-v1',\n",
       "   'modelId': 'amazon.titan-embed-text-v1',\n",
       "   'modelName': 'Titan Embeddings G1 - Text',\n",
       "   'providerName': 'Amazon',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['EMBEDDING'],\n",
       "   'responseStreamingSupported': False,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-embed-text-v2:0:8k',\n",
       "   'modelId': 'amazon.titan-embed-text-v2:0:8k',\n",
       "   'modelName': 'Titan Text Embeddings V2',\n",
       "   'providerName': 'Amazon',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['EMBEDDING'],\n",
       "   'responseStreamingSupported': False,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': [],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-embed-text-v2:0',\n",
       "   'modelId': 'amazon.titan-embed-text-v2:0',\n",
       "   'modelName': 'Titan Text Embeddings V2',\n",
       "   'providerName': 'Amazon',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['EMBEDDING'],\n",
       "   'responseStreamingSupported': False,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-embed-image-v1:0',\n",
       "   'modelId': 'amazon.titan-embed-image-v1:0',\n",
       "   'modelName': 'Titan Multimodal Embeddings G1',\n",
       "   'providerName': 'Amazon',\n",
       "   'inputModalities': ['TEXT', 'IMAGE'],\n",
       "   'outputModalities': ['EMBEDDING'],\n",
       "   'customizationsSupported': ['FINE_TUNING'],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-embed-image-v1',\n",
       "   'modelId': 'amazon.titan-embed-image-v1',\n",
       "   'modelName': 'Titan Multimodal Embeddings G1',\n",
       "   'providerName': 'Amazon',\n",
       "   'inputModalities': ['TEXT', 'IMAGE'],\n",
       "   'outputModalities': ['EMBEDDING'],\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/stability.stable-diffusion-xl-v1:0',\n",
       "   'modelId': 'stability.stable-diffusion-xl-v1:0',\n",
       "   'modelName': 'SDXL 1.0',\n",
       "   'providerName': 'Stability AI',\n",
       "   'inputModalities': ['TEXT', 'IMAGE'],\n",
       "   'outputModalities': ['IMAGE'],\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/stability.stable-diffusion-xl-v1',\n",
       "   'modelId': 'stability.stable-diffusion-xl-v1',\n",
       "   'modelName': 'SDXL 1.0',\n",
       "   'providerName': 'Stability AI',\n",
       "   'inputModalities': ['TEXT', 'IMAGE'],\n",
       "   'outputModalities': ['IMAGE'],\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/ai21.j2-grande-instruct',\n",
       "   'modelId': 'ai21.j2-grande-instruct',\n",
       "   'modelName': 'J2 Grande Instruct',\n",
       "   'providerName': 'AI21 Labs',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': False,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/ai21.j2-jumbo-instruct',\n",
       "   'modelId': 'ai21.j2-jumbo-instruct',\n",
       "   'modelName': 'J2 Jumbo Instruct',\n",
       "   'providerName': 'AI21 Labs',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': False,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/ai21.j2-mid',\n",
       "   'modelId': 'ai21.j2-mid',\n",
       "   'modelName': 'Jurassic-2 Mid',\n",
       "   'providerName': 'AI21 Labs',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': False,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/ai21.j2-mid-v1',\n",
       "   'modelId': 'ai21.j2-mid-v1',\n",
       "   'modelName': 'Jurassic-2 Mid',\n",
       "   'providerName': 'AI21 Labs',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': False,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/ai21.j2-ultra',\n",
       "   'modelId': 'ai21.j2-ultra',\n",
       "   'modelName': 'Jurassic-2 Ultra',\n",
       "   'providerName': 'AI21 Labs',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': False,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/ai21.j2-ultra-v1:0:8k',\n",
       "   'modelId': 'ai21.j2-ultra-v1:0:8k',\n",
       "   'modelName': 'Jurassic-2 Ultra',\n",
       "   'providerName': 'AI21 Labs',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': False,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/ai21.j2-ultra-v1',\n",
       "   'modelId': 'ai21.j2-ultra-v1',\n",
       "   'modelName': 'Jurassic-2 Ultra',\n",
       "   'providerName': 'AI21 Labs',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': False,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/ai21.jamba-instruct-v1:0',\n",
       "   'modelId': 'ai21.jamba-instruct-v1:0',\n",
       "   'modelName': 'Jamba-Instruct',\n",
       "   'providerName': 'AI21 Labs',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-instant-v1:2:100k',\n",
       "   'modelId': 'anthropic.claude-instant-v1:2:100k',\n",
       "   'modelName': 'Claude Instant',\n",
       "   'providerName': 'Anthropic',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-instant-v1',\n",
       "   'modelId': 'anthropic.claude-instant-v1',\n",
       "   'modelName': 'Claude Instant',\n",
       "   'providerName': 'Anthropic',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-v2:0:18k',\n",
       "   'modelId': 'anthropic.claude-v2:0:18k',\n",
       "   'modelName': 'Claude',\n",
       "   'providerName': 'Anthropic',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-v2:0:100k',\n",
       "   'modelId': 'anthropic.claude-v2:0:100k',\n",
       "   'modelName': 'Claude',\n",
       "   'providerName': 'Anthropic',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-v2:1:18k',\n",
       "   'modelId': 'anthropic.claude-v2:1:18k',\n",
       "   'modelName': 'Claude',\n",
       "   'providerName': 'Anthropic',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-v2:1:200k',\n",
       "   'modelId': 'anthropic.claude-v2:1:200k',\n",
       "   'modelName': 'Claude',\n",
       "   'providerName': 'Anthropic',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-v2:1',\n",
       "   'modelId': 'anthropic.claude-v2:1',\n",
       "   'modelName': 'Claude',\n",
       "   'providerName': 'Anthropic',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-v2',\n",
       "   'modelId': 'anthropic.claude-v2',\n",
       "   'modelName': 'Claude',\n",
       "   'providerName': 'Anthropic',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-3-sonnet-20240229-v1:0:28k',\n",
       "   'modelId': 'anthropic.claude-3-sonnet-20240229-v1:0:28k',\n",
       "   'modelName': 'Claude 3 Sonnet',\n",
       "   'providerName': 'Anthropic',\n",
       "   'inputModalities': ['TEXT', 'IMAGE'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-3-sonnet-20240229-v1:0:200k',\n",
       "   'modelId': 'anthropic.claude-3-sonnet-20240229-v1:0:200k',\n",
       "   'modelName': 'Claude 3 Sonnet',\n",
       "   'providerName': 'Anthropic',\n",
       "   'inputModalities': ['TEXT', 'IMAGE'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-3-sonnet-20240229-v1:0',\n",
       "   'modelId': 'anthropic.claude-3-sonnet-20240229-v1:0',\n",
       "   'modelName': 'Claude 3 Sonnet',\n",
       "   'providerName': 'Anthropic',\n",
       "   'inputModalities': ['TEXT', 'IMAGE'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-3-haiku-20240307-v1:0:48k',\n",
       "   'modelId': 'anthropic.claude-3-haiku-20240307-v1:0:48k',\n",
       "   'modelName': 'Claude 3 Haiku',\n",
       "   'providerName': 'Anthropic',\n",
       "   'inputModalities': ['TEXT', 'IMAGE'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-3-haiku-20240307-v1:0:200k',\n",
       "   'modelId': 'anthropic.claude-3-haiku-20240307-v1:0:200k',\n",
       "   'modelName': 'Claude 3 Haiku',\n",
       "   'providerName': 'Anthropic',\n",
       "   'inputModalities': ['TEXT', 'IMAGE'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-3-haiku-20240307-v1:0',\n",
       "   'modelId': 'anthropic.claude-3-haiku-20240307-v1:0',\n",
       "   'modelName': 'Claude 3 Haiku',\n",
       "   'providerName': 'Anthropic',\n",
       "   'inputModalities': ['TEXT', 'IMAGE'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-3-opus-20240229-v1:0:12k',\n",
       "   'modelId': 'anthropic.claude-3-opus-20240229-v1:0:12k',\n",
       "   'modelName': 'Claude 3 Opus',\n",
       "   'providerName': 'Anthropic',\n",
       "   'inputModalities': ['TEXT', 'IMAGE'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-3-opus-20240229-v1:0:28k',\n",
       "   'modelId': 'anthropic.claude-3-opus-20240229-v1:0:28k',\n",
       "   'modelName': 'Claude 3 Opus',\n",
       "   'providerName': 'Anthropic',\n",
       "   'inputModalities': ['TEXT', 'IMAGE'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-3-opus-20240229-v1:0:200k',\n",
       "   'modelId': 'anthropic.claude-3-opus-20240229-v1:0:200k',\n",
       "   'modelName': 'Claude 3 Opus',\n",
       "   'providerName': 'Anthropic',\n",
       "   'inputModalities': ['TEXT', 'IMAGE'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-3-opus-20240229-v1:0',\n",
       "   'modelId': 'anthropic.claude-3-opus-20240229-v1:0',\n",
       "   'modelName': 'Claude 3 Opus',\n",
       "   'providerName': 'Anthropic',\n",
       "   'inputModalities': ['TEXT', 'IMAGE'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['INFERENCE_PROFILE'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-3-5-sonnet-20240620-v1:0:18k',\n",
       "   'modelId': 'anthropic.claude-3-5-sonnet-20240620-v1:0:18k',\n",
       "   'modelName': 'Claude 3.5 Sonnet',\n",
       "   'providerName': 'Anthropic',\n",
       "   'inputModalities': ['TEXT', 'IMAGE'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-3-5-sonnet-20240620-v1:0:51k',\n",
       "   'modelId': 'anthropic.claude-3-5-sonnet-20240620-v1:0:51k',\n",
       "   'modelName': 'Claude 3.5 Sonnet',\n",
       "   'providerName': 'Anthropic',\n",
       "   'inputModalities': ['TEXT', 'IMAGE'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-3-5-sonnet-20240620-v1:0:200k',\n",
       "   'modelId': 'anthropic.claude-3-5-sonnet-20240620-v1:0:200k',\n",
       "   'modelName': 'Claude 3.5 Sonnet',\n",
       "   'providerName': 'Anthropic',\n",
       "   'inputModalities': ['TEXT', 'IMAGE'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-3-5-sonnet-20240620-v1:0',\n",
       "   'modelId': 'anthropic.claude-3-5-sonnet-20240620-v1:0',\n",
       "   'modelName': 'Claude 3.5 Sonnet',\n",
       "   'providerName': 'Anthropic',\n",
       "   'inputModalities': ['TEXT', 'IMAGE'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/cohere.command-text-v14:7:4k',\n",
       "   'modelId': 'cohere.command-text-v14:7:4k',\n",
       "   'modelName': 'Command',\n",
       "   'providerName': 'Cohere',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': ['FINE_TUNING'],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/cohere.command-text-v14',\n",
       "   'modelId': 'cohere.command-text-v14',\n",
       "   'modelName': 'Command',\n",
       "   'providerName': 'Cohere',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/cohere.command-r-v1:0',\n",
       "   'modelId': 'cohere.command-r-v1:0',\n",
       "   'modelName': 'Command R',\n",
       "   'providerName': 'Cohere',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/cohere.command-r-plus-v1:0',\n",
       "   'modelId': 'cohere.command-r-plus-v1:0',\n",
       "   'modelName': 'Command R+',\n",
       "   'providerName': 'Cohere',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/cohere.command-light-text-v14:7:4k',\n",
       "   'modelId': 'cohere.command-light-text-v14:7:4k',\n",
       "   'modelName': 'Command Light',\n",
       "   'providerName': 'Cohere',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': ['FINE_TUNING'],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/cohere.command-light-text-v14',\n",
       "   'modelId': 'cohere.command-light-text-v14',\n",
       "   'modelName': 'Command Light',\n",
       "   'providerName': 'Cohere',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/cohere.embed-english-v3:0:512',\n",
       "   'modelId': 'cohere.embed-english-v3:0:512',\n",
       "   'modelName': 'Embed English',\n",
       "   'providerName': 'Cohere',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['EMBEDDING'],\n",
       "   'responseStreamingSupported': False,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/cohere.embed-english-v3',\n",
       "   'modelId': 'cohere.embed-english-v3',\n",
       "   'modelName': 'Embed English',\n",
       "   'providerName': 'Cohere',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['EMBEDDING'],\n",
       "   'responseStreamingSupported': False,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/cohere.embed-multilingual-v3:0:512',\n",
       "   'modelId': 'cohere.embed-multilingual-v3:0:512',\n",
       "   'modelName': 'Embed Multilingual',\n",
       "   'providerName': 'Cohere',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['EMBEDDING'],\n",
       "   'responseStreamingSupported': False,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/cohere.embed-multilingual-v3',\n",
       "   'modelId': 'cohere.embed-multilingual-v3',\n",
       "   'modelName': 'Embed Multilingual',\n",
       "   'providerName': 'Cohere',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['EMBEDDING'],\n",
       "   'responseStreamingSupported': False,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/meta.llama2-13b-chat-v1:0:4k',\n",
       "   'modelId': 'meta.llama2-13b-chat-v1:0:4k',\n",
       "   'modelName': 'Llama 2 Chat 13B',\n",
       "   'providerName': 'Meta',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'LEGACY'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/meta.llama2-13b-chat-v1',\n",
       "   'modelId': 'meta.llama2-13b-chat-v1',\n",
       "   'modelName': 'Llama 2 Chat 13B',\n",
       "   'providerName': 'Meta',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'LEGACY'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/meta.llama2-70b-chat-v1:0:4k',\n",
       "   'modelId': 'meta.llama2-70b-chat-v1:0:4k',\n",
       "   'modelName': 'Llama 2 Chat 70B',\n",
       "   'providerName': 'Meta',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': [],\n",
       "   'modelLifecycle': {'status': 'LEGACY'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/meta.llama2-70b-chat-v1',\n",
       "   'modelId': 'meta.llama2-70b-chat-v1',\n",
       "   'modelName': 'Llama 2 Chat 70B',\n",
       "   'providerName': 'Meta',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'LEGACY'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/meta.llama2-13b-v1:0:4k',\n",
       "   'modelId': 'meta.llama2-13b-v1:0:4k',\n",
       "   'modelName': 'Llama 2 13B',\n",
       "   'providerName': 'Meta',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': ['FINE_TUNING'],\n",
       "   'inferenceTypesSupported': [],\n",
       "   'modelLifecycle': {'status': 'LEGACY'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/meta.llama2-13b-v1',\n",
       "   'modelId': 'meta.llama2-13b-v1',\n",
       "   'modelName': 'Llama 2 13B',\n",
       "   'providerName': 'Meta',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': [],\n",
       "   'modelLifecycle': {'status': 'LEGACY'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/meta.llama2-70b-v1:0:4k',\n",
       "   'modelId': 'meta.llama2-70b-v1:0:4k',\n",
       "   'modelName': 'Llama 2 70B',\n",
       "   'providerName': 'Meta',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': ['FINE_TUNING'],\n",
       "   'inferenceTypesSupported': [],\n",
       "   'modelLifecycle': {'status': 'LEGACY'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/meta.llama2-70b-v1',\n",
       "   'modelId': 'meta.llama2-70b-v1',\n",
       "   'modelName': 'Llama 2 70B',\n",
       "   'providerName': 'Meta',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': [],\n",
       "   'modelLifecycle': {'status': 'LEGACY'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/meta.llama3-8b-instruct-v1:0',\n",
       "   'modelId': 'meta.llama3-8b-instruct-v1:0',\n",
       "   'modelName': 'Llama 3 8B Instruct',\n",
       "   'providerName': 'Meta',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/meta.llama3-70b-instruct-v1:0',\n",
       "   'modelId': 'meta.llama3-70b-instruct-v1:0',\n",
       "   'modelName': 'Llama 3 70B Instruct',\n",
       "   'providerName': 'Meta',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/mistral.mistral-7b-instruct-v0:2',\n",
       "   'modelId': 'mistral.mistral-7b-instruct-v0:2',\n",
       "   'modelName': 'Mistral 7B Instruct',\n",
       "   'providerName': 'Mistral AI',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/mistral.mixtral-8x7b-instruct-v0:1',\n",
       "   'modelId': 'mistral.mixtral-8x7b-instruct-v0:1',\n",
       "   'modelName': 'Mixtral 8x7B Instruct',\n",
       "   'providerName': 'Mistral AI',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/mistral.mistral-large-2402-v1:0',\n",
       "   'modelId': 'mistral.mistral-large-2402-v1:0',\n",
       "   'modelName': 'Mistral Large (2402)',\n",
       "   'providerName': 'Mistral AI',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/mistral.mistral-small-2402-v1:0',\n",
       "   'modelId': 'mistral.mistral-small-2402-v1:0',\n",
       "   'modelName': 'Mistral Small',\n",
       "   'providerName': 'Mistral AI',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}}]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profile_names = ['default', 'profile_acct_4' ]\n",
    "os.environ[\"AWS_PROFILE\"] = 'profile_acct_4'\n",
    "boto3_client =  get_bedrock_client(region='us-east-1', runtime=False)\n",
    "boto3_client.list_foundation_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create new client\n",
      "  Using region: us-east-1\n",
      "  Using profile: profile_acct_4\n",
      "boto3 Bedrock client successfully created!\n",
      "s3(https://s3.us-east-1.amazonaws.com)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'AJCPM4AER8E9NKC2',\n",
       "  'HostId': 'gKm51BOeilB78eDlwIZ7iZ4SwgFxjxr7x5v/p9K9cZWqg+y9qDleD2HU5FEMtjmmkAoLCINbULI=',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': 'gKm51BOeilB78eDlwIZ7iZ4SwgFxjxr7x5v/p9K9cZWqg+y9qDleD2HU5FEMtjmmkAoLCINbULI=',\n",
       "   'x-amz-request-id': 'AJCPM4AER8E9NKC2',\n",
       "   'date': 'Fri, 06 Sep 2024 16:58:18 GMT',\n",
       "   'content-type': 'application/xml',\n",
       "   'transfer-encoding': 'chunked',\n",
       "   'server': 'AmazonS3'},\n",
       "  'RetryAttempts': 0},\n",
       " 'Buckets': [{'Name': 'aws-logs-425576326687-us-east-1',\n",
       "   'CreationDate': datetime.datetime(2023, 1, 18, 5, 31, 4, tzinfo=tzutc())},\n",
       "  {'Name': 'cf-templates-1enrtp945c59t-us-east-1',\n",
       "   'CreationDate': datetime.datetime(2022, 6, 13, 20, 23, 15, tzinfo=tzutc())},\n",
       "  {'Name': 'cf-templates-1enrtp945c59t-us-west-2',\n",
       "   'CreationDate': datetime.datetime(2024, 6, 17, 22, 49, 31, tzinfo=tzutc())},\n",
       "  {'Name': 'cloudtrail-awslogs-425576326687-5xbkwuj3-isengard-do-not-delete',\n",
       "   'CreationDate': datetime.datetime(2022, 6, 13, 20, 2, 7, tzinfo=tzutc())},\n",
       "  {'Name': 'cross-account-llama',\n",
       "   'CreationDate': datetime.datetime(2024, 9, 5, 21, 45, 18, tzinfo=tzutc())},\n",
       "  {'Name': 'do-not-delete-gatedgarden-audit-425576326687',\n",
       "   'CreationDate': datetime.datetime(2022, 6, 14, 7, 21, 15, tzinfo=tzutc())},\n",
       "  {'Name': 'feliplp-llama3-8b-bedrock',\n",
       "   'CreationDate': datetime.datetime(2024, 5, 22, 18, 15, 49, tzinfo=tzutc())},\n",
       "  {'Name': 'icmtestwriter',\n",
       "   'CreationDate': datetime.datetime(2024, 7, 31, 0, 7, tzinfo=tzutc())},\n",
       "  {'Name': 'jiten-llama2-import-model',\n",
       "   'CreationDate': datetime.datetime(2024, 5, 15, 9, 51, 38, tzinfo=tzutc())},\n",
       "  {'Name': 'jiten-llama2-import-model-us-west-2',\n",
       "   'CreationDate': datetime.datetime(2024, 5, 15, 18, 52, 1, tzinfo=tzutc())},\n",
       "  {'Name': 'jpil-425576326687-models',\n",
       "   'CreationDate': datetime.datetime(2024, 5, 17, 18, 23, 48, tzinfo=tzutc())},\n",
       "  {'Name': 'llama-from-js2',\n",
       "   'CreationDate': datetime.datetime(2023, 8, 25, 21, 51, 48, tzinfo=tzutc())},\n",
       "  {'Name': 'nsmagt-bedrock',\n",
       "   'CreationDate': datetime.datetime(2024, 5, 23, 10, 13, 21, tzinfo=tzutc())},\n",
       "  {'Name': 'sagemaker-425576326687-local-mode-mars',\n",
       "   'CreationDate': datetime.datetime(2023, 6, 28, 1, 28, 18, tzinfo=tzutc())},\n",
       "  {'Name': 'sagemaker-automated-execution-425576326687-us-east-1',\n",
       "   'CreationDate': datetime.datetime(2023, 1, 11, 19, 38, 21, tzinfo=tzutc())},\n",
       "  {'Name': 'sagemaker-automated-execution-425576326687-us-west-2',\n",
       "   'CreationDate': datetime.datetime(2024, 5, 26, 2, 2, 51, tzinfo=tzutc())},\n",
       "  {'Name': 'sagemaker-studio-425576326687-5fw5djxxw06',\n",
       "   'CreationDate': datetime.datetime(2023, 5, 12, 17, 30, 22, tzinfo=tzutc())},\n",
       "  {'Name': 'sagemaker-studio-425576326687-81x8smxozgr',\n",
       "   'CreationDate': datetime.datetime(2024, 5, 16, 10, 53, 40, tzinfo=tzutc())},\n",
       "  {'Name': 'sagemaker-studio-425576326687-9ippmgq8bdp',\n",
       "   'CreationDate': datetime.datetime(2024, 8, 14, 17, 54, 13, tzinfo=tzutc())},\n",
       "  {'Name': 'sagemaker-studio-425576326687-n2ycb0mxcr',\n",
       "   'CreationDate': datetime.datetime(2024, 5, 16, 10, 53, 38, tzinfo=tzutc())},\n",
       "  {'Name': 'sagemaker-studio-425576326687-u9wdhg0a26',\n",
       "   'CreationDate': datetime.datetime(2023, 6, 14, 1, 17, tzinfo=tzutc())},\n",
       "  {'Name': 'sagemaker-studio-425576326687-viv5z6rgdlp',\n",
       "   'CreationDate': datetime.datetime(2023, 6, 14, 1, 0, 2, tzinfo=tzutc())},\n",
       "  {'Name': 'sagemaker-us-east-1-425576326687',\n",
       "   'CreationDate': datetime.datetime(2022, 6, 27, 18, 39, 32, tzinfo=tzutc())},\n",
       "  {'Name': 'sagemaker-us-east-2-425576326687',\n",
       "   'CreationDate': datetime.datetime(2023, 5, 9, 23, 9, 13, tzinfo=tzutc())},\n",
       "  {'Name': 'sagemaker-us-west-2-425576326687',\n",
       "   'CreationDate': datetime.datetime(2023, 6, 14, 1, 20, 3, tzinfo=tzutc())},\n",
       "  {'Name': 'sm-emr-existing-cluster-0efb642c4925',\n",
       "   'CreationDate': datetime.datetime(2023, 1, 11, 16, 53, tzinfo=tzutc())},\n",
       "  {'Name': 'sm-emr-workshop-cluster-12504f22921b',\n",
       "   'CreationDate': datetime.datetime(2023, 1, 11, 19, 22, 54, tzinfo=tzutc())},\n",
       "  {'Name': 'sm-emr-workshop-cluster-12c701524fad',\n",
       "   'CreationDate': datetime.datetime(2023, 1, 15, 5, 20, 43, tzinfo=tzutc())},\n",
       "  {'Name': 'tonouma-425576326687-bedrock-models',\n",
       "   'CreationDate': datetime.datetime(2024, 5, 20, 1, 50, 34, tzinfo=tzutc())},\n",
       "  {'Name': 'tonouma-425576326687-us-west-2-bedrock-models',\n",
       "   'CreationDate': datetime.datetime(2024, 5, 20, 3, 7, 18, tzinfo=tzutc())}],\n",
       " 'Owner': {'DisplayName': 'rsgrewal+exp',\n",
       "  'ID': '8c7a3b5f2599d5655e3d769ed669a4251eab05499f0eb4934d00e33c64a88fa7'}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boto3_client_s3 =  get_bedrock_client(region='us-east-1', runtime=False, service_name=\"s3\")\n",
    "boto3_client_s3.list_buckets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.35.13\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "print(boto3.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " IMPORTANT\n",
    "\n",
    "*Note:* <span style=\"color:red\">This notebook sample uses `us-east-1` region and the inference profiles available there as example.\n",
    "Change the `region_name` and inference profile ids in the cells below depending on which region you are in and which inference profiles are available.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create new client\n",
      "  Using region: us-east-1\n",
      "  Using profile: profile_acct_4\n",
      "boto3 Bedrock client successfully created!\n",
      "sts(https://sts.amazonaws.com)\n",
      "Create new client\n",
      "  Using region: us-east-1\n",
      "  Using profile: profile_acct_4\n",
      "boto3 Bedrock client successfully created!\n",
      "bedrock(https://bedrock.us-east-1.amazonaws.com)\n",
      "Create new client\n",
      "  Using region: us-east-1\n",
      "  Using profile: profile_acct_4\n",
      "boto3 Bedrock client successfully created!\n",
      "bedrock-runtime(https://bedrock-runtime.us-east-1.amazonaws.com)\n",
      "425576326687 <botocore.client.Bedrock object at 0x108f9d7d0> <botocore.client.Bedrock object at 0x108f9d7d0>\n"
     ]
    }
   ],
   "source": [
    "# Make sure you have AWS credentials or AWS profile setup before running this cell\n",
    "profile_names = ['default', 'profile_acct_4' ]\n",
    "os.environ[\"AWS_PROFILE\"] = 'profile_acct_4'\n",
    "#boto3_client =  get_bedrock_client(region='us-east-1', runtime=False)\n",
    "\n",
    "region_name = 'us-east-1'\n",
    "account_id = get_bedrock_client(service_name='sts', region=region_name).get_caller_identity().get('Account')\n",
    "bedrock_client = get_bedrock_client(service_name='bedrock', region=region_name)\n",
    "bedrock_runtime = get_bedrock_client(service_name='bedrock-runtime', region=region_name)\n",
    "\n",
    "print(account_id, bedrock_client, bedrock_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understand Inference Profiles\n",
    "\n",
    "The cross-region inference feature comes with two additional APIs in the Bedrock client SDK:\n",
    "\n",
    "* `list_inference_profiles()` -> This API tells you all the models that are configured behind Inference Profiles for you.\n",
    "* `get_inference_profile(inferenceProfileIdentifier)` -> This API give you specific details of a certain Inference Profile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'inferenceProfileName': 'US Anthropic Claude 3 Sonnet',\n",
       "  'models': [{'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-3-sonnet-20240229-v1:0'},\n",
       "   {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/anthropic.claude-3-sonnet-20240229-v1:0'}],\n",
       "  'description': 'Routes requests to Anthropic Claude 3 Sonnet in us-east-1 and us-west-2.',\n",
       "  'createdAt': datetime.datetime(2024, 8, 26, 0, 0, tzinfo=tzutc()),\n",
       "  'updatedAt': datetime.datetime(2024, 8, 26, 0, 0, tzinfo=tzutc()),\n",
       "  'inferenceProfileArn': 'arn:aws:bedrock:us-east-1:425576326687:inference-profile/us.anthropic.claude-3-sonnet-20240229-v1:0',\n",
       "  'inferenceProfileId': 'us.anthropic.claude-3-sonnet-20240229-v1:0',\n",
       "  'status': 'ACTIVE',\n",
       "  'type': 'SYSTEM_DEFINED'},\n",
       " {'inferenceProfileName': 'US Anthropic Claude 3.5 Sonnet',\n",
       "  'models': [{'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-3-5-sonnet-20240620-v1:0'},\n",
       "   {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/anthropic.claude-3-5-sonnet-20240620-v1:0'}],\n",
       "  'description': 'Routes requests to Anthropic Claude 3.5 Sonnet in us-east-1 and us-west-2.',\n",
       "  'createdAt': datetime.datetime(2024, 8, 26, 0, 0, tzinfo=tzutc()),\n",
       "  'updatedAt': datetime.datetime(2024, 8, 26, 0, 0, tzinfo=tzutc()),\n",
       "  'inferenceProfileArn': 'arn:aws:bedrock:us-east-1:425576326687:inference-profile/us.anthropic.claude-3-5-sonnet-20240620-v1:0',\n",
       "  'inferenceProfileId': 'us.anthropic.claude-3-5-sonnet-20240620-v1:0',\n",
       "  'status': 'ACTIVE',\n",
       "  'type': 'SYSTEM_DEFINED'},\n",
       " {'inferenceProfileName': 'US Anthropic Claude 3 Opus',\n",
       "  'models': [{'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-3-opus-20240229-v1:0'},\n",
       "   {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/anthropic.claude-3-opus-20240229-v1:0'}],\n",
       "  'description': 'Routes requests to Anthropic Cluade 3 Opus in us-east-1 and us-west-2.',\n",
       "  'createdAt': datetime.datetime(2024, 8, 26, 0, 0, tzinfo=tzutc()),\n",
       "  'updatedAt': datetime.datetime(2024, 8, 26, 0, 0, tzinfo=tzutc()),\n",
       "  'inferenceProfileArn': 'arn:aws:bedrock:us-east-1:425576326687:inference-profile/us.anthropic.claude-3-opus-20240229-v1:0',\n",
       "  'inferenceProfileId': 'us.anthropic.claude-3-opus-20240229-v1:0',\n",
       "  'status': 'ACTIVE',\n",
       "  'type': 'SYSTEM_DEFINED'},\n",
       " {'inferenceProfileName': 'US Anthropic Claude 3 Haiku',\n",
       "  'models': [{'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-3-haiku-20240307-v1:0'},\n",
       "   {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/anthropic.claude-3-haiku-20240307-v1:0'}],\n",
       "  'description': 'Routes requests to Anthropic Claude 3 Haiku in us-east-1 and us-west-2.',\n",
       "  'createdAt': datetime.datetime(2024, 8, 26, 0, 0, tzinfo=tzutc()),\n",
       "  'updatedAt': datetime.datetime(2024, 8, 26, 0, 0, tzinfo=tzutc()),\n",
       "  'inferenceProfileArn': 'arn:aws:bedrock:us-east-1:425576326687:inference-profile/us.anthropic.claude-3-haiku-20240307-v1:0',\n",
       "  'inferenceProfileId': 'us.anthropic.claude-3-haiku-20240307-v1:0',\n",
       "  'status': 'ACTIVE',\n",
       "  'type': 'SYSTEM_DEFINED'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bedrock_client.list_inference_profiles()['inferenceProfileSummaries']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to note that cross-region inference offers two forms:\n",
    "\n",
    "**Foundation model in primary region**\n",
    "\n",
    "In this mode a Inference Profile is only configured for a model which exists in the primary region. For such model(s) a failover mechanism would exist allowing requests to be re-routed to secondary regions configured in Inference Profile. By default the request will go to primary region resources and only if the region is busy or you hit your quota limit, the request is routed to another region. In order to determine, which region the request should go to Amazon Bedrock intelligently checks in real-time which region has redundant capacity available. The on-demand principle still applies if none of the region has capacity to handle the request, then it will be throttled.\n",
    "\n",
    "In this mode, If the primary region doesn't have a certain model available, you will not be able to access it in a secondary region via cross-region inference feature.\n",
    "\n",
    "**Available via Inference Profiles**\n",
    "\n",
    "Select list of models are made available via cross-region inference, where Amazon Bedrock abstracts away the regional details and manages the hosting and inference routing automatically. Such foundation models then exist across the pre-defined region sets and the builder can build applications agnostic of setting up a region. This allows reliable throughput, access to leading foundation models as well as scalability in terms of throughput."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '1c72e959-9339-4bc1-8a2c-6e8e18f035d4',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'date': 'Fri, 06 Sep 2024 16:58:48 GMT',\n",
       "   'content-type': 'application/json',\n",
       "   'content-length': '686',\n",
       "   'connection': 'keep-alive',\n",
       "   'x-amzn-requestid': '1c72e959-9339-4bc1-8a2c-6e8e18f035d4'},\n",
       "  'RetryAttempts': 0},\n",
       " 'inferenceProfileName': 'US Anthropic Claude 3.5 Sonnet',\n",
       " 'models': [{'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-3-5-sonnet-20240620-v1:0'},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/anthropic.claude-3-5-sonnet-20240620-v1:0'}],\n",
       " 'description': 'Routes requests to Anthropic Claude 3.5 Sonnet in us-east-1 and us-west-2.',\n",
       " 'createdAt': datetime.datetime(2024, 8, 26, 0, 0, tzinfo=tzutc()),\n",
       " 'updatedAt': datetime.datetime(2024, 8, 26, 0, 0, tzinfo=tzutc()),\n",
       " 'inferenceProfileArn': 'arn:aws:bedrock:us-east-1:425576326687:inference-profile/us.anthropic.claude-3-5-sonnet-20240620-v1:0',\n",
       " 'inferenceProfileId': 'us.anthropic.claude-3-5-sonnet-20240620-v1:0',\n",
       " 'status': 'ACTIVE',\n",
       " 'type': 'SYSTEM_DEFINED'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bedrock_client.get_inference_profile(\n",
    "    inferenceProfileIdentifier='us.anthropic.claude-3-5-sonnet-20240620-v1:0'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the `get_inference_profile` API you can observe the `status` if a Inference Profile is `ACTIVE` or not and also which regions are configured for inference routing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Inference Profiles\n",
    "\n",
    "An Inference Profile is used in the same way as a foundation model using the `modelId` or `arn` of the model. Inference profile also comes with it's own id and arn, where the difference is in the prefix. For the inference profile you can expect a regional prefix such as `us.` or `eu.` behind the model id for it to be recognized as an inference profile. Also in the `arn`, you can find the difference from `:<region>::foundation-model/<model-id>` to `:<region>::inference-profile/<region-set-prefix>.<model-id>`\n",
    "\n",
    "You can use both the `arn` and the `modelId` with `Converse` API whereas only the `modelId` with `InvokeModel` API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converse API\n",
    "\n",
    "Amazon Bedrock now supports a unified messaging API for seamless application building experience. Read about all the models supported via this API [here](https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference.html#conversation-inference-supported-models-features).\n",
    "\n",
    "Let's send a request to both the foundation model as well as the Inference Profile to observe change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "::Foundation Model::Response time: 6 second(s)\n",
      "::model:id:anthropic.claude-3-haiku-20240307-v1:0::Response time: 6 second(s)\n",
      "::Foundation Model::Response output: [{'text': \"For cost-effective applications, Amazon S3 is generally the better choice compared to Amazon EFS (Elastic File System) for storing documents.\\n\\nHere's a comparison between the two services and why S3 is typically more cost-effective:\\n\\n1. Storage Costs:\\n   - S3 offers a pay-as-you-go storage pricing model, which is generally more cost-effective for storing large amounts of unstructured data like documents.\\n   - EFS has a higher base storage cost per GB compared to S3, making it less cost-effective for pure file storage.\\n\\n2. Data Access Patterns:\\n   - S3 is designed for object storage and is well-suited for infrequent access to documents, as it provides cost-effective storage for data that is not accessed frequently.\\n   - EFS is designed for file storage and provides a file system interface, making it more suitable for applications that require frequent access and manipulation of files.\\n\\n3. Scalability:\\n   - S3 is highly scalable and can store virtually unlimited amounts of data, making it a good choice for growing document storage needs.\\n   - EFS has a scalable storage capacity, but it may be more complex to manage and scale for large-scale document storage compared to S3.\\n\\n4. Durability and Availability:\\n   - Both S3 and EFS provide high durability and availability, but S3 has an edge in terms of its eleven 9s (99.999999999%) of durability.\\n\\nFor cost-effective applications that primarily need to store documents with infrequent access, Amazon S3 is generally the more suitable and cost-effective choice compared to Amazon EFS. S3 offers lower storage costs, better scalability, and higher durability, making it the preferred option for pure document storage in most cases.\"}]\n",
      "::Inference Profile::Response time: 5 second(s)\n",
      "::model:id:us.anthropic.claude-3-haiku-20240307-v1:0::Response time: 5 second(s)\n",
      "::Inference Profile::Response output: [{'text': 'For cost-effective applications, Amazon S3 is generally the more cost-effective choice for storing documents compared to Amazon EFS (Elastic File System).\\n\\nHere are some key points to consider:\\n\\n1. **Storage Costs**:\\n   - Amazon S3 offers lower storage costs per GB compared to Amazon EFS, especially for large data volumes.\\n   - S3 has a pay-as-you-go pricing model, where you only pay for the storage you use.\\n   - EFS has a more complex pricing structure, with charges for storage, throughput, and the number of file system operations.\\n\\n2. **Access Patterns**:\\n   - S3 is optimized for object storage and is well-suited for scenarios where you need to store and retrieve individual files or objects.\\n   - EFS is designed for file storage and provides a traditional file system interface, making it more suitable for applications that require frequent file updates, concurrent access, or POSIX-compliant file system operations.\\n\\n3. **Scalability and Performance**:\\n   - S3 is highly scalable and can handle virtually unlimited amounts of data, making it a good choice for growing data needs.\\n   - EFS provides a scalable file system, but its performance may be more dependent on the number of clients accessing the file system concurrently.\\n\\n4. **Use Cases**:\\n   - S3 is a popular choice for storing static content, backups, logs, and other types of unstructured data.\\n   - EFS is more suitable for use cases where you need a shared file system, such as for development environments, databases, or applications that require POSIX-compliant file system access.\\n\\nIn summary, for cost-effective applications where you need to store a large volume of documents or files that are primarily read-only or infrequently updated, Amazon S3 is generally the more cost-effective choice compared to Amazon EFS. However, if your application requires a shared file system with POSIX-compliant access, EFS may be the better option, even if the costs are higher.'}]\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "system_prompt = \"You are an expert on AWS services and always provide correct and concise answers.\"\n",
    "input_message = \"Should I be storing documents in Amazon S3 or EFS for cost effective applications?\"\n",
    "modelId = ('Foundation Model', 'anthropic.claude-3-haiku-20240307-v1:0')\n",
    "inferenceProfileId = ('Inference Profile', 'us.anthropic.claude-3-haiku-20240307-v1:0')\n",
    "\n",
    "for inference_type, id in [modelId, inferenceProfileId]:\n",
    "    start = time()\n",
    "    response = bedrock_runtime.converse(\n",
    "        modelId=id,\n",
    "        system=[{\"text\": system_prompt}],\n",
    "        messages=[{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"text\": input_message}]\n",
    "        }]\n",
    "    )\n",
    "    end = time()\n",
    "    print(f\"::{inference_type}::Response time: {int(end-start)} second(s)\")\n",
    "    print(f\"::model:id:{id}::Response time: {int(end-start)} second(s)\")\n",
    "    print(f\"::{inference_type}::Response output: {response['output']['message']['content']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can observe that using the same API and only the change in model ID you can expect similar behavior.\n",
    "\n",
    "It is also possible to use a full ARN in place of the model ID:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:bedrock:us-east-1:425576326687:inference-profile/us.anthropic.claude-3-sonnet-20240229-v1:0\n",
      "The choice between Amazon S3 and Amazon EFS for storing documents in a cost-effective manner depends on the specific requirements of your application. Here's a comparison to help you decide:\n",
      "\n",
      "**Amazon S3 (Simple Storage Service)**:\n",
      "- S3 is an object storage service designed for storing and retrieving any amount of data from anywhere on the internet.\n",
      "- It is highly scalable, durable, and cost-effective for storing large amounts of data.\n",
      "- S3 is a good choice for static files like documents, images, videos, backups, and archives.\n",
      "- Data is stored as objects within buckets, and you pay for what you use (storage space and data transfer).\n",
      "- S3 offers different storage classes (Standard, Intelligent-Tiering, One Zone-IA, Glacier) that provide varying levels of durability, availability, and cost.\n",
      "\n",
      "**Amazon EFS (Elastic File System)**:\n",
      "- EFS is a fully managed file storage service that provides simple, scalable file storage for use with AWS Cloud services and on-premises resources.\n",
      "- It is designed for use cases that require shared access to the same data from multiple instances or systems.\n",
      "- EFS is well-suited for applications that require a traditional file system interface and shared file access.\n",
      "- You pay for the amount of storage used per month and the amount of data transferred.\n",
      "- EFS can be more expensive than S3 for storing large amounts of infrequently accessed data.\n",
      "\n",
      "In summary, if you need to store and share documents across multiple instances or systems that require a file system interface, Amazon EFS would be the better choice. However, if you are storing static documents that don't require frequent access or shared access, Amazon S3 would be a more cost-effective option, especially for large amounts of data.\n",
      "\n",
      "For cost-effective applications that don't require shared file access or a file system interface, Amazon S3 is generally the more cost-effective choice for document storage, especially if you leverage the various storage classes based on your access patterns and durability requirements.\n"
     ]
    }
   ],
   "source": [
    "inference_profile_id = \"us.anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "inference_profile_arn = f\"arn:aws:bedrock:{region_name}:{account_id}:inference-profile/{inference_profile_id}\"\n",
    "print(inference_profile_arn)\n",
    "\n",
    "response = bedrock_runtime.converse(\n",
    "    modelId=inference_profile_arn,\n",
    "    system=[{\"text\": system_prompt}],\n",
    "    messages=[{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"text\": input_message}]\n",
    "    }]\n",
    ")\n",
    "print(response['output']['message']['content'][0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### InvokeModel API\n",
    "\n",
    "Most of the generative AI applications in production are already built on top of `InvokeModel` API, even the third-party tools also use this API for using the models. The cross-region inference feature is also compatible with this API. Where the Converse API only supports select models, all models available in Amazon Bedrock can leverage InvokeModel API.\n",
    "\n",
    "Let's send a request via both the foundation model id as well as the inference profile id to observe change via this API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "::Foundation Model::Response time: 18 second(s)\n",
      "::Foundation Model::Response output: The choice between Amazon S3 (Simple Storage Service) and Amazon EFS (Elastic File System) for storing documents depends on your specific use case and requirements. Here's a general comparison to help you decide:\n",
      "\n",
      "**Amazon S3**:\n",
      "- S3 is an object storage service designed for storing and retrieving any amount of data from anywhere on the internet.\n",
      "- It is highly scalable, durable, and cost-effective for storing large amounts of data.\n",
      "- S3 is ideal for storing static files, backups, logs, media files, and other types of unstructured data.\n",
      "- Data is stored as objects (files) within buckets, and you can set different storage classes based on access patterns to optimize costs.\n",
      "- S3 provides high availability and durability through data replication across multiple Availability Zones.\n",
      "- Access to S3 is over the network, which can introduce some latency compared to file systems.\n",
      "\n",
      "**Amazon EFS**:\n",
      "- EFS is a fully-managed file system service that provides a simple and scalable way to create and manage file systems in the AWS Cloud.\n",
      "- It is designed for use cases that require shared access to the same data from multiple compute resources (EC2 instances, Lambda functions, etc.) within the same AWS region.\n",
      "- EFS file systems can be mounted on multiple EC2 instances simultaneously, allowing for shared access and collaboration on files.\n",
      "- EFS provides low-latency access to data, making it suitable for applications that require frequent read/write operations.\n",
      "- EFS is more expensive than S3 for storing large amounts of data, as you pay for the provisioned throughput and storage capacity.\n",
      "\n",
      "In general, if you need to store and retrieve large amounts of static or infrequently accessed data in a cost-effective manner, Amazon S3 is the better choice. However, if you have applications that require shared access to files with low latency, or if you need to frequently read and write data, Amazon EFS may be a better fit, despite being more expensive.\n",
      "\n",
      "For cost-effective applications that don't require shared file access or low latency, S3 is typically the more cost-effective option for storing documents and other unstructured data.\n",
      "::Inference Profile::Response time: 16 second(s)\n",
      "::Inference Profile::Response output: The choice between Amazon S3 (Simple Storage Service) and Amazon EFS (Elastic File System) for storing documents depends on your specific use case and requirements. Here's a comparison to help you decide:\n",
      "\n",
      "**Amazon S3**:\n",
      "- S3 is an object storage service designed for storing and retrieving any amount of data from anywhere on the internet.\n",
      "- It is highly scalable, durable, and cost-effective for storing large amounts of data.\n",
      "- S3 is ideal for storing static files, backups, logs, media files, and other types of unstructured data.\n",
      "- Data is stored as objects (files) within buckets, and you can set different storage classes based on access patterns to optimize costs.\n",
      "- S3 provides high availability and durability by replicating data across multiple Availability Zones.\n",
      "- Access to S3 is over the internet, which may not be suitable for low-latency or high-throughput workloads.\n",
      "\n",
      "**Amazon EFS**:\n",
      "- EFS is a fully-managed file storage service that provides a simple and scalable file system interface.\n",
      "- It is designed for use with AWS cloud services and on-premises resources.\n",
      "- EFS is well-suited for workloads that require shared access to the same data from multiple compute resources, such as web servers, application servers, and data analytics clusters.\n",
      "- EFS provides low-latency access to data, making it suitable for applications that require high throughput and low latency.\n",
      "- EFS file systems can grow to petabyte scale, automatically expanding as you add data.\n",
      "- EFS is more expensive than S3 for storing large amounts of data, especially for infrequently accessed data.\n",
      "\n",
      "For cost-effective applications that involve storing and retrieving large amounts of unstructured data, such as documents, media files, or backups, Amazon S3 is generally the more cost-effective choice. S3 offers lower storage costs, especially for infrequently accessed data, and provides high durability and scalability.\n",
      "\n",
      "However, if your application requires shared access to the same data from multiple compute resources with low latency and high throughput, Amazon EFS may be a better choice, despite being more expensive than S3 for storage alone.\n",
      "\n",
      "Ultimately, the decision should be based on your specific requirements for data access patterns, performance, and cost considerations.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "body = json.dumps({\n",
    "    \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "    \"max_tokens\": 1024,\n",
    "    \"temperature\": 0.1,\n",
    "    \"top_p\": 0.9,\n",
    "    \"system\": system_prompt,\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": f\"{input_message}\",\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "})\n",
    "accept = 'application/json'\n",
    "contentType = 'application/json'\n",
    "modelId = ('Foundation Model', 'anthropic.claude-3-sonnet-20240229-v1:0')\n",
    "inferenceProfileId = ('Inference Profile', 'us.anthropic.claude-3-sonnet-20240229-v1:0')\n",
    "for inference_type, id in [modelId, inferenceProfileId]:\n",
    "    start = time()\n",
    "    response = bedrock_runtime.invoke_model(body=body, modelId=id, accept=accept, contentType=contentType)\n",
    "    end = time()\n",
    "    response_body = json.loads(response.get('body').read())\n",
    "    print(f\"::{inference_type}::Response time: {int(end-start)} second(s)\")\n",
    "    print(f\"::{inference_type}::Response output: {response_body['content'][0]['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangChain\n",
    "\n",
    "Below you can find integration on how to use the new cross-region inference feature with one of the popular open-source frameworks [LangChain](https://python.langchain.com/v0.2/docs/integrations/platforms/aws/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet langchain_aws langchain_community"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[LangChain](https://python.langchain.com/v0.2/docs/introduction/) with the help of integrations implements [`langchain-aws`](https://github.com/langchain-ai/langchain-aws) which provides support  `Converse` API via [`ChatBedrockConverse`](https://python.langchain.com/v0.2/docs/integrations/chat/bedrock/#beta-bedrock-converse-api). This allows you to use the latest models such as Anthropic Claude 3 Sonnet via this implementation. Below you can see an example of that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the sentence \"I love programming\" shaped into a poetic form:\n",
      "\n",
      "Code flows like poetry,\n",
      "Pixels dance with ecstasy.\n",
      "Logic's rhythm, my heart's key,\n",
      "Programming, my true reverie.\n"
     ]
    }
   ],
   "source": [
    "from langchain_aws import ChatBedrockConverse\n",
    "\n",
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a helpful assistant that shapes sentences into poetic form. Translate the user sentence.\",\n",
    "    ),\n",
    "    (\"human\", \"I love programming.\"),\n",
    "]\n",
    "\n",
    "llm = ChatBedrockConverse(\n",
    "    model='us.anthropic.claude-3-sonnet-20240229-v1:0',\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    client=bedrock_runtime,\n",
    ")\n",
    "\n",
    "print(llm.invoke(messages).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the ChatBedrock with Model id as the inference profile model id instead of the ARN\n",
    "\n",
    "**Option 1**\n",
    "\n",
    "1. We set the converse api flag to be true and that invokes the ChatBedrockConverse class which will invoke the model using the converse api class\n",
    "2. `Region` needs to be explictly passed in for this to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the sentence \"I love programming\" shaped into a poetic form:\n",
      "\n",
      "Code flows like poetry,\n",
      "Pixels dance with ecstasy.\n",
      "Logic's rhythm, my heart's key,\n",
      "Programming, my true reverie.\n"
     ]
    }
   ],
   "source": [
    "from langchain_aws import ChatBedrock\n",
    "\n",
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a helpful assistant that shapes sentences into poetic form. Translate the user sentence.\",\n",
    "    ),\n",
    "    (\"human\", \"I love programming.\"),\n",
    "]\n",
    "model_parameter = {\"temperature\": 0.0, \"top_p\": .5, \"max_tokens_to_sample\": 200}\n",
    "llm = ChatBedrock(\n",
    "    model_id='us.anthropic.claude-3-sonnet-20240229-v1:0',\n",
    "    model_kwargs=model_parameter, \n",
    "    beta_use_converse_api=True,  \n",
    "    client=bedrock_runtime,\n",
    "    region_name='us-east-1'\n",
    ")\n",
    "print(llm.invoke(messages).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Option 2 where we have the model provider**\n",
    "\n",
    "This does not use the converse api behind the scenes and calls the invoke api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the sentence translated into poetic form:\n",
      "\n",
      "Coding's my passion, a love so true,\n",
      "Crafting lines of logic, a dance anew.\n",
      "Algorithms and syntax, a symphony to behold,\n",
      "Programming's embrace, a story untold.\n"
     ]
    }
   ],
   "source": [
    "from langchain_aws import ChatBedrock\n",
    "\n",
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a helpful assistant that shapes sentences into poetic form. Translate the user sentence.\",\n",
    "    ),\n",
    "    (\"human\", \"I love programming.\"),\n",
    "]\n",
    "\n",
    "model_parameter = {\"temperature\": 0.0, \"top_p\": .5}\n",
    "llm = ChatBedrock(\n",
    "    provider=\"anthropic\",\n",
    "    model_id=\"us.anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "    model_kwargs=model_parameter, \n",
    "    client=bedrock_runtime,\n",
    ")\n",
    "print(llm.invoke(messages).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the invoke and the prescribed message format for Claude models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = json.dumps({\n",
    "    \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "    \"max_tokens\": 1024,\n",
    "    \"temperature\": 0.1,\n",
    "    \"top_p\": 0.9,\n",
    "    \"system\": 'You are a helpful assistant that shapes sentences into poetic form. Translate the user sentence.',\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"I love programming.\",\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "})\n",
    "model_parameter = {\"temperature\": 0.0, \"top_p\": .5}\n",
    "llm = ChatBedrock(\n",
    "    provider=\"anthropic\",\n",
    "    model_id=\"us.anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "    model_kwargs=model_parameter, \n",
    "    client=bedrock_runtime,\n",
    ")\n",
    "print(llm.invoke(messages).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring, Logging, and Metrics\n",
    "\n",
    "Using cross-region inference might route your request to a different region, based on the selected inference profile.\n",
    "\n",
    "If a request gets re-routed, the [Bedrock model invocation log](https://docs.aws.amazon.com/bedrock/latest/userguide/model-invocation-logging.html) will mention the used region in the `inferenceRegion` element of the JSON log entry.\n",
    "\n",
    "The below code snippet will enable the model invocation logging to a CloudWatch log group and then create a CloudWatch metric filter to select and count all model invocations that get routed to a secondary region. You can adapt this code to build a metric for a specific target region and to monitor latency based on region.\n",
    "\n",
    "This code snippet creates a new IAM role with appropriate permissions to enable the model invocation log delivered into CloudWatch Logs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# see https://docs.aws.amazon.com/bedrock/latest/userguide/model-invocation-logging.html\n",
    "\n",
    "log_group_name = 'bedrock-model-invocation-logging'\n",
    "role_name = 'AmazonBedrockModelInvocationCWDeliveryRole'\n",
    "assume_role_policy_doc = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\n",
    "                \"Service\": \"bedrock.amazonaws.com\"\n",
    "            },\n",
    "            \"Action\": \"sts:AssumeRole\",\n",
    "            \"Condition\": {\n",
    "                \"StringEquals\": {\n",
    "                    \"aws:SourceAccount\": account_id,\n",
    "                },\n",
    "                \"ArnLike\": {\n",
    "                    \"aws:SourceArn\": f\"arn:aws:bedrock:{region_name}:{account_id}:*\",\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "iam_client = boto3.client('iam', region_name=region_name)\n",
    "try:\n",
    "    response = iam_client.create_role(\n",
    "        RoleName=role_name,\n",
    "        AssumeRolePolicyDocument=json.dumps(assume_role_policy_doc),\n",
    "    )\n",
    "except iam_client.exceptions.EntityAlreadyExistsException:\n",
    "    pass\n",
    "policy_doc = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"logs:CreateLogStream\",\n",
    "                \"logs:PutLogEvents\",\n",
    "            ],\n",
    "            \"Resource\": f\"arn:aws:logs:{region_name}:{account_id}:log-group:{log_group_name}:log-stream:aws/bedrock/modelinvocations\",\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "iam_client.put_role_policy(\n",
    "    RoleName=role_name,\n",
    "    PolicyName=role_name,\n",
    "    PolicyDocument=json.dumps(policy_doc),\n",
    ")\n",
    "\n",
    "logs_client = boto3.client('logs', region_name=region_name)\n",
    "try:\n",
    "    logs_client.create_log_group(\n",
    "        logGroupName=log_group_name,\n",
    "    )\n",
    "except logs_client.exceptions.ResourceAlreadyExistsException:\n",
    "    pass\n",
    "logs_client.put_retention_policy(\n",
    "    logGroupName=log_group_name,\n",
    "    retentionInDays=365\n",
    ")\n",
    "\n",
    "response = bedrock_client.put_model_invocation_logging_configuration(\n",
    "    loggingConfig={\n",
    "        'cloudWatchConfig': {\n",
    "            'logGroupName': log_group_name,\n",
    "            'roleArn': f\"arn:aws:iam::{account_id}:role/{role_name}\",\n",
    "        },\n",
    "        'imageDataDeliveryEnabled': False,\n",
    "        'imageDataDeliveryEnabled': False,\n",
    "        'embeddingDataDeliveryEnabled': False,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the Model Invocation Log enabled and configured for CloudWatch Logs, you can now create a CloudWatch metric filter from the ingested JSON log entries by filtering for `inferenceRegion` to match / mis-match against your primary or target region:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_client.put_metric_filter(\n",
    "    logGroupName=log_group_name,\n",
    "    filterName='bedrock_cross_region_inference_rerouted',\n",
    "    filterPattern=f'{{ $.inferenceRegion != \"{region_name}\" }}',\n",
    "    metricTransformations=[\n",
    "        {\n",
    "            'metricNamespace': 'Custom',\n",
    "            'metricName': 'bedrock_cross_region_inference_rerouted',\n",
    "            'metricValue': '1',\n",
    "            'defaultValue': 0,\n",
    "            'unit': 'Count',\n",
    "        }\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Cross-region inference provides the ability to manage bursts and spiky traffic patterns across a variety of generative AI workloads and disparate request shapes.With this feature you can easily scale your workloads in production without the need of heavy-lifting, lengthy migrations and overhead of infrastructure management. Amazon Bedrock handles the routing securely, reliably and in a transparent manner while giving you the control you need.\n",
    "\n",
    "### Key considerations\n",
    "While building or migrating applications to use of cross-region inference, it is important to keep in mind the following:\n",
    "- **Latency** - If your application is latency sensitive, it is advised to properly test the use of cross-region inference prior to fully relying on it since the routing to different regions could lead to higher latency numbers thus impacting your application behavior.\n",
    "- **Compliance** - Cross-region inference comes with pre-defined region sets, if these sets contain a region where you can't operate or goes against your policy, then it is advised not to use Inference Profiles, instead utilize Foundation Models directly.\n",
    "- **Determinism** - If you need to have control over where your requests are (or will be) re-routed (other than primary region), it is better to consider just rely on Foundation Model directly. Also the model exclusive to cross-region inference exclusive models should be opted for carefully.\n",
    "- **Variety** - Inference Profiles do not provide access to multiple different models from different regions, it either acts as a failover mechanism for models in primary region or provides Inference Profile exclusive models where the construct of a region is abstracted away. If your business demands to span across variety of foundation models from multiple regions, it is wise to consider building your own architecture via VPC Peering/Transit Gateway or other architectural patterns.\n",
    "\n",
    "### Cleanup\n",
    "\n",
    "If you ran the **Monitoring, Logging, and Metrics** code, consider disabling the Model Invocation Log to avoid incuring associated cost."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AGR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
