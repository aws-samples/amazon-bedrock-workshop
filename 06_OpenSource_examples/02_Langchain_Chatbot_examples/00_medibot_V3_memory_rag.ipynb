{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Conversational Interface - Medical Clinic\n",
    "\n",
    "Conversational interfaces such as chatbots and virtual assistants can be used to enhance the user experience for your customers. Chatbots uses natural language processing (NLP) and machine learning algorithms to understand and respond to user queries. Chatbots can be used in a variety of applications, such as customer service, sales, and e-commerce, to provide quick and efficient responses to users. They can be accessed through various channels such as websites, social media platforms, and messaging apps. In this notebook, we will build a chatbot using two popular Foundation Models (FMs) in Amazon Bedrock, Claude V3 Sonnet and Llama 3 8b."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Set up: Introduction to ChatBedrock\n",
    "\n",
    "**Supports the following**\n",
    "1. Multiple Models from Bedrock \n",
    "2. Converse API\n",
    "3. Ability to do tool binding\n",
    "4. Ability to plug with LangGraph flows"
   ]
  },
  {
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-09-15T18:55:33.048985Z",
     "start_time": "2024-09-15T18:55:33.019601Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import warnings\n",
    "import os\n",
    "import textwrap\n",
    "import boto3\n",
    "import sys\n",
    "from io import StringIO\n",
    "from botocore.config import Config\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "load_dotenv()\n",
    "\n",
    "my_config = Config(\n",
    "    region_name = 'us-west-2',\n",
    "    signature_version = 'v4',\n",
    "    retries = {\n",
    "        'max_attempts': 10,\n",
    "        'mode': 'standard'\n",
    "    }\n",
    ")\n",
    "\n",
    "aws_access_key_id = os.getenv(\"AWS_ACCESS_KEY\")\n",
    "aws_secret_access_key = os.getenv(\"AWS_ACCESS_SECRET\")\n",
    "\n",
    "boto3_bedrock = boto3.client('bedrock-runtime',\n",
    "                      aws_access_key_id=aws_access_key_id,\n",
    "                      aws_secret_access_key=aws_secret_access_key,\n",
    "                    config=my_config\n",
    "                      )\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def print_ww(*args, width: int = 100, **kwargs):\n",
    "    \"\"\"Like print(), but wraps output to `width` characters (default 100)\"\"\"\n",
    "    buffer = StringIO()\n",
    "    try:\n",
    "        _stdout = sys.stdout\n",
    "        sys.stdout = buffer\n",
    "        print(*args, **kwargs)\n",
    "        output = buffer.getvalue()\n",
    "    finally:\n",
    "        sys.stdout = _stdout\n",
    "    for line in output.splitlines():\n",
    "        print(\"\\n\".join(textwrap.wrap(line, width=width)))\n"
   ],
   "outputs": [],
   "execution_count": 56
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### LangChain Expression Language (LCEL):\n",
    "According to LangChain: *\"LangChain Expression Language, or LCEL, is a declarative way to easily compose chains together. LCEL was designed from day 1 to support putting prototypes in production, with no code changes, from the simplest “prompt + LLM” chain to the most complex chains (we’ve seen folks successfully run LCEL chains with 100s of steps in production).\"*\n",
    "\n",
    "On this tutorial we will be using **LangChain Expression Language** to define and invoke our Chatbots."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Chatbot Architectures\n",
    "Chatbots can come in many shape and sizes, all depending on its use-case. Some models are meant to return general information. Others might be catered to a particular audience thus its inferences might be curtained to a particular tone. And others might need relevant context to give out an informed response to the user. Most robust ones will draw from all architectures and build on it. Below are a few popular types of Chatbots.\n",
    "\n",
    "1. **Chatbot (Naive)** - Zero-Shot chatbot with using FM model trained knowledge.\n",
    "2. **Chatbot using prompt** - Template driven - Chatbot with some context provided in the prompt template.\n",
    "3. **Chatbot with persona** - Chatbot with defined roles. i.e. Career Coach and Human interactions.\n",
    "4. **Contextual-aware chatbot** - Passing in context through an external file by generating embeddings.\n",
    "\n",
    "For this demo we will build a robust chatbot that will leverage an array of features drawn from the architectures above. But first lets dive deeper into the architectures."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Using `ChatBedrock` and `HumanMessage` objects to wrap up our message and invoke the LLM"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-15T18:55:38.688367Z",
     "start_time": "2024-09-15T18:55:38.685225Z"
    }
   },
   "source": [
    "from langchain_aws.chat_models.bedrock import ChatBedrock\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "model_parameter = {\"temperature\": 0.0, \"top_p\": .5, \"max_tokens_to_sample\": 2000}\n",
    "model_id = \"meta.llama3-8b-instruct-v1:0\"\n",
    "bedrock_llm = ChatBedrock(\n",
    "    model_id=model_id,\n",
    "    client=boto3_bedrock,\n",
    "    model_kwargs=model_parameter, \n",
    "    beta_use_converse_api=True\n",
    ")\n"
   ],
   "outputs": [],
   "execution_count": 57
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": ""
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Memory powered chatbot with Amazon Bedrock and LangChain\n",
    "The previous chatbot was able to answer us without issues, however because it lacks memory or context is not able to be very useful.\n",
    "In Conversational interfaces such as chatbots, it is highly important to remember previous interactions, both at a short term but also at a long term level.\n",
    "\n",
    "LangChain provides memory components in two forms. First, LangChain provides helper utilities for managing and manipulating previous chat messages. These are designed to be modular and useful regardless of how they are used. Secondly, LangChain provides easy ways to incorporate these utilities into chains.\n",
    "It allows us to easily define and interact with different types of abstractions, which make it easy to build powerful chatbots."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "![Amazon Bedrock - Conversational Interface](./images/chatbot_bedrock.png)\n",
    "*The UML above visualizes a conversational interface with Memory*"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Simple Conversation chain \n",
    "\n",
    "**Uses the In memory Chat Message History**\n",
    "\n",
    "The example below uses the same history for all sessions and shows how to self-manage chat history and later use _RunnableWithMessageHistory_ management which will allow us to also break down our chat history into multiple sessions.\n",
    "\n",
    "**Note**\n",
    "1. `Chat History` is a variable is a placeholder in the prompt template. which will have Human/Ai alternative messages\n",
    "2. Human query is the final question as `Input` variable\n",
    "3. `RunnableWithMessageHistory` is the class which we wrap the `chain` in to run with history. which is in [Docs link]('https://api.python.langchain.com/en/latest/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html#')\n",
    "4. Config object `{\"configurable\": {'session_id_variable':'value,....other keys}` is used by RunnableWithMessageHistory to manage multiple historical chat flows\n",
    "5. Configuration gets passed in as invoke({dict}, config={\"configurable\": {\"session_id\": \"abc123\"}}) and it gets converted to `RunnableConfig` which is passed into every invoke method. To access this we need to extend the Runnable class and access it\n",
    "6. The chain processes the inputs as a dict object\n",
    "\n",
    "\n",
    "Wrap the rag_chain with RunnableWithMessageHistory to automatically handle chat history:\n",
    "\n",
    "Any Chain wrapped with RunnableWithMessageHistory - will manage chat history variables appropriately, however the ChatTemplate should have the Placeholder for history"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T14:23:34.057686Z",
     "start_time": "2024-09-17T14:23:34.052497Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.runnables.config import RunnableConfig\n",
    "from langchain_core.runnables import Runnable\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt_with_history = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a pirate. Answer the following questions as best you can.\"),\n",
    "        (\"placeholder\", \"{chat_history}\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Add history to the in-memory chat history\n",
    "class ChatHistoryAdd(Runnable):\n",
    "    def __init__(self, chat_history):\n",
    "        self.chat_history = chat_history\n",
    "\n",
    "    def invoke(self, _input: str, config: RunnableConfig = None) -> str:\n",
    "        try:\n",
    "            self.chat_history.add_ai_message(_input)\n",
    "            return _input\n",
    "        except Exception as e:\n",
    "            return f\"Error processing input: {str(e)}\"\n",
    "        \n",
    "        \n",
    "history = InMemoryChatMessageHistory()\n",
    "chat_add = ChatHistoryAdd(history)\n",
    "\n",
    "# second way to create a callback runnable function\n",
    "def chat_user_input_add(input_dict: dict) -> dict:\n",
    "    payload = f\"\\n{input_dict['input']}\"\n",
    "    history.add_user_message(payload) \n",
    "    return input_dict \n",
    "\n",
    "chat_user_add = RunnableLambda(chat_user_input_add)\n",
    "\n",
    "memory_chain = (\n",
    "    RunnablePassthrough() \n",
    "    | chat_user_add\n",
    "    | prompt_with_history\n",
    "    | bedrock_llm\n",
    "    | chat_add\n",
    "    | StrOutputParser()\n",
    ")\n",
    "print_ww(f\"Chat history before invocation:\\n{history}\\n\")\n",
    "print_ww(f\"Chat history before runnable invocation:\\n{chat_add.chat_history}\\n\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat history before invocation:\n",
      "\n",
      "\n",
      "Chat history before runnable invocation:\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 86
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T14:23:50.066405Z",
     "start_time": "2024-09-17T14:23:44.999815Z"
    }
   },
   "cell_type": "code",
   "source": [
    "memory_chain.invoke(\n",
    "    {\"input\": \"what is the weather like in Seattle WA?\", \"chat_history\": history.messages},\n",
    "    config={\"configurable\": {\"session_id\": \"abc123\"}}\n",
    "                    )\n",
    "print_ww(\"\\n-----------------------\")\n",
    "print_ww(f\"Chat history after invocation:\\n{history}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----------------------\n",
      "Chat history after invocation:\n",
      "Human:\n",
      "what is the weather like in Seattle WA?\n",
      "AI:\n",
      "\n",
      "Arrr, shiver me timbers! As a pirate, I've had me share o' sailin' the seven seas, but I've also had\n",
      "me share o' dealin's with the scurvy dogs from Seattle, Washington. Now, about the weather in\n",
      "Seattle... (spits out a wad o' chewin' tobacco)\n",
      "\n",
      "Seattle's got a reputation fer bein' a damp and drizzly place, matey. The weather's often gray and\n",
      "overcast, with a misty rain that'll soak yer boots and make ye want to stay below deck. But don't ye\n",
      "worry, it's not all gloom and doom! The rain's not as fierce as the storms I've faced on the high\n",
      "seas, and the sun does peek out from behind the clouds every now and then.\n",
      "\n",
      "In the summer, the weather's a mite more pleasant, with temperatures in the mid-70s to mid-80s\n",
      "(that's 23 to 30 degrees Celsius fer ye landlubbers). But don't get too comfortable, or ye might\n",
      "find yerself caught in a sudden downpour!\n",
      "\n",
      "In the winter, it's a different story altogether. The rain's more frequent, and the temperatures can\n",
      "drop to the mid-30s to mid-40s (that's 2 to 7 degrees Celsius). But that's when the coffee flows\n",
      "like grog, and the locals gather 'round to swap tales o' the sea... er, I mean, the weather.\n",
      "\n",
      "So, if ye be plannin' a visit to Seattle, be sure to pack yer waterproof gear and a good sense o'\n",
      "humor. And if ye see me strollin' down the dock, don't be afraid to offer me a swig o' rum and a\n",
      "tale o' the sea!\n"
     ]
    }
   ],
   "execution_count": 87
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T14:23:54.570257Z",
     "start_time": "2024-09-17T14:23:54.567252Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print_ww(\"\\n-----------------------\")\n",
    "print_ww(f\"Chat history using runnable invocation:\\n{chat_add.chat_history}\") "
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----------------------\n",
      "Chat history using runnable invocation:\n",
      "Human:\n",
      "what is the weather like in Seattle WA?\n",
      "AI:\n",
      "\n",
      "Arrr, shiver me timbers! As a pirate, I've had me share o' sailin' the seven seas, but I've also had\n",
      "me share o' dealin's with the scurvy dogs from Seattle, Washington. Now, about the weather in\n",
      "Seattle... (spits out a wad o' chewin' tobacco)\n",
      "\n",
      "Seattle's got a reputation fer bein' a damp and drizzly place, matey. The weather's often gray and\n",
      "overcast, with a misty rain that'll soak yer boots and make ye want to stay below deck. But don't ye\n",
      "worry, it's not all gloom and doom! The rain's not as fierce as the storms I've faced on the high\n",
      "seas, and the sun does peek out from behind the clouds every now and then.\n",
      "\n",
      "In the summer, the weather's a mite more pleasant, with temperatures in the mid-70s to mid-80s\n",
      "(that's 23 to 30 degrees Celsius fer ye landlubbers). But don't get too comfortable, or ye might\n",
      "find yerself caught in a sudden downpour!\n",
      "\n",
      "In the winter, it's a different story altogether. The rain's more frequent, and the temperatures can\n",
      "drop to the mid-30s to mid-40s (that's 2 to 7 degrees Celsius). But that's when the coffee flows\n",
      "like grog, and the locals gather 'round to swap tales o' the sea... er, I mean, the weather.\n",
      "\n",
      "So, if ye be plannin' a visit to Seattle, be sure to pack yer waterproof gear and a good sense o'\n",
      "humor. And if ye see me strollin' down the dock, don't be afraid to offer me a swig o' rum and a\n",
      "tale o' the sea!\n"
     ]
    }
   ],
   "execution_count": 88
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T14:24:00.634348Z",
     "start_time": "2024-09-17T14:23:59.294429Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Follow-up question:\n",
    "memory_chain.invoke(\n",
    "    {\"input\": \"What is its states capital?\", \"chat_history\": history.messages}, \n",
    "    config={\"configurable\": {\"session_id\": \"abc123\"}})\n",
    "\n",
    "print_ww(\"\\n-----------------------\")\n",
    "print_ww(f\"Chat history after second invocation:\\n{history}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----------------------\n",
      "Chat history after second invocation:\n",
      "Human:\n",
      "what is the weather like in Seattle WA?\n",
      "AI:\n",
      "\n",
      "Arrr, shiver me timbers! As a pirate, I've had me share o' sailin' the seven seas, but I've also had\n",
      "me share o' dealin's with the scurvy dogs from Seattle, Washington. Now, about the weather in\n",
      "Seattle... (spits out a wad o' chewin' tobacco)\n",
      "\n",
      "Seattle's got a reputation fer bein' a damp and drizzly place, matey. The weather's often gray and\n",
      "overcast, with a misty rain that'll soak yer boots and make ye want to stay below deck. But don't ye\n",
      "worry, it's not all gloom and doom! The rain's not as fierce as the storms I've faced on the high\n",
      "seas, and the sun does peek out from behind the clouds every now and then.\n",
      "\n",
      "In the summer, the weather's a mite more pleasant, with temperatures in the mid-70s to mid-80s\n",
      "(that's 23 to 30 degrees Celsius fer ye landlubbers). But don't get too comfortable, or ye might\n",
      "find yerself caught in a sudden downpour!\n",
      "\n",
      "In the winter, it's a different story altogether. The rain's more frequent, and the temperatures can\n",
      "drop to the mid-30s to mid-40s (that's 2 to 7 degrees Celsius). But that's when the coffee flows\n",
      "like grog, and the locals gather 'round to swap tales o' the sea... er, I mean, the weather.\n",
      "\n",
      "So, if ye be plannin' a visit to Seattle, be sure to pack yer waterproof gear and a good sense o'\n",
      "humor. And if ye see me strollin' down the dock, don't be afraid to offer me a swig o' rum and a\n",
      "tale o' the sea!\n",
      "Human:\n",
      "What is its states capital?\n",
      "AI:\n",
      "\n",
      "Arrr, ye want to know the capital o' Washington state, eh? Well, matey, I'll tell ye it's Olympia!\n",
      "That's right, Olympia be the capital o' Washington, and it's a fine place to visit, especially if ye\n",
      "be lookin' fer a taste o' politics and history. Just watch yer step, or ye might trip over a sea o'\n",
      "bureaucrats!\n"
     ]
    }
   ],
   "execution_count": 89
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T14:24:44.539724Z",
     "start_time": "2024-09-17T14:24:44.537050Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InMemoryChatMessageHistory(messages=[HumanMessage(content='\\nwhat is the weather like in Seattle WA?'), AIMessage(content=\"\\n\\nArrr, shiver me timbers! As a pirate, I've had me share o' sailin' the seven seas, but I've also had me share o' dealin's with the scurvy dogs from Seattle, Washington. Now, about the weather in Seattle... (spits out a wad o' chewin' tobacco)\\n\\nSeattle's got a reputation fer bein' a damp and drizzly place, matey. The weather's often gray and overcast, with a misty rain that'll soak yer boots and make ye want to stay below deck. But don't ye worry, it's not all gloom and doom! The rain's not as fierce as the storms I've faced on the high seas, and the sun does peek out from behind the clouds every now and then.\\n\\nIn the summer, the weather's a mite more pleasant, with temperatures in the mid-70s to mid-80s (that's 23 to 30 degrees Celsius fer ye landlubbers). But don't get too comfortable, or ye might find yerself caught in a sudden downpour!\\n\\nIn the winter, it's a different story altogether. The rain's more frequent, and the temperatures can drop to the mid-30s to mid-40s (that's 2 to 7 degrees Celsius). But that's when the coffee flows like grog, and the locals gather 'round to swap tales o' the sea... er, I mean, the weather.\\n\\nSo, if ye be plannin' a visit to Seattle, be sure to pack yer waterproof gear and a good sense o' humor. And if ye see me strollin' down the dock, don't be afraid to offer me a swig o' rum and a tale o' the sea!\", response_metadata={'ResponseMetadata': {'RequestId': '5c7ef488-d8db-436e-ab39-4fe9d7aa2e49', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Tue, 17 Sep 2024 14:23:50 GMT', 'content-type': 'application/json', 'content-length': '1552', 'connection': 'keep-alive', 'x-amzn-requestid': '5c7ef488-d8db-436e-ab39-4fe9d7aa2e49'}, 'RetryAttempts': 0}, 'stopReason': 'end_turn', 'metrics': {'latencyMs': 4776}}, id='run-55cf78ed-6bb3-4002-b321-6722daf97663-0', usage_metadata={'input_tokens': 46, 'output_tokens': 370, 'total_tokens': 416}), HumanMessage(content='\\nWhat is its states capital?'), AIMessage(content=\"\\n\\nArrr, ye want to know the capital o' Washington state, eh? Well, matey, I'll tell ye it's Olympia! That's right, Olympia be the capital o' Washington, and it's a fine place to visit, especially if ye be lookin' fer a taste o' politics and history. Just watch yer step, or ye might trip over a sea o' bureaucrats!\", response_metadata={'ResponseMetadata': {'RequestId': 'a28093de-ca89-406b-b611-c925b869767c', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Tue, 17 Sep 2024 14:24:00 GMT', 'content-type': 'application/json', 'content-length': '500', 'connection': 'keep-alive', 'x-amzn-requestid': 'a28093de-ca89-406b-b611-c925b869767c'}, 'RetryAttempts': 0}, 'stopReason': 'end_turn', 'metrics': {'latencyMs': 1140}}, id='run-36b70245-830e-4712-a6a5-9852fbf19d21-0', usage_metadata={'input_tokens': 427, 'output_tokens': 84, 'total_tokens': 511})])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 90,
   "source": "chat_add.chat_history"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Building Chatbot with Context\n",
    "\n",
    "There are many ways to give our chatbot context. One of the most effective approaches is giving relevant context to our chatbot by retrieving only those relevant pieces to answer the question using vectors, highly-dimensional data generated by an `embeddings model` (algorithms trained to encapsulate information into dense representations in a multidimensional space) and stored in a Vector Database. First we **generate embeddings** by passing our available context to our embeddings model, typically, you will have an ingestion process which will run through your embedding model and generate the embeddings and store them. In this example we are using our own Titan Embeddings model.\n",
    "\n",
    "First step is to chunk our available context, embedd each chunk using our embeddings model and finally store all the chunks into a Vector Database or Vector Store for further retrival.\n",
    "\n",
    "![Embeddings](./images/embeddings_lang.png)\n",
    "\n",
    "Second process is the user request orchestration. User interaction with the chatbot, document retrieval, and finally invoking and returning the answer.\n",
    "\n",
    "![Chatbot](./images/chatbot_lang.png)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Architecture of a context powered Chatbot\n",
    "![4](./images/context-aware-chatbot.png)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### FAISS as VectorStore\n",
    "\n",
    "In order to be able to use embeddings for search, we need a store that can efficiently perform vector similarity searches. In this notebook we use FAISS, which is an in memory store. For permanently store vectors, one can use pgVector, Pinecone or Chroma.\n",
    "\n",
    "The langchain VectorStore API's are available [here](https://python.langchain.com/en/harrison-docs-refactor-3-24/reference/modules/vectorstore.html)\n",
    "\n",
    "To know more about the FAISS vector store please refer to this [document](https://arxiv.org/pdf/1702.08734.pdf).\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Titan embeddings Model\n",
    "\n",
    "Embeddings are a way to represent words, phrases or any other discrete items as vectors in a continuous vector space. This allows machine learning models to perform mathematical operations on these representations and capture semantic relationships between them.\n",
    "\n",
    "Embeddings are for example used for the RAG [document search capability](https://labelbox.com/blog/how-vector-similarity-search-works/) "
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-15T18:56:38.473967Z",
     "start_time": "2024-09-15T18:56:36.956077Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.document_loaders import CSVLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_aws.embeddings import BedrockEmbeddings\n",
    "\n",
    "br_embeddings = BedrockEmbeddings(model_id=\"amazon.titan-embed-text-v1\", client=boto3_bedrock)\n",
    "\n",
    "loader = CSVLoader(\"./rag_data/medi_history.csv\")\n",
    "documents_aws = loader.load() #\n",
    "print(f\"Number of documents={len(documents_aws)}\")\n",
    "\n",
    "docs = CharacterTextSplitter(chunk_size=2000, chunk_overlap=400, separator=\",\").split_documents(documents_aws)\n",
    "\n",
    "print(f\"Number of documents after split and chunking={len(docs)}\")\n",
    "vectorstore_faiss_aws = FAISS.from_documents(\n",
    "    documents=docs,\n",
    "     embedding = br_embeddings\n",
    ")\n",
    "print(f\"vectorstore_faiss_aws: number of elements in the index={vectorstore_faiss_aws.index.ntotal}::\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents=7\n",
      "Number of documents after split and chunking=7\n",
      "vectorstore_faiss_aws: number of elements in the index=7::\n"
     ]
    }
   ],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-15T18:56:40.328230Z",
     "start_time": "2024-09-15T18:56:38.495107Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "\n",
    "condense_question_system_template = (\n",
    "    \"\"\"\n",
    "    You are an assistant for question-answering tasks. ONLY Use the following pieces of retrieved context to answer the question.\n",
    "    If the answer is not in the context below , just say you do not have enough context. \n",
    "    If you don't know the answer, just say that you don't know. \n",
    "    Use three sentences maximum and keep the answer concise.\n",
    "    Context: {context} \n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "condense_question_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", condense_question_system_template),\n",
    "        (\"human\", \"{input}\"), \n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "def debug_inputs(input_dict: dict) -> dict:\n",
    "    return input_dict \n",
    "\n",
    "chat_user_debug = RunnableLambda(debug_inputs)\n",
    "\n",
    "# The chain \n",
    "qa_chain = (\n",
    "    {\n",
    "        \"context\": vectorstore_faiss_aws.as_retriever() | format_docs, # can work even without the format\n",
    "        \"input\": RunnablePassthrough(),\n",
    "    }\n",
    "    | chat_user_debug\n",
    "    | condense_question_prompt\n",
    "    | bedrock_llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print_ww(qa_chain.invoke(input=\"What are autonomous agents?\")) \n",
    "\n",
    "print_ww(qa_chain.invoke(input=\"What all pain medications can be used for headache?\")) "
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "I do not have enough context to answer this question.\n",
      "\n",
      "\n",
      "According to the context, Aspirin can be used primarily for headache. Additionally, with Aspirin,\n",
      "you can generally take Ibruphen and Tylenol.\n"
     ]
    }
   ],
   "execution_count": 60
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversation Role driven Chatbot with History and a Retriever.\n",
    "Wrap with Runnable Chat History and run the chat conversation\n",
    "\n",
    "![Amazon Bedrock - Conversational Interface](./images/context_aware_history_retriever.png)\n",
    "\n",
    "borrowed from https://github.com/langchain-ai/langchain\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-15T18:56:44.614160Z",
     "start_time": "2024-09-15T18:56:44.608700Z"
    }
   },
   "source": [
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "### This below LEVERAGES the In-memory with multiple sessions and session id\n",
    "store = {}\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "contextualized_question_system_template = (\n",
    "    \"Given a chat history and the latest user question \"\n",
    "    \"which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood \"\n",
    "    \"without the chat history. Do NOT answer the question, \"\n",
    "    \"just reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "\n",
    "contextualized_question_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualized_question_system_template),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "qa_system_prompt = \"\"\"You are an assistant for question-answering tasks. \\\n",
    "Use the following pieces of retrieved context to answer the question. \\\n",
    "If the answer is not present in the context, just say you do not have enough context to answer. \\\n",
    "If the input is not present in the context, just say you do not have enough context to answer. \\\n",
    "If the question is not present in the context, just say you do not have enough context to answer. \\\n",
    "If you don't know the answer, just say that you don't know. \\\n",
    "Use three sentences maximum and keep the answer concise.\\\n",
    "\n",
    "{context}\"\"\"\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", qa_system_prompt),\n",
    "    MessagesPlaceholder(\"chat_history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "question_answer_chain = create_stuff_documents_chain(bedrock_llm, qa_prompt)\n",
    "\n",
    "rag_chain = create_retrieval_chain(vectorstore_faiss_aws.as_retriever(), question_answer_chain) \n",
    "chain_with_history_and_rag = RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    "    output_messages_key=\"answer\",\n",
    ")\n"
   ],
   "outputs": [],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-15T18:56:46.841635Z",
     "start_time": "2024-09-15T18:56:45.681191Z"
    }
   },
   "cell_type": "code",
   "source": [
    "result = chain_with_history_and_rag.invoke(\n",
    "    {\"input\": \"What all pain medications can be used for headache?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"session_1\"}}\n",
    ")\n",
    "result"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'What all pain medications can be used for headache?',\n",
       " 'chat_history': [],\n",
       " 'context': [Document(metadata={'source': './rag_data/medi_history.csv', 'row': 6}, page_content='What all pain medications can be used for headache?: what all pain medications can be used?\\nFor your use case only Asprin can be used: Asprin can be used primarily'),\n",
       "  Document(metadata={'source': './rag_data/medi_history.csv', 'row': 5}, page_content='What all pain medications can be used for headache?: what all pain killers can be used?\\nFor your use case only Asprin can be used: Asprin can be used primarily'),\n",
       "  Document(metadata={'source': './rag_data/medi_history.csv', 'row': 1}, page_content='What all pain medications can be used for headache?: What pain medications can be used Asprin?\\nFor your use case only Asprin can be used: With Asprin you can generally take ibruphen, tylenol'),\n",
       "  Document(metadata={'source': './rag_data/medi_history.csv', 'row': 3}, page_content='What all pain medications can be used for headache?: what types of pain can be treated with asprin?\\nFor your use case only Asprin can be used: Asprin can be used to treat headache, body pain')],\n",
       " 'answer': '\\n\\nAccording to the context, Aspirin can be used primarily for headache treatment. Additionally, it is mentioned that with Aspirin, you can generally take Ibruphen and Tylenol for headache treatment.'}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-15T18:56:49.591176Z",
     "start_time": "2024-09-15T18:56:49.587627Z"
    }
   },
   "cell_type": "code",
   "source": "result['chat_history']",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 63
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ""
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As a follow on question\n",
    "\n",
    "1. The phrase `it` will be converted based on the chat history\n",
    "2. Retriever gets invoked to get relevant content based on chat history "
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-15T18:56:54.285643Z",
     "start_time": "2024-09-15T18:56:53.529127Z"
    }
   },
   "cell_type": "code",
   "source": [
    "follow_up_result = chain_with_history_and_rag.invoke(\n",
    "    {\"input\": \"What are medicines does it interfere with?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"session_1\"}}\n",
    ")\n",
    "print_ww(follow_up_result)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'What are medicines does it interfere with?', 'chat_history': [HumanMessage(content='What\n",
      "all pain medications can be used for headache?'), AIMessage(content='\\n\\nAccording to the context,\n",
      "Aspirin can be used primarily for headache treatment. Additionally, it is mentioned that with\n",
      "Aspirin, you can generally take Ibruphen and Tylenol for headache treatment.')], 'context':\n",
      "[Document(metadata={'source': './rag_data/medi_history.csv', 'row': 2}, page_content='What all pain\n",
      "medications can be used for headache?: what pain medications does Asprin interfere with?\\nFor your\n",
      "use case only Asprin can be used: With Asprin you can generally take all medicines except for XYZ'),\n",
      "Document(metadata={'source': './rag_data/medi_history.csv', 'row': 5}, page_content='What all pain\n",
      "medications can be used for headache?: what all pain killers can be used?\\nFor your use case only\n",
      "Asprin can be used: Asprin can be used primarily'), Document(metadata={'source':\n",
      "'./rag_data/medi_history.csv', 'row': 0}, page_content='What all pain medications can be used for\n",
      "headache?: what is asprin used for?\\nFor your use case only Asprin can be used: Asprin is used for\n",
      "treating headache issues, pain  and also for thinning blood'), Document(metadata={'source':\n",
      "'./rag_data/medi_history.csv', 'row': 6}, page_content='What all pain medications can be used for\n",
      "headache?: what all pain medications can be used?\\nFor your use case only Asprin can be used: Asprin\n",
      "can be used primarily')], 'answer': '\\n\\nAccording to the context, Aspirin interferes with medicines\n",
      "except for XYZ.'}\n"
     ]
    }
   ],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-15T18:56:55.662877Z",
     "start_time": "2024-09-15T18:56:54.837036Z"
    }
   },
   "cell_type": "code",
   "source": [
    "follow_up_result = chain_with_history.invoke(\n",
    "    {\"input\": \"Will it help with pain?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"session_1\"}}\n",
    ")\n",
    "print_ww(follow_up_result)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'Will it help with pain?', 'chat_history': [HumanMessage(content='What all pain\n",
      "medications can be used for headache?'), AIMessage(content='\\n\\nAccording to the context, Aspirin\n",
      "can be used primarily for headache treatment. Additionally, it is mentioned that with Aspirin, you\n",
      "can generally take Ibruphen and Tylenol for headache treatment.'), HumanMessage(content='What are\n",
      "medicines does it interfere with?'), AIMessage(content='\\n\\nAccording to the context, Aspirin\n",
      "interferes with medicines except for XYZ.')], 'context': [Document(metadata={'source':\n",
      "'./rag_data/medi_history.csv', 'row': 4}, page_content='What all pain medications can be used for\n",
      "headache?: what muscle pain can be trated with asprin?\\nFor your use case only Asprin can be used:\n",
      "Asprin can be used to treat all types of muscle pain'), Document(metadata={'source':\n",
      "'./rag_data/medi_history.csv', 'row': 5}, page_content='What all pain medications can be used for\n",
      "headache?: what all pain killers can be used?\\nFor your use case only Asprin can be used: Asprin can\n",
      "be used primarily'), Document(metadata={'source': './rag_data/medi_history.csv', 'row': 6},\n",
      "page_content='What all pain medications can be used for headache?: what all pain medications can be\n",
      "used?\\nFor your use case only Asprin can be used: Asprin can be used primarily'),\n",
      "Document(metadata={'source': './rag_data/medi_history.csv', 'row': 0}, page_content='What all pain\n",
      "medications can be used for headache?: what is asprin used for?\\nFor your use case only Asprin can\n",
      "be used: Asprin is used for treating headache issues, pain  and also for thinning blood')],\n",
      "'answer': '\\n\\nAccording to the context, Aspirin can be used to treat all types of muscle pain.'}\n"
     ]
    }
   ],
   "execution_count": 65
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
